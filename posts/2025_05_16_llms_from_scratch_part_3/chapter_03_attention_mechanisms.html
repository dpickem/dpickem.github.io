<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Daniel Pickem">
<meta name="dcterms.date" content="2025-05-16">

<title>LLMs From Scratch - Chapter 3: Attention Mechanisms – Daniel Pickem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0b37c64f34216b628666a8dac638b53b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-DGMKGL1EBP"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-DGMKGL1EBP', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="LLMs From Scratch - Chapter 3: Attention Mechanisms – Daniel Pickem">
<meta property="og:description" content="Daniel Pickem is currently a Staff Software Engineer at NVIDIA, where he focuses on developing scalable evaluation systems for autonomous vehicles. His passion, however, lies in LLMs, multimodal models, agentic AI, and frontier / foundation models in general.">
<meta property="og:image" content="https://learning.oreilly.com/covers/urn:orm:book:9781633437166/400w/">
<meta property="og:site_name" content="Daniel Pickem">
<meta name="twitter:title" content="LLMs From Scratch - Chapter 3: Attention Mechanisms – Daniel Pickem">
<meta name="twitter:description" content="Daniel Pickem is currently a Staff Software Engineer at NVIDIA, where he focuses on developing scalable evaluation systems for autonomous vehicles. His passion, however, lies in LLMs, multimodal models, agentic AI, and frontier / foundation models in general.">
<meta name="twitter:image" content="https://learning.oreilly.com/covers/urn:orm:book:9781633437166/400w/">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Daniel Pickem</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#attention-mechanisms-chapter-3" id="toc-attention-mechanisms-chapter-3" class="nav-link active" data-scroll-target="#attention-mechanisms-chapter-3">Attention Mechanisms (chapter 3)</a>
  <ul class="collapse">
  <li><a href="#acknowledgment" id="toc-acknowledgment" class="nav-link" data-scroll-target="#acknowledgment">Acknowledgment</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul></li>
  <li><a href="#simplified-self-attention" id="toc-simplified-self-attention" class="nav-link" data-scroll-target="#simplified-self-attention">Simplified self-attention</a>
  <ul class="collapse">
  <li><a href="#a-simple-example-single-query-vector" id="toc-a-simple-example-single-query-vector" class="nav-link" data-scroll-target="#a-simple-example-single-query-vector">A simple example (single query vector)</a></li>
  <li><a href="#a-simple-example-batch-query" id="toc-a-simple-example-batch-query" class="nav-link" data-scroll-target="#a-simple-example-batch-query">A simple example (batch query)</a></li>
  </ul></li>
  <li><a href="#trainable-self-attention" id="toc-trainable-self-attention" class="nav-link" data-scroll-target="#trainable-self-attention">Trainable self-attention</a>
  <ul class="collapse">
  <li><a href="#a-simple-example-single-query-vector-1" id="toc-a-simple-example-single-query-vector-1" class="nav-link" data-scroll-target="#a-simple-example-single-query-vector-1">A simple example (single query vector)</a></li>
  <li><a href="#a-self-attention-class" id="toc-a-self-attention-class" class="nav-link" data-scroll-target="#a-self-attention-class">A self-attention class</a></li>
  </ul></li>
  <li><a href="#causal-self-attention" id="toc-causal-self-attention" class="nav-link" data-scroll-target="#causal-self-attention">Causal self-attention</a>
  <ul class="collapse">
  <li><a href="#dropout-masking-additional-attention-weights" id="toc-dropout-masking-additional-attention-weights" class="nav-link" data-scroll-target="#dropout-masking-additional-attention-weights">Dropout: Masking additional attention weights</a></li>
  <li><a href="#a-compact-causal-attention-class" id="toc-a-compact-causal-attention-class" class="nav-link" data-scroll-target="#a-compact-causal-attention-class">A compact causal attention class</a></li>
  </ul></li>
  <li><a href="#multi-head-self-attention-naive" id="toc-multi-head-self-attention-naive" class="nav-link" data-scroll-target="#multi-head-self-attention-naive">Multi-head self-attention (naive)</a></li>
  <li><a href="#multi-head-self-attention-parallel" id="toc-multi-head-self-attention-parallel" class="nav-link" data-scroll-target="#multi-head-self-attention-parallel">Multi-head self-attention (parallel)</a>
  <ul class="collapse">
  <li><a href="#a-note-on-views" id="toc-a-note-on-views" class="nav-link" data-scroll-target="#a-note-on-views">A note on views</a></li>
  <li><a href="#a-note-on-batched-matrix-multiplications" id="toc-a-note-on-batched-matrix-multiplications" class="nav-link" data-scroll-target="#a-note-on-batched-matrix-multiplications">A note on batched matrix multiplications</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LLMs From Scratch - Chapter 3: Attention Mechanisms</h1>
  <div class="quarto-categories">
    <div class="quarto-category">llms</div>
    <div class="quarto-category">attention</div>
    <div class="quarto-category">tutorial</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Daniel Pickem </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="attention-mechanisms-chapter-3" class="level1">
<h1>Attention Mechanisms (chapter 3)</h1>
<p>This notebook explores attention mechanisms (including self-attention) based on Sebastian Raschka’s book (Chapter 3), implementing basic self-attention, causal self-attention, and multi-headed self-attention (as shown in the figure below).</p>
<section id="acknowledgment" class="level2">
<h2 class="anchored" data-anchor-id="acknowledgment">Acknowledgment</h2>
<p>All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://github.com/rasbt">Sebastian Raschka’s GitHub</a></li>
<li><a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Book Information</a>
<ul>
<li><a href="https://livebook.manning.com/book/build-a-large-language-model-from-scratch/chapter-3/9">Chapter 3</a></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-2.png" class="img-fluid figure-img"></p>
<figcaption>Attention mechanisms</figcaption>
</figure>
</div>
<div id="cell-2" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="simplified-self-attention" class="level1">
<h1>Simplified self-attention</h1>
<p>This mechanism is inspired by the Bahdanau attention mechanism (named after the author of the paper that introduced this mechanism). When generating an output token, the decoder has access to all input tokens selectively. The importance of each input token is determined by an attention weight. More on the Bahdanau attention mechanism is shown in appendix B of the book.</p>
<p>Note that self-attention refers to the fact that we are computing attention / importance weights with respect to a single input sequence. In other words, self-attention asseses and learns the relationships and dependencies between various parts of the input iteself.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-7.png" class="img-fluid figure-img"></p>
<figcaption>Simplified self-attention</figcaption>
</figure>
</div>
<p>In the figure above, we are computing the <strong>context vector</strong> z(2) for the query vector x(2). That context vector z(2) is based on other input elements <span class="math inline">\(x^{(i)}\)</span> in the input sequence <span class="math inline">\(x\)</span> (of length T) where the importance of each input element is determined by <strong>attention weights</strong> <span class="math inline">\(\alpha_{ij}\)</span>.</p>
<p>Note that <strong>attention weights <span class="math inline">\(\alpha_{ij}\)</span></strong> are the normalized version of <strong>attention scores <span class="math inline">\(\omega_{ij}\)</span></strong> we’ll see later on.</p>
<p>A context vector can be thought of as an ‘enriched’ embedding vector that carries context from all other token embedding vectors in the input sequence.</p>
<section id="a-simple-example-single-query-vector" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-example-single-query-vector">A simple example (single query vector)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-8.png" class="img-fluid figure-img"></p>
<figcaption>Self-attention example</figcaption>
</figure>
</div>
<p>These <strong>attention scores <span class="math inline">\(\omega_{ij}\)</span></strong> are then normalized to arrive at <strong>attention weights <span class="math inline">\(\alpha_{ij}\)</span></strong>. Normalization helps with interpretability and maintaining stability during the training process of LLMs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-9.png" class="img-fluid figure-img"></p>
<figcaption>Self-attention example continued</figcaption>
</figure>
</div>
<p>Computing the context vector is simply the attention-weighted sum of all input elements.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-10.png" class="img-fluid figure-img"></p>
<figcaption>Context vector computation</figcaption>
</figure>
</div>
<div id="cell-5" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the input sequence (T = 6).</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.tensor(</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">0.43</span>, <span class="fl">0.15</span>, <span class="fl">0.89</span>],  <span class="co"># Your    (x^1)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">0.55</span>, <span class="fl">0.87</span>, <span class="fl">0.66</span>],  <span class="co"># journey (x^2)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">0.57</span>, <span class="fl">0.85</span>, <span class="fl">0.64</span>],  <span class="co"># starts  (x^3)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">0.22</span>, <span class="fl">0.58</span>, <span class="fl">0.33</span>],  <span class="co"># with    (x^4)</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">0.77</span>, <span class="fl">0.25</span>, <span class="fl">0.10</span>],  <span class="co"># one     (x^5)</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">0.05</span>, <span class="fl">0.80</span>, <span class="fl">0.55</span>],  <span class="co"># step    (x^6)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the attention scores (for the second element of the sequence x^2)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: The attention score computes similarity based on the dot product of the query and key vectors,</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">#       which measures how aligned the query and key vectors are (a higher dot product indicates a</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">#       greater degree of alignment, i.e. similarity between two vectors). A dot product is essentially</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">#       a concise way of multiplying two vectors element-wise and summing the result.</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: In the context of self-attention, the dot product determines the amount of attention the query</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">#       should "pay" to each key in the input sequence.</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Via basic for-loops.</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> inputs[<span class="dv">1</span>]  <span class="co"># Python uses 0-based indexing.</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>attn_scores <span class="op">=</span> torch.empty(inputs.shape[<span class="dv">0</span>])</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x_i <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    attn_scores[i] <span class="op">=</span> torch.dot(x_i, query)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_scores)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Via matrix multiplication.</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>attn_scores_mm <span class="op">=</span> (</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    query <span class="op">@</span> inputs.T</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># The @ operator is syntactic sugar for matrix multiplication.</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attn_scores_mm)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that the two methods yield the same attention scores.</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(attn_scores, attn_scores_mm)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize the attention scores to get the attention weights.</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Via a naive approach.</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> attn_scores <span class="op">/</span> torch.<span class="bu">sum</span>(attn_scores)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention weights: </span><span class="sc">{</span>attn_weights<span class="sc">}</span><span class="ss"> (sum: </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">sum</span>(attn_weights)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Via the softmax function.</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: Softmax handles extreme values more gracefully and offers more favorable gradient properties</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co">#       during training.</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: Since the softmax function ensures that attention weights are always positive and sum to 1,</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="co">#       we can interpret the attention weights as probabilities.</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>attn_weights_softmax <span class="op">=</span> torch.nn.functional.softmax(attn_scores, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Attention weights: </span><span class="sc">{</span>attn_weights_softmax<span class="sc">}</span><span class="ss"> (sum: </span><span class="sc">{</span>torch<span class="sc">.</span><span class="bu">sum</span>(attn_weights_softmax)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the context vector z(2) for the query vector x(2).</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Via a naive approach via a for-loop.</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>context_vector_2 <span class="op">=</span> torch.zeros(inputs.shape[<span class="dv">1</span>])</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x_i <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    context_vector_2 <span class="op">+=</span> attn_weights_softmax[i] <span class="op">*</span> x_i</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vector: </span><span class="sc">{</span>context_vector_2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Via matrix multiplication.</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>context_vector_2_mm <span class="op">=</span> attn_weights_softmax <span class="op">@</span> inputs</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vector: </span><span class="sc">{</span>context_vector_2_mm<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that the two methods yield the same attention scores.</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(context_vector_2, context_vector_2_mm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])
tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])
Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656]) (sum: 1.0000001192092896)
Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]) (sum: 1.0)
Context vector: tensor([0.4419, 0.6515, 0.5683])
Context vector: tensor([0.4419, 0.6515, 0.5683])</code></pre>
</div>
</div>
</section>
<section id="a-simple-example-batch-query" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-example-batch-query">A simple example (batch query)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-11.png" class="img-fluid figure-img"></p>
<figcaption>Batched attention weight computation</figcaption>
</figure>
</div>
<p>A small side-note on tensor initialization.</p>
<ol type="1">
<li>torch.empty
<ul>
<li>Creates a tensor with uninitialized data - the tensor will be allocated in memory but the values are not initialized</li>
<li>The tensor contains whatever values were already in the allocated memory block (garbage values)</li>
<li>It’s faster than torch.zeros because it skips the step of initializing all values</li>
</ul></li>
<li>torch.zeros
<ul>
<li>Creates a tensor filled with the scalar value 0</li>
<li>Explicitly initializes all elements of the tensor to zero</li>
<li>Slightly slower than torch.empty because it needs to set all values to zero</li>
</ul></li>
</ol>
<p>When to use which: - Use torch.zeros when you need a tensor initialized with zeros (most common use case) - Use torch.empty when: - You’ll immediately overwrite all values in the tensor - Performance is critical and you don’t care about initial values - You’re creating a buffer that will be filled later</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-12.png" class="img-fluid figure-img"></p>
<figcaption>Computation flow</figcaption>
</figure>
</div>
<div id="cell-7" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the full attention weights matrix (a square matrix of shape (T, T)).</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inputs: 'Your journey starts with one step'"</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inputs shape: </span><span class="sc">{</span>inputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the unnormalized attention scores.</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Via a naive approach via nested for-loops.</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>attn_scores <span class="op">=</span> torch.zeros(inputs.shape[<span class="dv">0</span>], inputs.shape[<span class="dv">0</span>])</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x_i <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):  <span class="co"># Iterate over the rows of the inputs tensor</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, x_j <span class="kw">in</span> <span class="bu">enumerate</span>(inputs):  <span class="co"># Iterate over the columns of the inputs tensor</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        attn_scores[i, j] <span class="op">=</span> torch.dot(</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            x_i, x_j</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        )  <span class="co"># Compute the dot product of the row and column vectors</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Via matrix multiplication.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>attn_scores_mm <span class="op">=</span> inputs <span class="op">@</span> inputs.T</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(attn_scores, attn_scores_mm)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unnormalized attention scores:</span><span class="ch">\n</span><span class="sc">{</span>attn_scores_mm<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize the attention scores to get the attention weights.</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> torch.nn.functional.softmax(attn_scores_mm, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Normalized attention scores:</span><span class="ch">\n</span><span class="sc">{</span>attn_weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sum of attention weights for each row: </span><span class="sc">{</span>attn_weights<span class="sc">.</span><span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the context vectors for all query vectors.</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>context_vectors <span class="op">=</span> attn_weights <span class="op">@</span> inputs</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vectors:</span><span class="ch">\n</span><span class="sc">{</span>context_vectors<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Inputs: 'Your journey starts with one step'
Inputs shape: torch.Size([6, 3])
Unnormalized attention scores:
tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],
        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],
        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],
        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],
        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],
        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])

Normalized attention scores:
tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],
        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],
        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],
        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],
        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],
        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])
Sum of attention weights for each row: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])
Context vectors:
tensor([[0.4421, 0.5931, 0.5790],
        [0.4419, 0.6515, 0.5683],
        [0.4431, 0.6496, 0.5671],
        [0.4304, 0.6298, 0.5510],
        [0.4671, 0.5910, 0.5266],
        [0.4177, 0.6503, 0.5645]])</code></pre>
</div>
</div>
</section>
</section>
<section id="trainable-self-attention" class="level1">
<h1>Trainable self-attention</h1>
<p>This attention mechanism was used in the original GPT implementation and is often referred to as <strong>scaled dot-product attention</strong>.</p>
<p>The main difference to the simplified self-attention mechanism is the introduction of three trainable weight matrices <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_k\)</span>, <span class="math inline">\(W_v\)</span> (for query, key, and value respectively) that are used to project the embedded input tokens <span class="math inline">\(x^{(i)}\)</span> into query, key, and value vectors respectively.</p>
<p>In the image below note that only the second token (“journey”) is projected into a query vector since only the second token is “being queried”. All T input tokens, however, are projected into key and value vectors (which will later be used to compute the full attention weight matrix).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-14.png" class="img-fluid figure-img"></p>
<figcaption>Scaled dot-product attention</figcaption>
</figure>
</div>
<section id="a-simple-example-single-query-vector-1" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-example-single-query-vector-1">A simple example (single query vector)</h2>
<p>Unlike in the simplified self-attention mechanism, scaled dot-product attention computes attention scores not on raw token embeddings but on the tokens projected into key and value space (via weight matrices <span class="math inline">\(W_k\)</span> and <span class="math inline">\(W_v\)</span>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-15.png" class="img-fluid figure-img"></p>
<figcaption>Scaled dot-product attention example</figcaption>
</figure>
</div>
<p>Computing the normalized attention weights <span class="math inline">\(\alpha_{ij}\)</span> from unnormalized attention scores <span class="math inline">\(\omega_{ij}\)</span> is done via the softmax function as before. This time, however, we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (hence the name <em>scaled</em> dot-product attention).</p>
<p>NOTE: The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate (see page 69 in Sebastian Raschka’s <a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">book</a>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-16.png" class="img-fluid figure-img"></p>
<figcaption>Scaled dot-product attention example continued</figcaption>
</figure>
</div>
<p>The last step is to compute the context vector for <span class="math inline">\(x^{(2)}\)</span>, which is the weighted sum of all value vectors of the input sequence (i.e.&nbsp;the input tokens embedded via the <span class="math inline">\(W_v\)</span> matrix).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-17.png" class="img-fluid figure-img"></p>
<figcaption>Scaled dot-product attention example continued</figcaption>
</figure>
</div>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define input and output embedding size of the W_i embedding matrices.</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: For GPT-like models, the input embedding size is typically equal to the output embedding</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#       size.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> inputs[<span class="dv">1</span>]  <span class="co"># Python uses 0-based indexing.</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>d_in <span class="op">=</span> inputs.shape[<span class="dv">1</span>]  <span class="co"># The size of the input embedding dimension.</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>d_out <span class="op">=</span> <span class="dv">2</span>  <span class="co"># The size of the output embedding dimension.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input token / shape: </span><span class="sc">{</span>x_2<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>x_2<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the trainable weight matrices.</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: requires_grad=False is done here to reduce visual clutter in the outputs. When training the</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">#       model, requires_grad obviously has to be set to True to update the weights during training.</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>W_q <span class="op">=</span> torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>W_k <span class="op">=</span> torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>W_v <span class="op">=</span> torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Project the query input token into query, key, and value vectors.</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>query_2 <span class="op">=</span> x_2 <span class="op">@</span> W_q</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>key_2 <span class="op">=</span> x_2 <span class="op">@</span> W_k</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>value_2 <span class="op">=</span> x_2 <span class="op">@</span> W_v</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weight matrix shape: </span><span class="sc">{</span>W_q<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Projected query vector: </span><span class="sc">{</span>query_2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute key and value vectors for all input tokens.</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: Computing the context vector for the query vector x(2) requires the key and value vectors of</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co">#       all input tokens.</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> inputs <span class="op">@</span> W_k</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> inputs <span class="op">@</span> W_v</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Keys shape: </span><span class="sc">{</span>keys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Values shape: </span><span class="sc">{</span>values<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the unnormalized attention scores (for the query vector x(2) first).</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>keys_2 <span class="op">=</span> keys[<span class="dv">1</span>]</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>attn_scores_2 <span class="op">=</span> query_2.dot(keys_2)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unnormalized attention score for x^2: </span><span class="sc">{</span>attn_scores_2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the unnormalized attention scores for all input tokens.</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>attn_scores <span class="op">=</span> query_2 <span class="op">@</span> keys.T</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unnormalized attention scores: </span><span class="sc">{</span>attn_scores<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize the attention scores to get the attention weights.</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>d_k <span class="op">=</span> keys.shape[<span class="dv">1</span>]</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> torch.nn.functional.softmax(attn_scores <span class="op">/</span> d_k<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention weights: </span><span class="sc">{</span>attn_weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the context vector for the query vector x(2).</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>context_vector_2 <span class="op">=</span> torch.zeros(d_out)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, v_i <span class="kw">in</span> <span class="bu">enumerate</span>(values):</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    context_vector_2 <span class="op">+=</span> attn_weights[i] <span class="op">*</span> v_i</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>context_vector_2_mm <span class="op">=</span> attn_weights <span class="op">@</span> values</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(context_vector_2, context_vector_2_mm)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vector: </span><span class="sc">{</span>context_vector_2<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input token / shape: tensor([0.5500, 0.8700, 0.6600]) (torch.Size([3]))
Weight matrix shape: torch.Size([3, 2])
Projected query vector: tensor([-1.1729, -0.0048])
Keys shape: torch.Size([6, 2])
Values shape: torch.Size([6, 2])
Unnormalized attention score for x^2: 0.13763877749443054
Unnormalized attention scores: tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809])
Attention weights: tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117])
Context vector: tensor([0.2854, 0.4081])</code></pre>
</div>
</div>
</section>
<section id="a-self-attention-class" class="level2">
<h2 class="anchored" data-anchor-id="a-self-attention-class">A self-attention class</h2>
<p>In self-attention, we transform the input vectors in the input matrix X with the three weight matrices, <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_k\)</span>, and <span class="math inline">\(W_v\)</span>. The new compute the attention weight matrix based on the resulting queries (Q) and keys (K). Using the attention weights and values (V), we then compute the context vectors (Z).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-18.png" class="img-fluid figure-img"></p>
<figcaption>A Python class implementing self-attention</figcaption>
</figure>
</div>
<div id="cell-12" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttentionV1(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in: <span class="bu">int</span>, d_out: <span class="bu">int</span>):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_q <span class="op">=</span> nn.Parameter(torch.randn(d_in, d_out))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_k <span class="op">=</span> nn.Parameter(torch.randn(d_in, d_out))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_v <span class="op">=</span> nn.Parameter(torch.randn(d_in, d_out))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Project the input tokens into query, key, and value vectors.</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_q</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_k</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.W_v</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the unnormalized attention scores, i.e. the omegas.</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> query <span class="op">@</span> key.T</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize the attention scores to get the attention weights, i.e. the alphas.</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        d_k <span class="op">=</span> key.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> d_k<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the full set of context vectors.</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        context_vectors <span class="op">=</span> attn_weights <span class="op">@</span> value</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vectors</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttentionV2(nn.Module):</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A Python class implementing self-attention.</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="co">    V2 replaces the nn.Parameter objects with nn.Linear objects which effectively perform matrix</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co">    multiplication when the bias units are disabled.</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co">    One significant advantage of using nn.Linear objects is that nn.Linear has an optimized weight</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co">    initialization scheme that helps with stabilizing the training process and making it more</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="co">    effective.</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_in: <span class="bu">int</span>, d_out: <span class="bu">int</span>, qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_q <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_k <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_v <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Project the input tokens into query, key, and value vectors.</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.W_q(x)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.W_k(x)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.W_v(x)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the unnormalized attention scores, i.e. the omegas.</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> query <span class="op">@</span> key.T</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize the attention scores to get the attention weights, i.e. the alphas.</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>        d_k <span class="op">=</span> key.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> d_k<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the full set of context vectors.</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>        context_vectors <span class="op">=</span> attn_weights <span class="op">@</span> value</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vectors</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the self-attention class.</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>sa_v1 <span class="op">=</span> SelfAttentionV1(d_in, d_out)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>sa_v2 <span class="op">=</span> SelfAttentionV2(d_in, d_out)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: SelfAttentionV1 and SelfAttentionV2 give different outputs because they use different</span></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a><span class="co">#       initial weights for the weight matrices since nn.Linear uses a more sophisticated weight</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a><span class="co">#       initialization scheme.</span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vector 2 (from before): </span><span class="sc">{</span>context_vector_2_mm<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vectors (V1):</span><span class="ch">\n</span><span class="sc">{</span>sa_v1(inputs)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vectors (V2):</span><span class="ch">\n</span><span class="sc">{</span>sa_v2(inputs)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Context vector 2 (from before): tensor([0.2854, 0.4081])
Context vectors (V1):
tensor([[0.2845, 0.4071],
        [0.2854, 0.4081],
        [0.2854, 0.4075],
        [0.2864, 0.3974],
        [0.2863, 0.3910],
        [0.2860, 0.4039]], grad_fn=&lt;MmBackward0&gt;)
Context vectors (V2):
tensor([[0.5322, 0.2491],
        [0.5316, 0.2488],
        [0.5316, 0.2488],
        [0.5340, 0.2501],
        [0.5331, 0.2497],
        [0.5337, 0.2499]], grad_fn=&lt;MmBackward0&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="causal-self-attention" class="level1">
<h1>Causal self-attention</h1>
<p>Causal attention or <em>masked attention</em> restricts a model to only consider previous and current inputs in a sequence when processing a given query token when computing attention scores (compare that to standard self-attention that considers the entire sequence as seen above).</p>
<p>Causal self-attention masks out future tokens such that the are not taken into account when computing context vectors. In the diagram below, any token above the diagonal of the attention matrix is a future token that should not be taken into account.</p>
<p>For example, the token “Your” can only attend to the first token in the sequence (i.e.&nbsp;“Your”) while the third token “starts” can attend to all prior tokens as well, i.e.&nbsp;“Your”, “journey”, and “starts”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-19.png" class="img-fluid figure-img"></p>
<figcaption>Causal self-attention</figcaption>
</figure>
</div>
<p>The causal self-attention implementation will modify our previous self-attention implementation by introducing a mask to modify the attention weight matrix.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-20.png" class="img-fluid figure-img"></p>
<figcaption>Causal self-attention implementation sketch</figcaption>
</figure>
</div>
<p>The above implementation results in wasted computation since we still compute the full attention matrix and normalize twice (once in the softmax operation on the full attention matrix and once after masking out upper triangular entries). A more efficient implementation below relies on a property of the softmax function (where negative infinity entries in the attention matrix are essentially zero probability entries). Mathematically, this occurs because <span class="math inline">\(e^{-\infty}  \rightarrow 0\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-21.png" class="img-fluid figure-img"></p>
<figcaption>Causal self-attention implementation sketch efficient</figcaption>
</figure>
</div>
<div id="cell-14" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the attention scores.</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: Reuses the query and key weight matrices of the SelfAttention_v2 object from the previous</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">#       section for convenience</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>queries <span class="op">=</span> sa_v2.W_q(inputs)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> sa_v2.W_k(inputs)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>attn_scores <span class="op">=</span> queries <span class="op">@</span> keys.T</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">**</span> <span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention weights:</span><span class="ch">\n</span><span class="sc">{</span>attn_weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple mask with 0s above the main diagonal</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: torch.tril creates a lower triangular matrix with ones on and below the diagonal.</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> attn_scores.shape[<span class="dv">0</span>]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>mask_simple <span class="op">=</span> torch.tril(torch.ones(context_length, context_length)).<span class="bu">type</span>(torch.int64)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Simple mask:</span><span class="ch">\n</span><span class="sc">{</span>mask_simple<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mask with negative infinity entries for the upper triangular entries.</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: torch.triu creates an upper triangular matrix with ones on and above the diagonal.</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> attn_weights.shape[<span class="dv">0</span>]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.triu(torch.ones(context_length, context_length), diagonal<span class="op">=</span><span class="dv">1</span>).<span class="bu">type</span>(</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    torch.int64</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mask (for softmax):</span><span class="ch">\n</span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the mask to the attention scores.</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>attn_weights_masked <span class="op">=</span> attn_weights.masked_fill(mask.<span class="bu">bool</span>(), <span class="op">-</span>torch.inf)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Attention weights masked:</span><span class="ch">\n</span><span class="sc">{</span>attn_weights_masked<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize the masked attention weights.</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>attn_weights_masked_normalized <span class="op">=</span> torch.softmax(</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    attn_weights_masked <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">**</span> <span class="fl">0.5</span>, dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Attention weights masked and normalized:</span><span class="ch">\n</span><span class="sc">{</span>attn_weights_masked_normalized<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Attention weights:
tensor([[0.1825, 0.1568, 0.1576, 0.1657, 0.1796, 0.1577],
        [0.1852, 0.1554, 0.1562, 0.1655, 0.1814, 0.1563],
        [0.1852, 0.1554, 0.1562, 0.1654, 0.1814, 0.1563],
        [0.1756, 0.1611, 0.1615, 0.1662, 0.1740, 0.1616],
        [0.1804, 0.1589, 0.1594, 0.1655, 0.1765, 0.1593],
        [0.1760, 0.1605, 0.1610, 0.1664, 0.1749, 0.1612]],
       grad_fn=&lt;SoftmaxBackward0&gt;)

Simple mask:
tensor([[1, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1]])

Mask (for softmax):
tensor([[0, 1, 1, 1, 1, 1],
        [0, 0, 1, 1, 1, 1],
        [0, 0, 0, 1, 1, 1],
        [0, 0, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0]])

Attention weights masked:
tensor([[0.1825,   -inf,   -inf,   -inf,   -inf,   -inf],
        [0.1852, 0.1554,   -inf,   -inf,   -inf,   -inf],
        [0.1852, 0.1554, 0.1562,   -inf,   -inf,   -inf],
        [0.1756, 0.1611, 0.1615, 0.1662,   -inf,   -inf],
        [0.1804, 0.1589, 0.1594, 0.1655, 0.1765,   -inf],
        [0.1760, 0.1605, 0.1610, 0.1664, 0.1749, 0.1612]],
       grad_fn=&lt;MaskedFillBackward0&gt;)

Attention weights masked and normalized:
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5053, 0.4947, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3380, 0.3309, 0.3311, 0.0000, 0.0000, 0.0000],
        [0.2517, 0.2491, 0.2492, 0.2500, 0.0000, 0.0000],
        [0.2017, 0.1987, 0.1988, 0.1996, 0.2012, 0.0000],
        [0.1678, 0.1659, 0.1660, 0.1666, 0.1676, 0.1660]],
       grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
</div>
<section id="dropout-masking-additional-attention-weights" class="level2">
<h2 class="anchored" data-anchor-id="dropout-masking-additional-attention-weights">Dropout: Masking additional attention weights</h2>
<p>Dropout is a technique where randomly selected hidden layer units are ignored (or dropped out) which helps prevent overfitting during training because the model is not allowed to become overly reliant on any specific set of hidden layer units. Note that dropout is only used <em>during training</em> and disabled afterwards.</p>
<p>Dropout in self-attention is most commonly applied at two specific times: 1. after calculating the attention weights 2. after applying the attention weights to the value vectors</p>
<p>Here we’ll apply dropout after applying the attention weights to the value vectors (which is the more common variant in practice).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-22.png" class="img-fluid figure-img"></p>
<figcaption>Dropout in causal self-attention</figcaption>
</figure>
</div>
<div id="cell-16" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the dropout module (choose a dropout probability of 50%)</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some example data (a matrix of ones).</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>example <span class="op">=</span> torch.ones(<span class="dv">6</span>, <span class="dv">6</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Example:</span><span class="ch">\n</span><span class="sc">{</span>example<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: Applying dropout scales the outputs by a factor of 1/(1-p) during training. This means that</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">#       during evaluation the module simply computes an identity function. This is done to compensate</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">#       for the reduction of active elements and is crucial to maintain the overall balance of the</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co">#       attention weights as it ensures that the average influence of the attention mechanism remains</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">#       consistent during both the training and inference phases.</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># See https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dropout:</span><span class="ch">\n</span><span class="sc">{</span>dropout(example)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply dropout to the attention weights.</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dropout:</span><span class="ch">\n</span><span class="sc">{</span>dropout(attn_weights_masked_normalized)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Example:
tensor([[1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1.]])
Dropout:
tensor([[2., 2., 2., 2., 2., 2.],
        [0., 2., 0., 0., 0., 0.],
        [0., 0., 2., 0., 2., 0.],
        [2., 2., 0., 0., 0., 2.],
        [2., 0., 0., 0., 0., 2.],
        [0., 2., 0., 0., 0., 0.]])
Dropout:
tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.6622, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4982, 0.0000, 0.5000, 0.0000, 0.0000],
        [0.0000, 0.3974, 0.3975, 0.3993, 0.4024, 0.0000],
        [0.3355, 0.3319, 0.0000, 0.0000, 0.3353, 0.3320]],
       grad_fn=&lt;MulBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="a-compact-causal-attention-class" class="level2">
<h2 class="anchored" data-anchor-id="a-compact-causal-attention-class">A compact causal attention class</h2>
<div id="cell-18" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We want to ensure that our implementation works with batches of data (as produced by the</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># dataloader implemented in chapter 2).</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> torch.stack([inputs, inputs], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch shape: </span><span class="sc">{</span>batch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalAttention(nn.Module):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        d_in: <span class="bu">int</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        d_out: <span class="bu">int</span>,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        context_length: <span class="bu">int</span>,</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        dropout_prob: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cache d_out for later use.</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_out <span class="op">=</span> d_out</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the weight matrices.</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_q <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_k <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_v <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the dropout module.</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compared to the previous implementation, we now use a dropout layer.</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(p<span class="op">=</span>dropout_prob)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register a buffer for the mask.</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: Buffers are not trained and are not subject to gradient descent.</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The use of register_buffer in PyTorch is not strictly necessary for all use cases</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       but offers several advantages here. For instance, when we use the CausalAttention</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       class in our LLM, buffers are automatically moved to the appropriate device (CPU or</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       GPU) along with our model, which will be relevant when training our LLM. This means</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       we don’t need to manually ensure these tensors are on the same device as your model</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       parameters, avoiding device mismatch errors.</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mask"</span>, torch.triu(torch.ones(context_length, context_length), diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor, verbose: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract input dimensions.</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        batch_size, num_tokens, d_in <span class="op">=</span> x.shape</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Project input into query, key, and value vectors.</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.W_q(x)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.W_k(x)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.W_v(x)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the unnormalized attention scores.</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply the mask to the attention scores.</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: In PyTorch, operations with a trailing underscore are performed in-place, avoiding</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       unnecessary memory copies.</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>        attn_scores.masked_fill_(<span class="va">self</span>.mask.<span class="bu">bool</span>()[:num_tokens, :num_tokens], <span class="op">-</span>torch.inf)</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Unnormalized causal attention scores (shape: </span><span class="sc">{</span>attn_scores<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">):</span><span class="ch">\n</span><span class="sc">{</span>attn_scores<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize the attention scores.</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> <span class="va">self</span>.d_out<span class="op">**</span><span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Normalized causal attention weights (shape: </span><span class="sc">{</span>attn_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">):</span><span class="ch">\n</span><span class="sc">{</span>attn_weights<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply dropout to the attention weights.</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> <span class="va">self</span>.dropout(attn_weights)</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the context vectors.</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>        context_vectors <span class="op">=</span> attn_weights <span class="op">@</span> value</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Context vectors (shape: </span><span class="sc">{</span>context_vectors<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">):</span><span class="ch">\n</span><span class="sc">{</span>context_vectors<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vectors</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the causal attention class.</span></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> batch.shape[<span class="dv">1</span>]</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>ca <span class="op">=</span> CausalAttention(d_in, d_out, context_length, <span class="fl">0.0</span>)</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a>context_vecs <span class="op">=</span> ca(batch)</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vector shape: </span><span class="sc">{</span>context_vecs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch shape: torch.Size([2, 6, 3])
Context vector shape: torch.Size([2, 6, 2])</code></pre>
</div>
</div>
</section>
</section>
<section id="multi-head-self-attention-naive" class="level1">
<h1>Multi-head self-attention (naive)</h1>
<p>The term “multi-head” refers to dividing the attention mechanism into multiple “heads,” each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.</p>
<p>In a basic implementation of multi-head self-attention, one could just stack multiple causal self-attention modules as is done in the following figure and the MultiHeadAttentionWrapper below. Each head has its own weights and all heads’ outputs are combined by stacking the output tensors. This is a rather inefficient implementation since the individual heads are processed sequentially.</p>
<p>Using multiple instances of the self-attention mechanism can be computationally intensive, but it’s crucial for the kind of complex pattern recognition that models like transformer-based LLMs are known for.</p>
<p>As mentioned before, the main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections—the results of multiplying the input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-24.png" class="img-fluid figure-img"></p>
<figcaption>Naive multi-head self-attention</figcaption>
</figure>
</div>
<p>The example below shows how the individual heads compute their output and how the output tensors are stacked.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-25.png" class="img-fluid figure-img"></p>
<figcaption>Naive multi-head self-attention example</figcaption>
</figure>
</div>
<div id="cell-20" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttentionWrapper(nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        d_in: <span class="bu">int</span>,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        d_out: <span class="bu">int</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        context_length: <span class="bu">int</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        dropout: <span class="bu">float</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        num_heads: <span class="bu">int</span>,</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initialize the multi-head attention wrapper.</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">            d_in: The dimension of the input.</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">            d_out: The dimension of the output.</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co">            context_length: The length of the context. This argument sets the length of the causal</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">                mask, i.e. the maximum supported sequence length.</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="co">            dropout: The dropout probability.</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co">            num_heads: The number of attention heads.</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co">            qkv_bias: Whether to use a bias in the query, key, and value projections.</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList(</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: All heads are processed sequentially which is inefficient.</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([head(x) <span class="cf">for</span> head <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with a simple example.</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> batch.shape[<span class="dv">1</span>]  <span class="co"># This is the number of tokens</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>d_in, d_out <span class="op">=</span> <span class="dv">3</span>, <span class="dv">2</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadAttentionWrapper(</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    d_in<span class="op">=</span>d_in, d_out<span class="op">=</span>d_out, context_length<span class="op">=</span>context_length, dropout<span class="op">=</span><span class="fl">0.0</span>, num_heads<span class="op">=</span><span class="dv">2</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>context_vecs <span class="op">=</span> mha(batch)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: The shapes below indicate that the input batch (shape [2,6,3]) is transformed into a tensor</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a><span class="co">#       of shape [2,6,4] where the last dimension is the concatenation of the outputs of the two heads.</span></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Input size:                       [2,6,3] (last dim is d_in)</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Individual head output size:      [2,6,2] (last dim is d_out)</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenated head output size:    [2,6,4] (last dim is d_out * num_heads)</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vectors:</span><span class="ch">\n</span><span class="sc">{</span>context_vecs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch shape: </span><span class="sc">{</span>batch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Context vectors shape: </span><span class="sc">{</span>context_vecs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Context vectors:
tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]],

        [[-0.4519,  0.2216,  0.4772,  0.1063],
         [-0.5874,  0.0058,  0.5891,  0.3257],
         [-0.6300, -0.0632,  0.6202,  0.3860],
         [-0.5675, -0.0843,  0.5478,  0.3589],
         [-0.5526, -0.0981,  0.5321,  0.3428],
         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=&lt;CatBackward0&gt;)
Batch shape: torch.Size([2, 6, 3])
Context vectors shape: torch.Size([2, 6, 4])</code></pre>
</div>
</div>
</section>
<section id="multi-head-self-attention-parallel" class="level1">
<h1>Multi-head self-attention (parallel)</h1>
<p>One way to improve the efficiency of multi-head self-attention (MHA) is by computing the outputs for all attention heads simultaneously via matrix multiplication.</p>
<p>The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.</p>
<p>The below image compares the two implementations. In the MultiHeadAttentionWrapper class with two attention heads, we initialized two weight matrices, <span class="math inline">\(W_{q_1}\)</span> and <span class="math inline">\(W_{q_2}\)</span>, and computed two query matrices, <span class="math inline">\(Q_1\)</span> and <span class="math inline">\(Q_2\)</span> (top). In the MultiheadAttention class, we initialize one larger weight matrix <span class="math inline">\(W_q\)</span>, only perform one matrix multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into <span class="math inline">\(Q_1\)</span> and <span class="math inline">\(Q_2\)</span> (bottom).</p>
<p>The splitting of the query, key, and value tensors is achieved through tensor reshaping and transposing operations using PyTorch’s .view and .transpose methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://drek4537l1klr.cloudfront.net/raschka/Figures/3-26.png" class="img-fluid figure-img"></p>
<figcaption>Multi-head self-attention</figcaption>
</figure>
</div>
<div id="cell-22" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        d_in: <span class="bu">int</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        d_out: <span class="bu">int</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        context_length: <span class="bu">int</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        dropout: <span class="bu">float</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        num_heads: <span class="bu">int</span>,</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        qkv_bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initialize the multi-head attention class.</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">            d_in: The dimension of the input.</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co">            d_out: The dimension of the output.</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co">            context_length: The length of the context. This argument sets the length of the causal</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co">                mask, i.e. the maximum supported sequence length.</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co">            dropout: The dropout probability.</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co">            num_heads: The number of attention heads.</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co">            qkv_bias: Whether to use a bias in the query, key, and value projections.</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Verify that the output dimension is divisible by the number of heads, which is required</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># for splitting the output into the specified number of heads.</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> d_out <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"d_out must be divisible by num_heads"</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cache the output dimension and the number of heads for later use.</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: This reduces the projection dim to match the desired output dim.</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The key operation is to split the d_out dimension into num_heads and head_dim,</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       where head_dim = d_out / num_heads. This splitting is then achieved using the .view</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       (b, num_tokens, num_heads, head_dim).</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_out <span class="op">=</span> d_out</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> d_out <span class="op">//</span> num_heads</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the weight matrices.</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: Only a single weight matrix is initialized for the query, key, and value projections</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       since we will split the output into the specified number of heads.</span></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_q <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_k <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_v <span class="op">=</span> nn.Linear(d_in, d_out, bias<span class="op">=</span>qkv_bias)</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the output projection layer.</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: This implementation uses a Linear layer to combine all head outputs.</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(d_out, d_out)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the dropout layer.</span></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register a buffer for the mask.</span></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>            <span class="st">"mask"</span>, torch.triu(torch.ones(context_length, context_length), diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract input dimensions.</span></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The customary notation for these dimensions is [B, T, D] where:</span></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   B = batch size</span></span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   T = sequence length (num_tokens)</span></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   D = model dimension (d_in)</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>        b, num_tokens, d_in <span class="op">=</span> x.shape</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Project the input into query, key, and value vectors.</span></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The shape of the projected query, key, and value vectors is [B, T, D]</span></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> <span class="va">self</span>.W_k(x)  <span class="co"># [B, T, D]</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> <span class="va">self</span>.W_q(x)  <span class="co"># [B, T, D]</span></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> <span class="va">self</span>.W_v(x)  <span class="co"># [B, T, D]</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"keys.shape: </span><span class="sc">{</span>keys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"queries.shape: </span><span class="sc">{</span>queries<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"values.shape: </span><span class="sc">{</span>values<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split the query, key, and value vectors into the specified number of heads.</span></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: We implicitly split the matrix by adding a num_heads dimension. Then we unroll the</span></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim).</span></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute head dimension</span></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>        head_dim <span class="op">=</span> <span class="va">self</span>.d_out <span class="op">//</span> <span class="va">self</span>.num_heads</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"head_dim: </span><span class="sc">{</span>head_dim<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape to [B, T, H, D_h] where:</span></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   B = batch size</span></span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   T = sequence length (num_tokens)</span></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   H = number of heads</span></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>        <span class="co">#   D_h = dimension per head (head_dim)</span></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: We implicitly split the matrix by adding a num_heads dimension. Then we unroll the</span></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim).</span></span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: See section "A note on views" for more details.</span></span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> keys.view(b, num_tokens, <span class="va">self</span>.num_heads, head_dim)</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> values.view(b, num_tokens, <span class="va">self</span>.num_heads, head_dim)</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> queries.view(b, num_tokens, <span class="va">self</span>.num_heads, head_dim)</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"keys.shape: </span><span class="sc">{</span>keys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"values.shape: </span><span class="sc">{</span>values<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"queries.shape: </span><span class="sc">{</span>queries<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transpose from [B, T, H, D_h] to [B, H, T, D_h], i.e. from shape</span></span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)</span></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: This transposition is crucial for correctly aligning the queries, keys, and values</span></span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       across the different heads and performing batched matrix multiplications</span></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       efficiently.</span></span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: This reshaping results in each head having access to the full sequence of tokens</span></span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       (i.e. a tensor of shape T x D_h).</span></span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a>        keys <span class="op">=</span> keys.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># shape [B, H, T, D_h]</span></span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a>        queries <span class="op">=</span> queries.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># shape [B, H, T, D_h]</span></span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> values.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># shape [B, H, T, D_h]</span></span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"keys.shape: </span><span class="sc">{</span>keys<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"values.shape: </span><span class="sc">{</span>values<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"queries.shape: </span><span class="sc">{</span>queries<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the unnormalized attention scores via a dot product for each head.</span></span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: We transpose the T and D_h dimension (i.e. num_tokens and head_dim) just like we</span></span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       have already done in the "Trainable self-attention" section.</span></span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: Change key shape from [B, H, T, D_h] to [B, H, D_h, T]</span></span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The output shape of attn_scores is [B, H, T, T], i.e. a square matrix of size T</span></span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       (i.e. sequence length) for each head.</span></span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The following operation does a batched matrix multiplication between queries and</span></span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       keys. In this case, the matrix multiplication implementation in PyTorch handles the</span></span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       four-dimensional input tensor so that the matrix multiplication is carried out</span></span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       between the two last dimensions (num_tokens, head_dim) and then repeated for the</span></span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       individual heads.</span></span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: See section "A note on batched matrix multiplications" for more details.</span></span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a>        attn_scores <span class="op">=</span> queries <span class="op">@</span> keys.transpose(<span class="dv">2</span>, <span class="dv">3</span>)  <span class="co"># shape [B, H, T, T]</span></span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"attn_scores.shape: </span><span class="sc">{</span>attn_scores<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The mask is truncated to the number of tokens in the input sequence (i.e. sequence</span></span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a>        <span class="co"># length T)</span></span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a>        mask_bool <span class="op">=</span> <span class="va">self</span>.mask.<span class="bu">bool</span>()[:num_tokens, :num_tokens]  <span class="co"># shape [T, T]</span></span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"mask_bool.shape: </span><span class="sc">{</span>mask_bool<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply the mask to the attention scores.</span></span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a>        attn_scores.masked_fill_(mask_bool, <span class="op">-</span>torch.inf)  <span class="co"># shape [B, H, T, T]</span></span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the normalized attention weights (as before)</span></span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The scaling factor is the last dimension of the keys tensor (i.e. head_dim, see</span></span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       line 88).</span></span>
<span id="cb18-140"><a href="#cb18-140" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_scores <span class="op">/</span> keys.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">**</span> <span class="fl">0.5</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-141"><a href="#cb18-141" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> <span class="va">self</span>.dropout(attn_weights)  <span class="co"># shape [B, H, T, T]</span></span>
<span id="cb18-142"><a href="#cb18-142" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"attn_weights.shape: </span><span class="sc">{</span>attn_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-143"><a href="#cb18-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-144"><a href="#cb18-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the context vectors.</span></span>
<span id="cb18-145"><a href="#cb18-145" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The shapes of the individual tensors are:</span></span>
<span id="cb18-146"><a href="#cb18-146" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       - attn_weights: [B, H, T, T]</span></span>
<span id="cb18-147"><a href="#cb18-147" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       - values: [B, H, T, D_h]</span></span>
<span id="cb18-148"><a href="#cb18-148" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       - context_vec: [B, H, T, D_h]</span></span>
<span id="cb18-149"><a href="#cb18-149" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       - context_vec.transposed: [B, T, H, D_h]</span></span>
<span id="cb18-150"><a href="#cb18-150" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: The context vectors from all heads are transposed back to the shape</span></span>
<span id="cb18-151"><a href="#cb18-151" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       (b, num_tokens, num_heads, head_dim).</span></span>
<span id="cb18-152"><a href="#cb18-152" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> (attn_weights <span class="op">@</span> values).transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># shape [B, T, H, D_h]</span></span>
<span id="cb18-153"><a href="#cb18-153" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"context_vec.shape: </span><span class="sc">{</span>context_vec<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-154"><a href="#cb18-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-155"><a href="#cb18-155" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape/flatten the context vectors from [B, T, H, D_h] to [B, T, D_out], i.e. combine all</span></span>
<span id="cb18-156"><a href="#cb18-156" aria-hidden="true" tabindex="-1"></a>        <span class="co"># individual attention heads (d_out = H * D_h).</span></span>
<span id="cb18-157"><a href="#cb18-157" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: Combines heads, where self.d_out = self.num_heads * self.head_dim (see line 32).</span></span>
<span id="cb18-158"><a href="#cb18-158" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> context_vec.contiguous().view(b, num_tokens, <span class="va">self</span>.d_out)</span>
<span id="cb18-159"><a href="#cb18-159" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"context_vec.shape: </span><span class="sc">{</span>context_vec<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-160"><a href="#cb18-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-161"><a href="#cb18-161" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally project the context vectors to the output dimension.</span></span>
<span id="cb18-162"><a href="#cb18-162" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: This output projection layer is not strictly necessary (see appendix B of the book</span></span>
<span id="cb18-163"><a href="#cb18-163" aria-hidden="true" tabindex="-1"></a>        <span class="co">#       for more details), but it is commonly used in many LLM architectures.</span></span>
<span id="cb18-164"><a href="#cb18-164" aria-hidden="true" tabindex="-1"></a>        context_vec <span class="op">=</span> <span class="va">self</span>.out_proj(context_vec)</span>
<span id="cb18-165"><a href="#cb18-165" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> context_vec</span>
<span id="cb18-166"><a href="#cb18-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-167"><a href="#cb18-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-168"><a href="#cb18-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with a simple example.</span></span>
<span id="cb18-169"><a href="#cb18-169" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)</span>
<span id="cb18-170"><a href="#cb18-170" aria-hidden="true" tabindex="-1"></a>batch_size, context_length, d_in <span class="op">=</span> batch.shape</span>
<span id="cb18-171"><a href="#cb18-171" aria-hidden="true" tabindex="-1"></a>d_out <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-172"><a href="#cb18-172" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadAttention(d_in, d_out, context_length, <span class="fl">0.0</span>, num_heads<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-173"><a href="#cb18-173" aria-hidden="true" tabindex="-1"></a>context_vecs <span class="op">=</span> mha(batch)</span>
<span id="cb18-174"><a href="#cb18-174" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(context_vecs)</span>
<span id="cb18-175"><a href="#cb18-175" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"context_vecs.shape:"</span>, context_vecs.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>keys.shape: torch.Size([2, 6, 2])
queries.shape: torch.Size([2, 6, 2])
values.shape: torch.Size([2, 6, 2])
head_dim: 1
keys.shape: torch.Size([2, 6, 2, 1])
values.shape: torch.Size([2, 6, 2, 1])
queries.shape: torch.Size([2, 6, 2, 1])
keys.shape: torch.Size([2, 2, 6, 1])
values.shape: torch.Size([2, 2, 6, 1])
queries.shape: torch.Size([2, 2, 6, 1])
attn_scores.shape: torch.Size([2, 2, 6, 6])
mask_bool.shape: torch.Size([6, 6])
attn_weights.shape: torch.Size([2, 2, 6, 6])
context_vec.shape: torch.Size([2, 6, 2, 1])
context_vec.shape: torch.Size([2, 6, 2])
tensor([[[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]],

        [[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]]], grad_fn=&lt;ViewBackward0&gt;)
context_vecs.shape: torch.Size([2, 6, 2])</code></pre>
</div>
</div>
<section id="a-note-on-views" class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-views">A note on views</h2>
<div id="cell-24" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example for reshaping a tensor from [2, 3, 4] to [2, 3, 2, 2] (via views)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>B, T, D <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn((B, T, D))</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.shape)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>x_view <span class="op">=</span> x.view(B, T, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_view.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2, 3, 4])
torch.Size([2, 3, 2, 2])</code></pre>
</div>
</div>
</section>
<section id="a-note-on-batched-matrix-multiplications" class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-batched-matrix-multiplications">A note on batched matrix multiplications</h2>
<div id="cell-26" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.2745</span>, <span class="fl">0.6584</span>, <span class="fl">0.2775</span>, <span class="fl">0.8573</span>],</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.8993</span>, <span class="fl">0.0390</span>, <span class="fl">0.9268</span>, <span class="fl">0.7388</span>],</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.7179</span>, <span class="fl">0.7058</span>, <span class="fl">0.9156</span>, <span class="fl">0.4340</span>],</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.0772</span>, <span class="fl">0.3565</span>, <span class="fl">0.1479</span>, <span class="fl">0.5331</span>],</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.4066</span>, <span class="fl">0.2318</span>, <span class="fl">0.4545</span>, <span class="fl">0.9737</span>],</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.4606</span>, <span class="fl">0.5159</span>, <span class="fl">0.4220</span>, <span class="fl">0.5786</span>],</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a batched matrix multiplication between a and a.transpose(2, 3), i.e. num_tokens and</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co"># head_dim are transposed.</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: [1, 2, 3, 4] @ [1, 2, 4, 3] = [1, 2, 3, 3]</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: In this case, the matrix multiplication implementation in PyTorch handles the</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co">#       four-dimensional input tensor so that the matrix multiplication is carried out between the</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="co">#       two last dimensions (num_tokens, head_dim) and then repeated for the individual heads (as</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="co">#       well as for each batch separately, i.e. the first dimension which here is just one element).</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>aat <span class="op">=</span> a <span class="op">@</span> a.transpose(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of aat: </span><span class="sc">{</span>aat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(aat)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of aat: torch.Size([1, 2, 3, 3])
tensor([[[[1.3208, 1.1631, 1.2879],
          [1.1631, 2.2150, 1.8424],
          [1.2879, 1.8424, 2.0402]],

         [[0.4391, 0.7003, 0.5903],
          [0.7003, 1.3737, 1.0620],
          [0.5903, 1.0620, 0.9912]]]])</code></pre>
</div>
</div>
<div id="cell-27" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A less compact version of the above operation is as follows:</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>first_head <span class="op">=</span> a[<span class="dv">0</span>, <span class="dv">0</span>, :, :]</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>first_res <span class="op">=</span> first_head <span class="op">@</span> first_head.T</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First head:</span><span class="ch">\n</span><span class="st">"</span>, first_res)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>second_head <span class="op">=</span> a[<span class="dv">0</span>, <span class="dv">1</span>, :, :]</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>second_res <span class="op">=</span> second_head <span class="op">@</span> second_head.T</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Second head:</span><span class="ch">\n</span><span class="st">"</span>, second_res)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>First head:
 tensor([[1.3208, 1.1631, 1.2879],
        [1.1631, 2.2150, 1.8424],
        [1.2879, 1.8424, 2.0402]])

Second head:
 tensor([[0.4391, 0.7003, 0.5903],
        [0.7003, 1.3737, 1.0620],
        [0.5903, 1.0620, 0.9912]])</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/dpickem\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Daniel Pickem, 2025 - <a href="disclaimer">Disclaimer</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>
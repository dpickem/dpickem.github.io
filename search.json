[
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "",
    "text": "This notebook explores pretraining process of LLMs based on Sebastian Raschka’s book (Chapter 5). In particular, it discusses the following:\n\nComputing the training and validation set losses to assess the quality of LLM-generated text during training\nImplementing a training function and pretraining the LLM\nSaving and loading model weights to continue training an LLM\nLoading pretrained weights from OpenAI\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 5\n\nPytorch Lightning - great tutorial collection\n\n\n\n\nTopic overview\n\n\n\n# This installs the ipynb package which enables importing functions defined in other notebooks.\n# %pip install ipynb\n\n\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nfrom ipynb.fs.full.chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\nfrom ipynb.fs.full.chapter_02_dataset_creation import create_dataloader_v1\n\n\n# Instantiate the GPT-2 configuration with shortened context length.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=256,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.1,\n    qkv_bias=False,\n)\n\n\n# Create two training examples in a batch.\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nbatch = []\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\n\n\n# Test the GPT model.\ntorch.manual_seed(123)\n\n# Run the model on the batch.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\nout = model(batch)\n\nprint(f\"Input batch: {batch}\")\nprint(f\"Output shape: {out.shape}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#resources",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 5\n\nPytorch Lightning - great tutorial collection\n\n\n\n\nTopic overview\n\n\n\n# This installs the ipynb package which enables importing functions defined in other notebooks.\n# %pip install ipynb\n\n\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nfrom ipynb.fs.full.chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\nfrom ipynb.fs.full.chapter_02_dataset_creation import create_dataloader_v1\n\n\n# Instantiate the GPT-2 configuration with shortened context length.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=256,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.1,\n    qkv_bias=False,\n)\n\n\n# Create two training examples in a batch.\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nbatch = []\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\n\n\n# Test the GPT model.\ntorch.manual_seed(123)\n\n# Run the model on the batch.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\nout = model(batch)\n\nprint(f\"Input batch: {batch}\")\nprint(f\"Output shape: {out.shape}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#text-to-token-conversion",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#text-to-token-conversion",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Text to token conversion",
    "text": "Text to token conversion\n\ndef text_to_token_ids(\n    text: str, tokenizer: Optional[tiktoken.Encoding] = None\n) -&gt; torch.Tensor:\n    \"\"\"Convert a text string to a tensor of token IDs.\n\n    Args:\n        text: The text to convert to token IDs.\n        tokenizer: The tokenizer to use.\n\n    Returns:\n        torch.Tensor: A tensor of token IDs.\n    \"\"\"\n    # Instantiate a default tokenizer (if non was provided).\n    # Tokenize the input text.\n    encoded = tokenizer.encode(text, allowed_special={\"&lt;|endoftext|&gt;\"})\n\n    # Convert the tokenized text to a tensor.\n    # NOTE: .unsqueeze(0) adds the batch dimension.\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n    return encoded_tensor"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#token-to-text-conversion",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#token-to-text-conversion",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Token to text conversion",
    "text": "Token to text conversion\n\ndef token_ids_to_text(\n    token_ids: torch.Tensor, tokenizer: Optional[tiktoken.Encoding] = None\n) -&gt; str:\n    \"\"\"Convert a tensor of token IDs to a text string.\n\n    Args:\n        token_ids: The tensor of token IDs to convert to text.\n        tokenizer: The tokenizer to use.\n\n    Returns:\n        str: The text string.\n    \"\"\"\n    # Instantiate a default tokenizer (if non was provided).\n    # NOTE: .squeeze(0) removes the batch dimension.\n    flat = token_ids.squeeze(0)\n    return tokenizer.decode(flat.tolist())\n\n\n# Test the text to token conversion.\nstart_context = \"Every effort moves you\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M.context_length,\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#example---step-by-step",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#example---step-by-step",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Example - step by step",
    "text": "Example - step by step\n\n# Develop the loss function using a batch of two simple examples.\ninputs = torch.tensor(\n    [[16833, 3626, 6100], [40, 1107, 588]],  # [\"every effort moves\", \"I really like\"]\n)\n\n# Define the targets, which are the next tokens in the sequences.\ntargets = torch.tensor(\n    [\n        [3626, 6100, 345],\n        [1107, 588, 11311],\n    ]  # [\" effort moves you\", \" really like chocolate\"]\n)\n\n# Compute the logits for the inputs.\n# NOTE: We disable gradient computation since gradients are only used for training.\nwith torch.no_grad():\n    logits = model(inputs)\n\n# Compute the probabilities of each token in the vocabulary.\n# NOTE: The shape of probas is [B, T, V] where\n#\n# B is the batch size\n# T is the sequence length\n# V is the vocabulary size.\nprobas = torch.softmax(logits, dim=-1)\nprint(f\"Probas shape: {probas.shape}\")\n\n# Step 3 and 4: Convert the probabilities to token IDs via a greedy decoding strategy.\ntoken_ids = torch.argmax(probas, dim=-1, keepdim=True)\n# Print both batches of token IDs.\nprint(\"Token IDs:\\n\", token_ids)\n\n# Step 5: Convert the token IDs back to text.\nprint(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\nprint(f\"Outputs batch 1:\" f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n\n\n# For each of the two input texts, we can print the initial softmax probability scores\n# corresponding to the target tokens using the following code:\n\nbatch_idx = 0\n# TODO: Why can't we just use probas[batch_idx, :, targets[batch_idx]] since T = 3?\ntarget_probas_1 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]\nprint(f\"probas.shape: {probas.shape}\")\nprint(\"Text 1:\", target_probas_1)\n\nbatch_idx = 1\ntarget_probas_2 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]\nprint(\"Text 2:\", target_probas_2)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#computing-the-loss-step-by-step",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#computing-the-loss-step-by-step",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Computing the loss step by step",
    "text": "Computing the loss step by step\n\n\n\nLoss computation\n\n\n\n# Compute the log probabilities of the target tokens.\n# NOTE: Working with logarithms of probability scores is more manageable in mathematical\n#       optimization than handling the scores directly.\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\nprint(f\"log_probas: {log_probas}\")\n\n# Compute the average log probability of the target tokens.\navg_log_probas = torch.mean(log_probas)\nprint(f\"avg_log_probas: {avg_log_probas}\")\n\n# The goal is to get the average log probability as close to 0 as possible by updating the model’s\n# weights as part of the training process. However, in deep learning, the common practice isn’t to\n# push the average log probability up to 0 but rather to bring the negative average log probability\n# down to 0. The negative average log probability is simply the average log probability multiplied\n# by –1.\nneg_avg_log_probas = avg_log_probas * -1\nprint(f\"neg_avg_log_probas: {neg_avg_log_probas}\")\n\n\n# As we can see, the logits tensor has three dimensions: batch size, number of tokens, and\n# vocabulary size. The targets tensor has two dimensions: batch size and number of tokens.\n# For the cross_entropy loss function in PyTorch, we want to flatten these tensors by combining\n# them over the batch dimension:\nprint(\"Logits shape:\", logits.shape)\nprint(\"Targets shape:\", targets.shape)\n\nlogits_flat = logits.flatten(0, 1)\ntargets_flat = targets.flatten()\nprint(\"Flattened logits:\", logits_flat.shape)\nprint(\"Flattened targets:\", targets_flat.shape)\n\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\nprint(loss)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#the-difference-between-cross-entropy-perplexity-and-kl-divergence",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#the-difference-between-cross-entropy-perplexity-and-kl-divergence",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "The difference between cross-entropy, perplexity, and KL-divergence",
    "text": "The difference between cross-entropy, perplexity, and KL-divergence\n\nCross-entropy\nCross-entropy measures how well a predicted probability distribution \\(q\\) matches a true distribution \\(p\\). It’s defined as:\n\\[\nH(p, q) = -\\sum_{x} p(x) \\log q(x)\n\\]\nwhere \\(x\\) runs over all possible events. Intuitively, it’s the average number of bits needed to encode samples from \\(p\\), if they’re encoded according to \\(q\\). The lower the cross-entropy, the closer \\(q\\) is to \\(p\\).\nAccording to Wikipedia, in information theory, the cross-entropy between two probability distributions \\({\\displaystyle p}\\) and \\({\\displaystyle q}\\), over the same underlying set of events, measures the average number of bits needed to identify an event drawn from the set when the coding scheme used for the set is optimized for an estimated probability distribution \\({\\displaystyle q}\\), rather than the true distribution \\({\\displaystyle p}\\).\nThis statement reflects a fundamental idea from information theory: cross-entropy measures the cost of encoding data from one distribution \\(p\\) under the assumptions of another distribution \\(q\\). The unit “bits” arises because we’re working in the context of binary information encoding. Intuitively, each bit represents a yes/no choice, and the cross-entropy tells us, on average, how many such choices we’d need to make to encode the true outcomes from \\(p\\), given that our model assigns probabilities according to \\(q\\).\n\nIf \\(q\\) perfectly matches \\(p\\), the encoding is as efficient as possible—this is essentially the entropy \\(H(p)\\) of the true distribution.\n\nIf \\(q\\) differs from \\(p\\), the encoder based on \\(q\\) will make less informed decisions, leading to longer or more error-prone codes on average.\n\nThe “lower” cross-entropy means we’re closer to the ideal scenario where \\(q \\approx p\\), which indicates our model (represented by \\(q\\)) is doing a better job of approximating the true distribution \\(p\\).\n\nConversely, a higher cross-entropy indicates that \\(q\\) diverges significantly from \\(p\\), causing inefficiencies and increasing the average number of bits needed.\n\nSo, the cross-entropy not only quantifies the difference between two distributions, but also translates that difference into the practical costs of encoding data.\nExample:\n- True distribution: \\(p = [0.7, 0.2, 0.1]\\) - Predicted distribution 1: \\(q_1 = [0.6, 0.3, 0.1]\\) - Predicted distribution 2: \\(q_2 = [0.9, 0.05, 0.05]\\) - \\(H(p, q_1)\\) will be lower than \\(H(p, q_2)\\), because \\(q_1\\) is closer to \\(p\\) than \\(q_2\\).\n\n\nPerplexity\nPerplexity is often used in language modeling and other probabilistic models to measure how well a model predicts a sample. It’s defined as the exponentiated average negative log-probability:\n\\[\n\\text{Perplexity}(p, q) = 2^{H(p, q)}\n\\]\nThis represents the effective number of choices the model assigns to each outcome. A lower perplexity means the model is more confident in its predictions. Perplexity is often viewed as a normalized measure of cross-entropy, expressed in terms of the equivalent branching factor. For instance, if a language model’s perplexity is 10, it implies the model is, on average, as uncertain as making a single choice out of 10 equally likely outcomes.\nAccording to Wikipedia, in information theory, perplexity is a measure of uncertainty in the value of a sample from a discrete probability distribution. The larger the perplexity, the less likely it is that an observer can guess the value which will be drawn from the distribution.\nFrom Sebastian Raschka’s book:\nPerplexity is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence.\nPerplexity measures how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset. Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution. Perplexity can be calculated as perplexity = torch.exp(loss), which returns tensor(48725.8203) when applied to the previously calculated loss.\nPerplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step. In the given example, this would translate to the model being unsure about which among 48,725 tokens in the vocabulary to generate as the next token.\nChatGPT provides a similar intuitive explanation. If we consider a language model predicting the next word in a sentence, perplexity provides a numerical summary of how uncertain or “perplexed” the model is, on average, when choosing among possible outcomes. A perplexity value of 10, for example, indicates that the model’s uncertainty is equivalent to having 10 equally likely choices for each word it predicts. In other words, lower perplexity means the model is more confident in its predictions, as it can narrow down the possible outcomes to a smaller, more focused set. Higher perplexity indicates greater uncertainty or poorer model performance, since the model must spread its probability mass across more outcomes, essentially “considering” a larger range of possibilities before making a prediction.\nThis interpretation of perplexity as a kind of “average branching factor” makes it particularly useful in evaluating the quality of language models. Instead of dealing with abstract bits or logarithms (as in cross-entropy), perplexity translates the model’s predictive efficiency into a form that’s more intuitive.\nExample:\n- Suppose a language model predicts a sentence like “The cat sat on the ____” with probabilities for possible words:\n- \\(p(\\text{mat})\\) = 0.8, \\(p(\\text{floor})\\) = 0.15, \\(p(\\text{roof})\\) = 0.05 - If the true word is “mat” and the model’s probabilities closely match this, the perplexity will be low.\n- If the model assigns much lower probability to “mat” and higher to other options, the perplexity will increase, indicating worse predictions.\n\n\nKL Divergence (Kullback-Leibler Divergence)\nKL divergence measures how one probability distribution ( q ) diverges from a reference distribution ( p ). It’s given by:\n\\[\nD_{KL}(p \\parallel q) = \\sum_{x} p(x) \\log\\frac{p(x)}{q(x)}\n\\]\nKL divergence is always non-negative and equals zero only when ( p = q ). Unlike cross-entropy, it explicitly quantifies the “distance” (in an information-theoretic sense) between the two distributions. While cross-entropy tells us how many bits are needed to encode ( p ) using ( q ), KL divergence tells us how many extra bits are needed compared to using the true distribution ( p ) itself.\nExample:\n- True distribution: p = [0.5, 0.5] - Predicted distribution 1: \\(q_1\\) = [0.6, 0.4] - Predicted distribution 2: \\(q_2\\) = [0.9, 0.1] - \\(D_{KL}(p \\parallel q_1)\\) is smaller than \\(D_{KL}(p \\parallel q_2)\\), because \\(q_1\\) is closer to \\(p\\).\n- If \\(q_1\\) becomes equal to \\(p\\), the KL divergence will be zero.\n\n\nComparing the Concepts\n\nCross-Entropy vs. KL Divergence:\n\nCross-entropy combines the entropy of \\(p\\), which is fixed for a given \\(p\\), and the KL divergence from \\(p\\) to \\(q\\):\n\\[\nH(p, q) = H(p) + D_{KL}(p \\parallel q)\n\\]\nWhile cross-entropy measures the total coding cost under \\(q\\), KL divergence isolates the inefficiency due to \\(q\\)’s divergence from \\(p\\).\n\nPerplexity and Cross-Entropy:\n\nPerplexity is derived directly from cross-entropy, converting the measure into an interpretable “average number of choices.” It essentially provides a more human-readable version of the model’s performance.\n\nBoth low perplexity and low cross-entropy indicate a better model fit, but perplexity is the exponential form and gives a more intuitive sense of the model’s uncertainty.\n\nPerplexity and KL Divergence:\n\nWhile perplexity is connected to cross-entropy, KL divergence is a more nuanced measure that focuses on how much \\(q\\) deviates from \\(p\\) rather than the raw efficiency of encoding.\n\nPerplexity doesn’t directly measure divergence; instead, it measures how well the model predicts, which can be related to divergence indirectly through the cross-entropy.\n\n\nIn summary, cross-entropy and perplexity are practical metrics for evaluating how well a predictive model matches a true distribution, with perplexity offering a more intuitive interpretation. KL divergence, on the other hand, is a more fundamental information-theoretic measure that quantifies how much one distribution differs from another, forming a building block for understanding the inefficiencies captured by cross-entropy."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-cross-entropy-loss-for-a-given-batch.",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-cross-entropy-loss-for-a-given-batch.",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Utility function to compute the cross-entropy loss for a given batch.",
    "text": "Utility function to compute the cross-entropy loss for a given batch.\n\ndef calc_loss_batch(\n    input_batch: torch.Tensor,\n    target_batch: torch.Tensor,\n    model: nn.Module,\n    device: torch.device,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the cross-entropy loss for a given batch.\n\n    Args:\n        input_batch: The input batch.\n        target_batch: The target batch.\n        model: The model.\n        device: The device to compute the loss on.\n\n    Returns:\n        The cross-entropy loss for the input batch.\n    \"\"\"\n    # Transfer the input and target batches to the device.\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    # Compute the logits for the input batch.\n    logits = model(input_batch)\n\n    # Compute the cross-entropy loss for the input batch.\n    # NOTE: We flatten the logits and targets to have a shape of B * T where B is the batch size.\n    #\n    # logits: [B, T, V] -&gt; [B * T, V]\n    # targets: [B, T] -&gt; [B * T]\n    loss = torch.nn.functional.cross_entropy(\n        logits.flatten(0, 1), target_batch.flatten()\n    )\n    return loss"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-loss-for-a-data-loader",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-loss-for-a-data-loader",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Utility function to compute the loss for a data loader",
    "text": "Utility function to compute the loss for a data loader\n\ndef calc_loss_loader(\n    data_loader: torch.utils.data.DataLoader,\n    model: nn.Module,\n    device: torch.device,\n    num_batches: Optional[int] = None,\n) -&gt; float:\n    \"\"\"Compute the cross-entropy loss for a given data loader.\n\n    Args:\n        data_loader: The data loader.\n        model: The model.\n        device: The device to compute the loss on.\n        num_batches: The number of batches to compute the loss on.\n\n    Returns:\n        The cross-entropy loss for the entire data loader.\n    \"\"\"\n    total_loss = 0.0\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        # Iteratives over all batches if no fixed num_batches is specified\n        num_batches = len(data_loader)\n    else:\n        # Reduces the number of batches to match the total number of batches in the data loader if\n        # num_batches exceeds the number of batches in the data loader.\n        num_batches = min(num_batches, len(data_loader))\n\n    # Iterate over all batches in the data loader (or a subset thereof).\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i &lt; num_batches:\n            # Compute the loss for the input batch.\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n\n            # Sum the loss for each batch.\n            total_loss += loss.item()\n        else:\n            break\n\n    # Return the average loss over the number of batches.\n    return total_loss / num_batches\n\n\n# Test the loss computation.\n\n# If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any\n# changes to the code.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"Using device: {device}\")\n\n# Disables gradient tracking for efficiency because we are not training yet.\nwith torch.no_grad():\n    # Via the “device” setting, we ensure the data is loaded onto the same device as the LLM model.\n    train_loss = calc_loss_loader(train_loader, model, device)\n    val_loss = calc_loss_loader(val_loader, model, device)\n\nprint(f\"Training loss  : {train_loss}\")\nprint(f\"Validation loss: {val_loss}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#evaluation-utilities",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#evaluation-utilities",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Evaluation utilities",
    "text": "Evaluation utilities\n\ndef evaluate_model(\n    model: nn.Module,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_iter: int,\n):\n    \"\"\"Evaluate the model on the training and validation sets.\"\"\"\n\n    # Set the model to evaluation mode.\n    # NOTE: In evaluation mode, certain layers like dropout are disabled to ensure stable,\n    #       reproducible results.\n    model.eval()\n\n    # Disables gradient tracking, which is not required during evaluation (to reduce the\n    # computational overhead).\n    with torch.no_grad():\n        train_loss = calc_loss_loader(\n            train_loader, model, device, num_batches=eval_iter\n        )\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n\n    # Sets the model back to training mode.\n    model.train()\n\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(\n    model: nn.Module,\n    tokenizer: tiktoken.core.Encoding,\n    device: torch.device,\n    start_context: str,\n) -&gt; None:\n    \"\"\"Generate and print a sample from the model.\"\"\"\n    # Set the model to evaluation mode.\n    model.eval()\n\n    # Get the context size from the model's positional embedding weight.\n    context_size = model.pos_emb.weight.shape[0]\n\n    # Encode the start context and move to the device.\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    # Generate the text.\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n        )\n\n    # Decode the generated text.\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(decoded_text.replace(\"\\n\", \" \"))\n\n    # Set the model back to training mode.\n    model.train()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#pretraining-function",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#pretraining-function",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Pretraining function",
    "text": "Pretraining function\n\ndef train_model_simple(\n    model: nn.Module,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    start_context: str,\n    tokenizer: tiktoken.core.Encoding,\n    num_epochs: int = 10,\n    eval_freq: int = 5,\n    eval_iter: int = 5,\n):\n    # Initializes lists to track losses and tokens seen.\n    # TODO: Tracking of training statistics can be done more efficiently and elegantly.\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    for epoch in range(num_epochs):\n        model.train()\n\n        # Start the main training loop.\n        # Use tqdm to show progress with epoch and local step information\n        for local_step, (input_batch, target_batch) in enumerate(\n            tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n        ):\n            # Resets loss gradients from the previous batch iteration.\n            optimizer.zero_grad()\n\n            # Compute the loss for the input batch.\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n\n            # Calculates loss gradients.\n            loss.backward()\n\n            # Updates model weights using loss gradients\n            optimizer.step()\n\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            # Optional evaluation step.\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(\n                    f\"Ep {epoch+1} (Step {global_step:06d}, Batch {local_step+1}): \"\n                    f\"Train loss {train_loss:.3f}, \"\n                    f\"Val loss {val_loss:.3f}\"\n                )\n\n        # Prints a sample text after each epoch.\n        generate_and_print_sample(model, tokenizer, device, start_context)\n\n    return train_losses, val_losses, track_tokens_seen\n\n\n# Test the training loop.\ntorch.manual_seed(123)\n\n# Instantiate the model and move it to the device.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\n\n# Instantiate the optimizer.\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n\n# Train the model.\nnum_epochs = 10\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model,\n    train_loader,\n    val_loader,\n    optimizer,\n    device,\n    start_context=\"Every effort moves you\",\n    tokenizer=tokenizer,\n)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#plot-losses",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#plot-losses",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Plot losses",
    "text": "Plot losses\n\nfrom typing import Tuple\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\ndef plot_losses(\n    epochs_seen: torch.Tensor,\n    tokens_seen: torch.Tensor,\n    train_losses: torch.Tensor,\n    val_losses: torch.Tensor,\n    figsize: Tuple[int, int] = (8, 6),\n):\n    \"\"\"Plot the training and validation losses.\"\"\"\n    fig, ax1 = plt.subplots(figsize=figsize)\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2 = ax1.twiny()\n    ax2.plot(tokens_seen, train_losses, alpha=0)\n    ax2.set_xlabel(\"Tokens seen\")\n    fig.tight_layout()\n    plt.show()\n\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#temperature-sampling",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#temperature-sampling",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Temperature sampling",
    "text": "Temperature sampling\nTemperature scaling is a technique that adds a probabilistic selection process to the next-token generation task. Instead of always sampling the token with the highest probability as the next token using torch.argmax, also known as greedy decoding, we can replace argmax with a function that samples from a probability distribution (to generate text with more variety).\n\n\n\nTemperature sampling\n\n\n\n# Use a small sample vocabulary to illustrate temperature sampling.\nvocab = {\n    \"closer\": 0,\n    \"every\": 1,\n    \"effort\": 2,\n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5,\n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8,\n}\ninverse_vocab = {v: k for k, v in vocab.items()}\n\n# Assume the LLM generated the following logits for the next token, i.e. \"every effort moves you\".\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n\n# We convert the logits into probabilities via the softmax function and obtain the token ID\n# corresponding to the generated token via the argmax function, which we can then map back\n# into text via the inverse vocabulary:\nprobas = torch.softmax(next_token_logits, dim=0)\nnext_token_id = torch.argmax(probas).item()\nprint(f\"Greedy decoding: {inverse_vocab[next_token_id]}\")\n\n\n# Instead of greedy decoding via argmax, we can sample from the probability distribution\n# to generate text with more variety (via a multinomial distribution).\n\n# This is done via replacing the argmax with a sampling process from an multinomial\n# distribution.\ntorch.manual_seed(123)\nnext_token_id = torch.multinomial(probas, num_samples=1).item()\nprint(f\"Temperature sampling: {inverse_vocab[next_token_id]}\")\n\n\ndef print_sampled_tokens(probas: torch.Tensor, num_samples: int = 1_000):\n    \"\"\"Print the sampled tokens from the probability distribution.\"\"\"\n    torch.manual_seed(123)\n    sample = [\n        torch.multinomial(probas, num_samples=1).item() for i in range(num_samples)\n    ]\n\n    sampled_ids = torch.bincount(torch.tensor(sample))\n    for i, freq in enumerate(sampled_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\n\n\nprint_sampled_tokens(probas)\n\n\n# We can further control the distribution and selection process via a concept called temperature\n# scaling. Temperature scaling is just a fancy description for dividing the logits by a number\n# greater than 0.\ndef softmax_with_temperature(logits: torch.Tensor, temperature: float) -&gt; torch.Tensor:\n    \"\"\"Apply softmax with temperature scaling.\"\"\"\n    scaled_logits = logits / temperature\n    return torch.softmax(scaled_logits, dim=0)\n\n\n# Sample with original, lower, and higher confidence.\n# NOTE: In the plot below, temperature scaling manifests itself with sharper (lower temperatures)\n#       or more diffuse (higher temperatures) probability distributions.\n# NOTE: A temperature of 1 corresponds to no temperature scaling.\n# NOTE: Decreasing the temperature to 0.1 sharpens the distribution, so the most likely token\n#       (here, “forward”) will have an even higher probability score. Likewise, increasing the\n#       temperature to 5 makes the distribution more uniform.\n# NOTE: A temperature of 5 results in a more uniform distribution where other tokens are selected\n#       more often. This can add more variety to the generated texts but also more often results\n#       in nonsensical text.\ntemperatures = [1, 0.1, 5]\n\n# Temperature scaling the logits.\nscaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n\n# Plot the results.\nx = torch.arange(len(vocab))\nbar_width = 0.15\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i, T in enumerate(temperatures):\n    rects = ax.bar(\n        x + i * bar_width, scaled_probas[i], bar_width, label=f\"Temperature = {T}\"\n    )\n\nax.set_ylabel(\"Probability\")\nax.set_xticks(x)\nax.set_xticklabels(vocab.keys(), rotation=90)\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#top-k-sampling",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#top-k-sampling",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Top-k sampling",
    "text": "Top-k sampling\nNaive temperature sampling with higher temperatures leads to potentially more interesting and creative outputs. However, one downside of this approach is that it sometimes leads to grammatically incorrect or completely nonsensical outputs.\nTop-k sampling, when combined with probabilistic sampling and temperature scaling, can improve the text generation results. In top-k sampling, we can restrict the sampled tokens to the top-k most likely tokens and exclude all other tokens from the selection process by masking their probability scores.\nThe top-k approach replaces all nonselected logits with negative infinity value (-inf), such that when computing the softmax values, the probability scores of the non-top-k tokens are 0, and the remaining probabilities sum up to 1 (this is a similar masking trick as in the causal attention module).\n\n\n\nTop-k sampling\n\n\n\n# Assume the LLM generated the following logits for the next token, i.e. \"every effort moves you\".\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n\n# Define the top-k value.\ntop_k = 3\n\n# Get the top-k logits and their positions.\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\nprint(f\"Top logits: {top_logits}\")\nprint(f\"Top positions: {top_pos}\")\n\n# Subsequently, we apply PyTorch’s where function to set the logit values of tokens that are below\n# the lowest logit value within our top-three selection to negative infinity (-inf):\n\nnew_logits = torch.where(\n    # Identifies logits less than the minimum in the top 3.\n    condition=next_token_logits &lt; top_logits[-1],\n    # Assigns –inf to these lower logits.\n    input=torch.tensor(float(\"-inf\")),\n    # Keeps the original logits for the top-k tokens.\n    other=next_token_logits,\n)\nprint(f\"New logits (top-k sampling): {new_logits}\")\n\n# Apply the softmax function to turn these into next-token probabilities.\n# NOTE: We can now apply the temperature scaling and multinomial function for probabilistic\n#       sampling to select the next token among these three non-zero probability scores to\n#       generate the next token.\ntopk_probas = torch.softmax(new_logits, dim=0)\nprint(f\"Top-k probabilities: {topk_probas}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#an-updated-text-generation-function",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#an-updated-text-generation-function",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "An updated text generation function",
    "text": "An updated text generation function\n\ndef generate(\n    model: nn.Module,\n    idx: torch.Tensor,\n    max_new_tokens: int,\n    context_size: int,\n    temperature: float = 0.0,\n    top_k: Optional[int] = None,\n    eos_id: Optional[int] = None,\n):\n    \"\"\"Generate text with the model.\n\n    Args:\n        model: The model to use for generation.\n        idx: The input tokens.\n        max_new_tokens: The maximum number of tokens to generate.\n        context_size: The size of the context window.\n        temperature: The temperature to use for sampling.\n        top_k: The top-k value to use for sampling.\n        eos_id: The end-of-sequence token.\n\n    Returns:\n        The generated tokens.\n    \"\"\"\n    # The for loop is the same as before: gets logits and only focuses on the last time step.\n    # NOTE: Generate at most max_new_tokens tokens.\n    for _ in range(max_new_tokens):\n        # Only consider the last context_size tokens (this is typically informed by the model's\n        # supported context length or length of the positional embedding weight).\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n\n        # Only consider the last time step (i.e. the next token).\n        logits = logits[:, -1, :]\n\n        # Optionally filter logits with top_k sampling.\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits &lt; min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits\n            )\n\n        # Optionally apply temperature scaling.\n        if temperature &gt; 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            # If temperature is 0, use greedy decoding.\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        # Stop generating if we encounter the EOS (end of sequence) token.\n        if idx_next == eos_id:\n            break\n\n        # Concatenate the next token to the running sequence.\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n\n# Test the generation function.\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=model.to(device),  # Ensure model is on the correct device\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n    max_new_tokens=15,\n    context_size=GPT_CONFIG_124M.context_length,\n    top_k=15,\n    temperature=1.4,\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#without-the-optimizer-state",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#without-the-optimizer-state",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Without the optimizer state",
    "text": "Without the optimizer state\n\n# Saving a PyTorch model is relatively straightforward. The recommended way is to save a model’s\n# state_dict, a dictionary mapping each layer to its parameters, using the torch.save function:\n# NOTE: For the GPT2-124M model, this results in a file of roughly 623M.\ntorch.save(model.state_dict(), \"model.pth\")\n\n\n# Loading the model is equally straightforward. Note, however, that one needs to reinitialize the\n# model architecture first and then load the state_dict into an existing model instance:\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\n\n# NOTE: Set the model to evaluation mode since a model is most likely loaded for inference tasks\n#       (since the optimizer state is not saved/loaded to/from disk).\nmodel.eval()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#with-the-optimizer-state",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#with-the-optimizer-state",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "With the optimizer state",
    "text": "With the optimizer state\n\n# If we plan to continue pre-training a model later—for example, using the train_model_simple\n# function we defined earlier in this chapter—saving the optimizer state is also recommended.\n\n# Adaptive optimizers such as AdamW store additional parameters for each model weight. AdamW uses\n# historical data to adjust learning rates for each model parameter dynamically. Without it, the\n# optimizer resets, and the model may learn suboptimally or even fail to converge properly, which\n# means it will lose the ability to generate coherent text.\n# NOTE: For the GPT2-124M model, this results in a file of roughly 1.9G (or almost 3x the size of\n#       the model weights alone).\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"model_and_optimizer.pth\",\n)\n\n\n# Define the checkpoint to load the model and optimizer states from disk.\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n\n# Load the model and optimizer states from the checkpoint.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n# NOTE: Set the model to training mode since a model is most likely loaded for further training\n#       (since the optimizer state is saved/loaded to/from disk).\nmodel.train()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#creating-the-right-config",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#creating-the-right-config",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Creating the right config",
    "text": "Creating the right config\n\nfrom dataclasses import asdict\n\nasdict(GPT_CONFIG_124M)\n\n\n# Update the model configuration to conform to the model size.\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Instantiate a base config.\ntmp_config = asdict(GPT_CONFIG_124M)\n\n# Load the overlay parameters.\nmodel_name = \"gpt2-small (124M)\"\ntmp_config.update(model_configs[model_name])\n\n# Update the context length to match OpenAI's GPT-2 models.\ntmp_config.update({\"context_length\": 1024})\n\n# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n# working with pretrained weights, we need to match the settings for consistency and enable these\n# bias vectors.\ntmp_config.update({\"qkv_bias\": True})\n\n# Instantiate the new configuration.\nNEW_CONFIG = GPTConfig(**tmp_config)\n\n# Initialize the model with the new configuration.\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#loading-weights-into-the-model",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#loading-weights-into-the-model",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Loading weights into the model",
    "text": "Loading weights into the model\n\ndef assign(left, right):\n    \"\"\"Safely assign the right weight tensor to the left layer.\n\n    Checks whether two tensors or arrays (left and right) have the same dimensions or shape and\n    returns the right tensor as trainable PyTorch parameters.\n    \"\"\"\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n\n    return torch.nn.Parameter(torch.tensor(right))\n\n\ndef load_weights_into_gpt(gpt: GPTModel, params: dict):\n    # Sets the model’s positional and token embedding weights to those specified in params.\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n\n    # Iterates over each transformer block in the model.\n    for b in range(len(params[\"blocks\"])):\n        # The np.split function is used to divide the attention and bias weights into three equal\n        # parts for the query, key, and value components.\n        q_w, k_w, v_w = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.weight = assign(\n            gpt.trf_blocks[b].mha.W_q.weight, q_w.T\n        )\n        gpt.trf_blocks[b].mha.W_k.weight = assign(\n            gpt.trf_blocks[b].mha.W_k.weight, k_w.T\n        )\n        gpt.trf_blocks[b].mha.W_v.weight = assign(\n            gpt.trf_blocks[b].mha.W_v.weight, v_w.T\n        )\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.bias = assign(gpt.trf_blocks[b].mha.W_q.bias, q_b)\n        gpt.trf_blocks[b].mha.W_k.bias = assign(gpt.trf_blocks[b].mha.W_k.bias, k_b)\n        gpt.trf_blocks[b].mha.W_v.bias = assign(gpt.trf_blocks[b].mha.W_v.bias, v_b)\n        gpt.trf_blocks[b].mha.out_proj.weight = assign(\n            gpt.trf_blocks[b].mha.out_proj.weight,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].mha.out_proj.bias = assign(\n            gpt.trf_blocks[b].mha.out_proj.bias,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n        )\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].pre_attention_norm.scale = assign(\n            gpt.trf_blocks[b].pre_attention_norm.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_attention_norm.shift = assign(\n            gpt.trf_blocks[b].pre_attention_norm.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.scale = assign(\n            gpt.trf_blocks[b].pre_ff_norm.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.shift = assign(\n            gpt.trf_blocks[b].pre_ff_norm.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n        )\n\n        gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n        gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n\n        # The original GPT-2 model by OpenAI reused the token embedding weights in the output layer\n        # to reduce the total number of parameters, which is a concept known as weight tying.\n        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n\n\n# Load the weights into the model.\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\n\n\n# Test the model to verify that it can generate coherent text.\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=gpt,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=NEW_CONFIG.context_length,\n    top_k=50,\n    temperature=1.5,\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "This notebook explores the fine-tuning process of LLMs with the purpose of creating a classification model based on Sebastian Raschka’s book (Chapter 6). In particular, it discusses the following:\n\nIntroducing different LLM fine-tuning approaches\nPreparing a dataset for text classification\nModifying a pretrained LLM for fine-tuning\nFine-tuning an LLM to identify spam messages\nEvaluating the accuracy of a fine-tuned LLM classifier\nUsing a fine-tuned LLM to classify new data\n\n\n\n\nInstruction-tuned models can typically handle a broader range of tasks\nMore general approach that can handle multiple tasks\nBest suited for models that need to handle a variety of tasks based on complex user instructions\nThese models improve flexibility and interaction quality\nInstruction fine-tuning requires larger datasets and greater computational resources\n\n\n\n\n\nIdeal for projects requiring precise categorization into predefined classes (e.g. sentiment analysis or spam detection)\nSpecialized approach targeted at outputting a specific set of labels\nThe model is restricted to only the labels encountered during training\nRequires less data and compute power\n\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 6\n\n\n\n\n\nTopic overview\n\n\n\n# Install import-ipynb for importing ipynb files.\n# %pip install import-ipynb\n\n\nfrom typing import Optional, Tuple\nimport urllib.request\nimport zipfile\nimport os\nfrom pathlib import Path\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\nfrom chapter_02_dataset_creation import create_dataloader_v1\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\n\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n)\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#instruction-fine-tuning",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#instruction-fine-tuning",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "Instruction-tuned models can typically handle a broader range of tasks\nMore general approach that can handle multiple tasks\nBest suited for models that need to handle a variety of tasks based on complex user instructions\nThese models improve flexibility and interaction quality\nInstruction fine-tuning requires larger datasets and greater computational resources"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-fine-tuning",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-fine-tuning",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "Ideal for projects requiring precise categorization into predefined classes (e.g. sentiment analysis or spam detection)\nSpecialized approach targeted at outputting a specific set of labels\nThe model is restricted to only the labels encountered during training\nRequires less data and compute power"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#resources",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 6\n\n\n\n\n\nTopic overview\n\n\n\n# Install import-ipynb for importing ipynb files.\n# %pip install import-ipynb\n\n\nfrom typing import Optional, Tuple\nimport urllib.request\nimport zipfile\nimport os\nfrom pathlib import Path\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\nfrom chapter_02_dataset_creation import create_dataloader_v1\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\n\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n)\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#download-the-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#download-the-dataset",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Download the dataset",
    "text": "Download the dataset\n\nurl = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\nzip_path = Path(\"data/sms_spam_collection.zip\")\nextracted_path = Path(\"data/sms_spam_collection\")\ndata_file_path = extracted_path / \"SMSSpamCollection.tsv\"\n\n\ndef download_and_unzip_spam_data(\n    url: str, zip_path: Path, extracted_path: Path, data_file_path: Path\n):\n    \"\"\"Download and unzip the spam data from the UCI repository.\n\n    Args:\n        url: The URL of the zip file.\n        zip_path: The path to save the zip file.\n        extracted_path: The path to save the extracted files.\n        data_file_path: The path to save the data file.\n    \"\"\"\n    # Check if the file already exists.\n    if data_file_path.exists():\n        print(f\"{data_file_path} already exists. Skipping download \" \"and extraction.\")\n        return\n\n    # Download the zip file.\n    with urllib.request.urlopen(url) as response:\n        with open(zip_path, \"wb\") as out_file:\n            out_file.write(response.read())\n\n    # Extract the zip file.\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extracted_path)\n\n    # Add a .tsv extension to the file (tab-separated values).\n    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n    os.rename(original_file_path, data_file_path)\n    print(f\"File downloaded and saved as {data_file_path}\")\n\n\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#load-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#load-dataset",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Load dataset",
    "text": "Load dataset\n\n# Load data into a pandas DataFrame.\ndf = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n\n# Show the label count.\nprint(df[\"Label\"].value_counts())\n\n# Show a few examples.\ndf.head()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#balancing-the-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#balancing-the-dataset",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Balancing the dataset",
    "text": "Balancing the dataset\n\n# Create a balanced dataset by undersampling the majority class.\ndef create_balanced_dataset(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Create a balanced dataset by undersampling the majority class.\n\n    NOTE: This function can quite significantly reduce the size of the dataset.\n\n    Args:\n        df: The input DataFrame.\n\n    Returns:\n        A balanced DataFrame.\n    \"\"\"\n    num_spam = len(df[df[\"Label\"] == \"spam\"])\n    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n    return balanced_df.reset_index(drop=True)\n\n\n# Create a balanced dataset.\nbalanced_df = create_balanced_dataset(df)\nprint(balanced_df[\"Label\"].value_counts())\n\n# Convert string labels to integers.\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\nbalanced_df"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#splitting-the-datast",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#splitting-the-datast",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Splitting the datast",
    "text": "Splitting the datast\n\ndef random_split(\n    df: pd.DataFrame, train_frac: float, validation_frac: float\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Split a DataFrame into train, validation, and test sets.\n\n    NOTE: The size of the test set is implied to be the remainder of train and validation fraction\n          (all fractions should add up to 1).\n\n    Args:\n        df: The input DataFrame.\n        train_frac: The fraction of the dataset to use for training.\n        validation_frac: The fraction of the dataset to use for validation.\n\n    Returns:\n        A tuple of DataFrames for train, validation, and test sets.\n    \"\"\"\n    # Shuffle the entire DataFrame\n    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n\n    # Calculate split indices (for train and validation explicitly.)\n    train_end = int(len(df) * train_frac)\n    validation_end = train_end + int(len(df) * validation_frac)\n\n    # Split the DataFrame.\n    train_df = df[:train_end]\n    validation_df = df[train_end:validation_end]\n    test_df = df[validation_end:]\n\n    return train_df, validation_df, test_df\n\n\n# Test size is implied to be 0.2 as the remainder.\ntrain_df, validation_df, test_df = random_split(\n    df=balanced_df, train_frac=0.7, validation_frac=0.1\n)\nprint(f\"Train set size: {len(train_df)}\")\nprint(f\"Validation set size: {len(validation_df)}\")\nprint(f\"Test set size: {len(test_df)}\")\n\n# Save the DataFrames to CSV files.\ntrain_df.to_csv(extracted_path / \"train.csv\", index=None)\nvalidation_df.to_csv(extracted_path / \"validation.csv\", index=None)\ntest_df.to_csv(extracted_path / \"test.csv\", index=None)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-datasets",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-datasets",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Creating the datasets",
    "text": "Creating the datasets\nPreviously, we utilized a sliding window technique to generate uniformly sized text chunks, which we then grouped into batches for more efficient model training. Each chunk functioned as an individual training instance. However, we are now working with a spam dataset that contains text messages of varying lengths. To batch these messages as we did with the text chunks, we have two primary options:\n\nTruncate all messages to the length of the shortest message in the dataset or batch.\nPad all messages to the length of the longest message in the dataset or batch.\n\nThe first option is computationally cheaper, but it may result in significant information loss if shorter messages are much smaller than the average or longest messages, potentially reducing model performance. So, we opt for the second option, which preserves the entire content of all messages.\nTo implement batching, where all messages are padded to the length of the longest message in the dataset, we add padding tokens to all shorter messages. For this purpose, we use “&lt;|endoftext|&gt;” as a padding token. However, instead of appending the string “&lt;|endoftext|&gt;” to each of the text messages directly, we can add the token ID corresponding to “&lt;|endoftext|&gt;” to the encoded text messages\n\n\n\nPadding approach\n\n\nThe example below shows what a training batch looks like. A single training batch consisting of eight text messages represented as token IDs. Each text message consists of 120 token IDs. A class label array stores the eight class labels corresponding to the text messages, which can be either 0 (“not spam”) or 1 (“spam”).\n\n\n\nTraining batch example\n\n\n\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nprint(tokenizer.encode(\"&lt;|endoftext|&gt;\", allowed_special={\"&lt;|endoftext|&gt;\"}))\n\n\n# Creating a Dataset class.\nclass SpamDataset(Dataset):\n    \"\"\"Dataset class for the spam dataset.\"\"\"\n\n    def __init__(\n        self,\n        csv_file: Path,\n        tokenizer: tiktoken.Encoding,\n        max_length: Optional[int] = None,\n        pad_token_id: int = 50256,\n    ):\n        \"\"\"\n        Initializes the SpamDataset class.\n\n        Args:\n            csv_file: The path to the CSV file containing the data.\n            tokenizer: The tokenizer to use.\n            max_length: The maximum length of the encoded texts.\n            pad_token_id: The ID of the padding token.\n        \"\"\"\n        # Load the data from the CSV file.\n        self.data = pd.read_csv(csv_file)\n\n        # Pretokenize all texts.\n        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n\n        if max_length is None:\n            # If no maximum length is provided, use the longest encoded text.\n            self.max_length = self._longest_encoded_length()\n        else:\n            # Truncate sequences if they are longer than max_length.\n            self.max_length = max_length\n            self.encoded_texts = [\n                encoded_text[: self.max_length] for encoded_text in self.encoded_texts\n            ]\n\n        # Pads sequences to the longest sequence\n        self.encoded_texts = [\n            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n            for encoded_text in self.encoded_texts\n        ]\n\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        encoded = self.encoded_texts[index]\n        label = self.data.iloc[index][\"Label\"]\n        return (\n            torch.tensor(encoded, dtype=torch.long),\n            torch.tensor(label, dtype=torch.long),\n        )\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def _longest_encoded_length(self) -&gt; int:\n        \"\"\"Determine the longest encoded text length.\"\"\"\n        return max(len(encoded_text) for encoded_text in self.encoded_texts)\n\n\n# Load the training dataset.\ntrain_dataset = SpamDataset(\n    csv_file=extracted_path / \"train.csv\", max_length=None, tokenizer=tokenizer\n)\n\n# Load the validation and test sets and limit the max length to the same value as the training set.\n# NOTE: Importantly, any validation and test set samples exceeding the length of the longest\n#       training example are truncated using encoded_text[:self.max_length] in the SpamDataset code\n#       we defined earlier. This truncation is optional; you can set max_length=None for both\n#       validation and test sets, provided there are no sequences exceeding 1,024 tokens in these\n#       sets.\nval_dataset = SpamDataset(\n    csv_file=extracted_path / \"validation.csv\",\n    max_length=None,\n    tokenizer=tokenizer,\n)\ntest_dataset = SpamDataset(\n    csv_file=extracted_path / \"test.csv\",\n    max_length=None,\n    tokenizer=tokenizer,\n)\n\n# Show the maximum length of the encoded texts.\nprint(f\"Maximum length of the encoded texts: {train_dataset.max_length}\")\nprint(f\"Maximum length of the encoded texts: {val_dataset.max_length}\")\nprint(f\"Maximum length of the encoded texts: {test_dataset.max_length}\")\n\n# Verify that the maximum length does not exceed the context length.\nassert train_dataset.max_length &lt;= GPT_CONFIG_124M.context_length\nassert val_dataset.max_length &lt;= GPT_CONFIG_124M.context_length\nassert test_dataset.max_length &lt;= GPT_CONFIG_124M.context_length"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-data-loaders",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-data-loaders",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Creating the data loaders",
    "text": "Creating the data loaders\n\n# This num_worker setting ensures compatibility with most computers.\nnum_workers = 0\nbatch_size = 8\ntorch.manual_seed(123)\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    drop_last=True,\n)\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\ntest_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\n\n# Show the size of the data loaders.\nprint(f\"Train set size: {len(train_loader)}\")\nprint(f\"Validation set size: {len(val_loader)}\")\nprint(f\"Test set size: {len(test_loader)}\")\n\n# Show the first batch of the training set.\n# NOTE: As we can see, the input batches consist of eight training examples with 120 tokens each,\n#       as expected. The label tensor stores the class labels corresponding to the eight training\n#       examples.\nprint(\"\\nFirst training batch:\")\nfor input_batch, target_batch in train_loader:\n    print(\"Input batch dimensions:\", input_batch.shape)\n    print(\"Label batch dimensions\", target_batch.shape)\n    break"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#initializing-a-model-with-pretrained-weights",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#initializing-a-model-with-pretrained-weights",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Initializing a model with pretrained weights",
    "text": "Initializing a model with pretrained weights\n\nimport dataclasses\n\n# Load the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Update the model configuration to conform to the model size.\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Instantiate a base config.\ntmp_config = dataclasses.asdict(GPT_CONFIG_124M)\n\n# Load the overlay parameters.\nmodel_name = \"gpt2-small (124M)\"\ntmp_config.update(model_configs[model_name])\n\n# Update the context length to match OpenAI's GPT-2 models.\ntmp_config.update({\"context_length\": 1024})\n\n# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n# working with pretrained weights, we need to match the settings for consistency and enable these\n# bias vectors.\ntmp_config.update({\"qkv_bias\": True})\n\n# Instantiate the new configuration.\nNEW_CONFIG = GPTConfig(**tmp_config)\n\n# Initialize the model with the new configuration.\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()\n\n\n# NOTE: This code is copied from chapter_05_pretraining_on_unlabeled_data.ipynb because the import\n#       from load_weights_into_gpt.py is not working.\n\n\ndef assign(left, right):\n    \"\"\"Safely assign the right weight tensor to the left layer.\n\n    Checks whether two tensors or arrays (left and right) have the same dimensions or shape and\n    returns the right tensor as trainable PyTorch parameters.\n    \"\"\"\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n\n    return torch.nn.Parameter(torch.tensor(right))\n\n\ndef load_weights_into_gpt(gpt: GPTModel, params: dict):\n    # Sets the model’s positional and token embedding weights to those specified in params.\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n\n    # Iterates over each transformer block in the model.\n    for b in range(len(params[\"blocks\"])):\n        # The np.split function is used to divide the attention and bias weights into three equal\n        # parts for the query, key, and value components.\n        q_w, k_w, v_w = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.weight = assign(\n            gpt.trf_blocks[b].mha.W_q.weight, q_w.T\n        )\n        gpt.trf_blocks[b].mha.W_k.weight = assign(\n            gpt.trf_blocks[b].mha.W_k.weight, k_w.T\n        )\n        gpt.trf_blocks[b].mha.W_v.weight = assign(\n            gpt.trf_blocks[b].mha.W_v.weight, v_w.T\n        )\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.bias = assign(gpt.trf_blocks[b].mha.W_q.bias, q_b)\n        gpt.trf_blocks[b].mha.W_k.bias = assign(gpt.trf_blocks[b].mha.W_k.bias, k_b)\n        gpt.trf_blocks[b].mha.W_v.bias = assign(gpt.trf_blocks[b].mha.W_v.bias, v_b)\n        gpt.trf_blocks[b].mha.out_proj.weight = assign(\n            gpt.trf_blocks[b].mha.out_proj.weight,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].mha.out_proj.bias = assign(\n            gpt.trf_blocks[b].mha.out_proj.bias,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n        )\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].pre_attention_norm.scale = assign(\n            gpt.trf_blocks[b].pre_attention_norm.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_attention_norm.shift = assign(\n            gpt.trf_blocks[b].pre_attention_norm.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.scale = assign(\n            gpt.trf_blocks[b].pre_ff_norm.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.shift = assign(\n            gpt.trf_blocks[b].pre_ff_norm.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n        )\n\n        gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n        gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n\n        # The original GPT-2 model by OpenAI reused the token embedding weights in the output layer\n        # to reduce the total number of parameters, which is a concept known as weight tying.\n        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n\n\n# Download the GPT-2 weights.\nsettings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n\n# Load the weights into the model.\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\n\n\n# Test the model to verify that it can generate coherent text.\ntext_1 = \"Every effort moves you\"\ntoken_ids = generate_text_simple(\n    model=gpt.to(device),\n    idx=text_to_token_ids(text_1, tokenizer).to(device),\n    max_new_tokens=15,\n    context_size=NEW_CONFIG.context_length,\n)\nprint(token_ids_to_text(token_ids, tokenizer))\n\n\n# Check if the model is already capable of classifying spam and ham messages via instruction\n# examples.\ntext_2 = (\n    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n    \" 'You are a winner you have been specially\"\n    \" selected to receive $1000 cash or a $2000 award.'\"\n)\ntoken_ids = generate_text_simple(\n    model=gpt.to(device),\n    idx=text_to_token_ids(text_2, tokenizer).to(device),\n    max_new_tokens=23,\n    context_size=NEW_CONFIG.context_length,\n)\nprint(token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#modify-model-for-fine-tuning-adding-a-classification-head",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#modify-model-for-fine-tuning-adding-a-classification-head",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Modify model for fine-tuning (adding a classification head)",
    "text": "Modify model for fine-tuning (adding a classification head)\nAdapting a GPT model for spam classification by altering its architecture. Initially, the model’s linear output layer mapped 768 hidden units to a vocabulary of 50,257 tokens. To detect spam, we replace this layer with a new output layer that maps the same 768 hidden units to just two classes, representing “spam” and “not spam.”\n\nFine-tuning selected layers vs. all layers\nSince we start with a pretrained model, it’s not necessary to fine-tune all model layers. In neural network-based language models, the lower layers generally capture basic language structures and semantics applicable across a wide range of tasks and datasets. So, fine-tuning only the last layers (i.e., layers near the output), which are more specific to nuanced linguistic patterns and task-specific features, is often sufficient to adapt the model to new tasks. A nice side effect is that it is computationally more efficient to fine-tune only a small number of layers.\n\n\n\nModel modification\n\n\n\n# Prepare the model for fine-tuning.\n\n# 1. Freeze all parameters in the model.\nfor param in gpt.parameters():\n    param.requires_grad = False\n\n# 2. Replace the final linear layer with a new one for the two classes.\ntorch.manual_seed(123)\nnum_classes = 2\ngpt.out_head = nn.Linear(GPT_CONFIG_124M.emb_dim, num_classes)\n\n# Mark additional layers as trainable, in particular the last transformer block as well as the\n# final layer norm.\nfor param in gpt.trf_blocks[-1].parameters():\n    param.requires_grad = True\nfor param in gpt.final_norm.parameters():\n    param.requires_grad = True\n\n\n# Try running the model with a random input to see that it is working.\ninputs_str = \"Do you have time\"\ninputs = tokenizer.encode(inputs_str)\ninputs = torch.tensor(inputs).unsqueeze(0)\nprint(\"Inputs:\", inputs_str)\nprint(\"Inputs dimensions:\", inputs.shape)  # B x T, i.e. batch size x sequence length\n\nwith torch.no_grad():\n    outputs = gpt.to(device)(inputs.to(device))\n\n# NOTE: The output shape is B x T x 2, i.e. batch size x sequence length x number of classes.\n#       The model produces logits for each class and for each token in the input sequence.\n# NOTE: We are interested in fine-tuning this model to return a class label indicating whether a\n#       model input is “spam” or “not spam.” We don’t need to fine-tune all four output rows;\n#       instead, we can focus on a single output token. In particular, we will focus on the last\n#       row corresponding to the last output token.\nprint(\"Outputs:\\n\", outputs)\nprint(\n    \"Outputs dimensions:\", outputs.shape\n)  # B x T x 2, i.e. batch size x sequence length x number of classes\nprint(\"Last output token:\", outputs[:, -1, :])\n\n\n\nSelecting the right output for fine-tuning\n\n\n\nOutput selection\n\n\nTo understand why we are particularly interested in the last output token only let’s take a look at the attention mechanism. We have already explored the attention mechanism, which establishes a relationship between each input token and every other input token, and the concept of a causal attention mask. This mask restricts a token’s focus to its current position and the those before it, ensuring that each token can only be influenced by itself and the preceding tokens (as shown below).\nThe empty cells indicate masked positions due to the causal attention mask, preventing tokens from attending to future tokens. The values in the cells represent attention scores; the last token, time, is the only one that computes attention scores for all preceding tokens.\nThe last token in a sequence accumulates the most information since it is the only token with access to data from all the previous tokens. Therefore, in our spam classification task, we focus on this last token during the fine-tuning process.\n\n\n\nOutput selection"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#evaluation-utilities",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#evaluation-utilities",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Evaluation utilities",
    "text": "Evaluation utilities\nSimilar to next token prediction, we use softmax to compute probabilities for the output logits, in particular, probabilities for each class (spam, not spam) - as shown below.\n\n\n\nComputing classification probabilities\n\n\n\n# Compute the probabilities for the last output token.\nprobas = torch.softmax(outputs[:, -1, :], dim=-1)\n\n# Compute the class label.\n# NOTE: {\"ham\": 0, \"spam\": 1}\nlabel = torch.argmax(probas)\nprint(\"Inputs:\", inputs_str)\nprint(\"Class label:\", label.item())\n\n\n# A utility function for computing classification accuracy for a data loader.\ndef calc_accuracy_loader(\n    data_loader: DataLoader,\n    model: GPTModel,\n    device: torch.device,\n    num_batches: int = None,\n) -&gt; float:\n    \"\"\"Compute the accuracy of a model on a data loader.\n\n    Args:\n        data_loader: The data loader to compute the accuracy on.\n        model: The model to compute the accuracy on.\n        device: The device to compute the accuracy on.\n        num_batches: The number of batches to compute the accuracy on. Defaults to None.\n\n    Returns:\n        The accuracy of the model on the data loader.\n    \"\"\"\n    # Set the model to evaluation mode (to avoid tracking gradients).\n    model.eval()\n\n    # Initialize the number of correct predictions and the number of examples.\n    correct_predictions, num_examples = 0, 0\n\n    # If the number of batches is not specified, use all batches.\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n\n    # Iterate over the data loader.\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        # If the number of batches has not been reached, compute the accuracy.\n        if i &lt; num_batches:\n            input_batch = input_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            # Compute the logits for the last output token.\n            with torch.no_grad():\n                # NOTE: The output shape is B x T x 2, i.e. batch size x sequence length x number\n                #       of classes. Here, we are only interested in the logits for the last output\n                #       token.\n                logits = model(input_batch)[:, -1, :]\n\n            # Compute the predicted labels.\n            # NOTE: dim=-1 computes the argmax over the classes.\n            predicted_labels = torch.argmax(logits, dim=-1)\n\n            # Update the number of examples and the number of correct predictions.\n            num_examples += predicted_labels.shape[0]\n            correct_predictions += (predicted_labels == target_batch).sum().item()\n        else:\n            break\n\n    return correct_predictions / num_examples\n\n\n# Compute baseline accuracy for the not yet fine-tuned model.\n\n# Move the model to the device.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpt.to(device)\ntorch.manual_seed(123)\n\n# Compute the accuracy for the training, validation, and test sets.\ntrain_accuracy = calc_accuracy_loader(train_loader, gpt, device, num_batches=10)\nval_accuracy = calc_accuracy_loader(val_loader, gpt, device, num_batches=10)\ntest_accuracy = calc_accuracy_loader(test_loader, gpt, device, num_batches=10)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n\n\nDefine the loss function\nHowever, before we begin fine-tuning the model, we must define the loss function we will optimize during training. Our objective is to maximize the spam classification accuracy of the model, which means that the preceding code should output the correct class labels: 0 for non-spam and 1 for spam. Because classification accuracy is not a differentiable function, we use cross-entropy loss as a proxy to maximize accuracy.\n\ndef calc_loss_batch(\n    input_batch: torch.Tensor,\n    target_batch: torch.Tensor,\n    model: GPTModel,\n    device: torch.device,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for a batch of inputs and targets.\n\n    Args:\n        input_batch: The input batch.\n        target_batch: The target batch.\n        model: The model.\n        device: The device.\n\n    Returns:\n        The loss for the batch.\n    \"\"\"\n    # Move the input and target batches to the device.\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    # Compute the logits for the last output token.\n    logits = model(input_batch)[:, -1, :]\n\n    # Compute the loss (only for the last output token).\n    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n\n    return loss\n\n\ndef calc_loss_loader(\n    data_loader: DataLoader,\n    model: GPTModel,\n    device: torch.device,\n    num_batches: int = None,\n) -&gt; float:\n    \"\"\"Compute the loss for a data loader.\n\n    Args:\n        data_loader: The data loader.\n        model: The model.\n        device: The device.\n        num_batches: The number of batches to compute the loss on.\n\n    Returns:\n        The loss for the data loader.\n    \"\"\"\n    # Initialize the total loss.\n    total_loss = 0.0\n\n    # If the data loader is empty, return NaN.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    # If the number of batches is not specified, use all batches.\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n\n    # Iterate over the data loader.\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i &lt; num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n\n    return total_loss / num_batches\n\n\n# Compute the loss for the training, validation, and test sets.\n\n# Disables gradient tracking for efficiency because we are not training yet\nwith torch.no_grad():\n    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=5)\n    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches=5)\n    test_loss = calc_loss_loader(test_loader, gpt, device, num_batches=5)\n\nprint(f\"Training loss: {train_loss:.3f}\")\nprint(f\"Validation loss: {val_loss:.3f}\")\nprint(f\"Test loss: {test_loss:.3f}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#loss-visualization",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#loss-visualization",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Loss visualization",
    "text": "Loss visualization\nThe model’s training and validation loss over the five training epochs. Both the training loss, represented by the solid line, and the validation loss, represented by the dashed line, sharply decline in the first epoch and gradually stabilize toward the fifth epoch. This pattern indicates good learning progress and suggests that the model learned from the training data while generalizing well to the unseen validation data.\n\n# Plot the training and validation losses.\nimport matplotlib.pyplot as plt\n\n\ndef plot_values(\n    epochs_seen: torch.Tensor,\n    examples_seen: torch.Tensor,\n    train_values: list[float],\n    val_values: list[float],\n    label: str = \"loss\",\n):\n    \"\"\"Plot the training and validation losses.\n\n    Args:\n        epochs_seen: The number of epochs seen.\n        examples_seen: The number of examples seen.\n        train_values: The training values.\n        val_values: The validation values.\n        label: The label for the plot.\n    \"\"\"\n    # Create the plot.\n    fig, ax1 = plt.subplots(figsize=(8, 6))\n\n    # Plot the training and validation losses against the epochs.\n    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n\n    # Set the x-axis label.\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(label.capitalize())\n    ax1.legend()\n\n    # Creates a second x-axis for examples seen\n    ax2 = ax1.twiny()\n\n    # Invisible plot for aligning ticks\n    ax2.plot(examples_seen, train_values, alpha=0)\n    ax2.set_xlabel(\"Examples seen\")\n\n    # Adjusts layout to make room.\n    fig.tight_layout()\n\n    # Save the plot.\n    plt.savefig(f\"{label}-plot.pdf\")\n\n    # Show the plot.\n    plt.show()\n\n\n# Create the epochs and examples seen tensors.\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n\n# Plot the values.\nplot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-accuracy-plot",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-accuracy-plot",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Classification accuracy plot",
    "text": "Classification accuracy plot\nBoth the training accuracy (solid line) and the validation accuracy (dashed line) increase substantially in the early epochs and then plateau, achieving almost perfect accuracy scores of 1.0. The close proximity of the two lines throughout the epochs suggests that the model does not overfit the training data very much.\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n\nplot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#performance-metrics-on-all-data-sets",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#performance-metrics-on-all-data-sets",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Performance metrics on all data sets",
    "text": "Performance metrics on all data sets\nThe training and test set performances are almost identical. The slight discrepancy between the training and test set accuracies suggests minimal overfitting of the training data. Typically, the validation set accuracy is somewhat higher than the test set accuracy because the model development often involves tuning hyperparameters to perform well on the validation set, which might not generalize as effectively to the test set. This situation is common, but the gap could potentially be minimized by adjusting the model’s settings, such as increasing the dropout rate (drop_rate) or the weight_decay parameter in the optimizer configuration.\n\ntrain_accuracy = calc_accuracy_loader(train_loader, gpt, device)\nval_accuracy = calc_accuracy_loader(val_loader, gpt, device)\ntest_accuracy = calc_accuracy_loader(test_loader, gpt, device)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#using-the-model-for-classification",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#using-the-model-for-classification",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Using the model for classification",
    "text": "Using the model for classification\n\n\n\nModel usage\n\n\n\ndef classify_review(\n    text: str,\n    model: GPTModel,\n    tokenizer: tiktoken.Encoding,\n    device: torch.device,\n    max_length: Optional[int] = None,\n    pad_token_id: int = 50256,\n):\n    \"\"\"Classify a review using a fine-tuned GPT model.\n\n    Args:\n        text: The review to classify.\n        model: The fine-tuned GPT model.\n        tokenizer: The tokenizer.\n        device: The device to classify the review on.\n        max_length: The maximum length of the review.\n        pad_token_id: The padding token ID (defaults to the end-of-text token).\n\n    Returns:\n        The predicted label.\n    \"\"\"\n    model.eval()\n    # Prepares inputs to the model\n    input_ids = tokenizer.encode(text)\n\n    # Determine the maximum supported context length.\n    supported_context_length = model.pos_emb.weight.shape[1]\n\n    # Truncates sequences if they are too long\n    input_ids = input_ids[: min(max_length, supported_context_length)]\n\n    # Determine the maximum sequence length.\n    max_length = max_length if max_length is not None else supported_context_length\n\n    # Pad sequences to the longest sequence length.\n    input_ids += [pad_token_id] * (max_length - len(input_ids))\n\n    # Add a batch dimension.\n    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n\n    # Model inference without gradient tracking.\n    with torch.no_grad():\n        # Logits for the last output token.\n        logits = model(input_tensor)[:, -1, :]\n\n    predicted_label = torch.argmax(logits, dim=-1).item()\n\n    # Return the predicted label.\n    return \"spam\" if predicted_label == 1 else \"not spam\"\n\n\n# Try classifying some examples.\ntext_1 = (\n    \"You are a winner you have been specially\"\n    \" selected to receive $1000 cash or a $2000 award.\"\n)\nprint(\n    classify_review(\n        text=text_1,\n        model=gpt,\n        tokenizer=tokenizer,\n        device=device,\n        max_length=train_dataset.max_length,\n    )\n)\n\ntext_2 = (\n    \"Hey, just wanted to check if we're still on\" \" for dinner tonight? Let me know!\"\n)\nprint(\n    classify_review(\n        text=text_2,\n        model=gpt,\n        tokenizer=tokenizer,\n        device=device,\n        max_length=train_dataset.max_length,\n    )\n)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#save-the-model-checkpoint-to-disk",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#save-the-model-checkpoint-to-disk",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Save the model checkpoint to disk",
    "text": "Save the model checkpoint to disk\n\n# Save the model checkpoint to disk.\ntorch.save(gpt.state_dict(), \"review_classifier.pth\")\n\n\n# Load the model checkpoint from disk.\nmodel_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\ngpt.load_state_dict(model_state_dict)"
  },
  {
    "objectID": "posts/2025_06_04_robust_autonomy_emerges_from_self_play/index.html",
    "href": "posts/2025_06_04_robust_autonomy_emerges_from_self_play/index.html",
    "title": "Robust Autonomy Emerges from Self-Play",
    "section": "",
    "text": "A few weeks ago I came across a paper titled “Robust Autonomy Emerges from Self-Play” in the TLDR newsletter (which is worth subscribing to if you want to stay on top of the latest news in AI, but that’s not the point of this post).\nThe above paper was interesting for many reasons not the least of which was a sentimental one. It was published by former colleagues of mine at Apple and appears to be the latest (and last?) public artifact of Apples self-driving efforts.\nOver ten authors contributed to the paper, but I’ve only had the pleasure of (tangentially) working with the following:\n\nBrody Huval\nStuart Bowers\nPhilipp Krähenbühl\nVladlen Koltun\n\nIn my opinion, the approach in the paper stands out for several reasons:\n\nThe complete absence of transformers or any attention mechanism. The model only uses MLPs and pooling layers.\nThe relatively small size of the model - it uses a total of just 6M parameters (3M for the actor and 3M for the critic), which seems tiny for a model controlling autonomous vehicles.\nThe fairly minimalistic reward function formulation that combines a handful of components to encode good driving behavior.\nThe enormous throughput of interactions with the simulation environment (4.4 billion state transitions per hour on an 8-GPU node).\nLast but not least, the emergence of realistic and robust driving behavior for a model that has never seen human-driving data.\n\nMy favorite part about this paper, though, was the conditioning input C to the model which modulates the policy’s behavior and enables inference-time modifications of agent behavior by simply changing conditioning inputs. More aggressive driving? Simply modify the weights on some reward function components. You want a truck instead of a passenger vehicle behavior? Increase the agent’s dimensions and dynamics through C and it will behave like a truck. A single model can be used to simulate a diverse variety of agents and behaviors - which is a powerful capability for realistic agents in simulation (and possibly even for a policy that runs on-vehicle).\nI enjoyed reading the paper to the point where I put together the following slide deck and presented it for an autonomous driving reading group.\n\n\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "This notebook explores dataset/dataloader creation techniques based on Sebastian Raschka’s book (Chapter 2), implementing data sampling, batching, and other techniques.\n\n\nBorrowed from Manning’s Live Books\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\n\nimport re\nfrom typing import Any, Dict, List, Tuple\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0\n\ntiktoken version: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#dataset",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "Borrowed from Manning’s Live Books"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#resources",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\n\nimport re\nfrom typing import Any, Dict, List, Tuple\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0\n\ntiktoken version: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_13_llms_from_scratch/index.html",
    "href": "posts/2025_05_13_llms_from_scratch/index.html",
    "title": "Build a Large Language Model (From Scratch)",
    "section": "",
    "text": "I recently finished reading (or rather working through) my technical book of the month - Build a Large Language Model (From Scratch) by Sebastian Raschka and wanted to share my notes and codeing-along Jupyter notebooks here.\n\nMy take\nFirst, I want to emphasize what an awesome read this was. It presumes very little prior knowledge (other than being able to code in Python) and does a great job of building up the concepts, theory, and intuition for LLMs - all the way to training your own GPT-2 model and instruction fine-tuning it.\nMost of my previous technical books have been O’Reilly books, which also impressed me - especially Generative Deep Learning, 2nd Edition by David Foster. That book and most O’Reilly books I’ve read have provide a good balance of theory and practice / coding but Sebastian’s book does a better job of building from the ground up. In that sense, “Building a Large Language Model from Scratch” reminds me more of Andrej Karpathy’s Neural Networks: Zero to Hero, which I can also highly recommend. The latter goes even more into detail on the math and theory (including through derivations of backpropagation, which Sebastian skips in favor of focusing on the code).\nOverall, I can highly recommend Sebastian’s book to anyone who wants to understand LLMs and wants to follow a well-structured longform tutorial on building LLMs.\n\n\nBook summary (by Gemini)\nThe book “Build a Large Language Model (From Scratch)” by Sebastian Raschka guides readers through the process of creating, training, and fine-tuning Large Language Models (LLMs) from the ground up. This hands-on book aims to demystify LLMs by teaching readers how to build one step-by-step, without relying on existing LLM libraries. The core idea is that by building an LLM (comparable to GPT-2 in capabilities) yourself, you gain a deep understanding of its internal workings, limitations, and customization methods. The resulting LLM can be run on a standard laptop.\nKey learnings from the book include:\n\nPlanning and Coding: Learn to design and code all components of an LLM.\nDataset Preparation: Understand how to prepare datasets suitable for LLM training.\nTraining Pipeline: Construct a complete training pipeline.\nPretraining and Fine-tuning: Pretrain the model on a general corpus and then fine-tune it for specific tasks like text classification or with custom data.\nInstruction Following: Use human feedback to ensure the LLM adheres to instructions.\nLoading Pretrained Weights: Learn how to load pretrained weights into an LLM.\n\nBy working through the book, readers can expect to build a GPT-style LLM, evolve it into a text classifier, and ultimately create a chatbot that can follow conversational instructions.\nThe target audience for this book is individuals with intermediate Python skills and some existing knowledge of machine learning. While a GPU is recommended for faster training, it is optional.\nThe book is praised for its practical, code-driven approach that makes complex concepts accessible. You can find more details on the Manning Publications website: https://www.manning.com/books/build-a-large-language-model-from-scratch.\n\n\nJupyter notebooks\nOver the next few days, I’ll be adding Jupyter notebooks to this blog - one notebook for each chapter of the book. Besides functional code that mostly follows the book (but improves upon it in terms of type annotation, structure, and readability), these notebooks also contain additional material that expand upon certain concepts I wanted to explore more deeply than the book does.\nThe full set of notebooks is also available on my GitHub repo.\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html",
    "href": "posts/2025_05_05_welcome/index.html",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "Hello and welcome to Layer by Layer - and experiment in learning in public where I’ll be sharing my exploration of machine learning concepts, techniques, and research.\n\n\nThe machine learning landscape is evolving at a breathtaking pace. From transformers to diffusion models, from reinforcement learning to neural radiance fields - there’s an overwhelming amount of innovation happening. As someone navigating this field, I often find myself wanting to document my learning journey, break down complex ideas, and create resources I wish had existed when I was starting out.\nThis blog is my attempt to:\n\nDocument my learning process - Writing helps me solidify my understanding\nDevelop and share practical tutorials - With code examples that actually work. This is inspired by a few of the ML blogging greats: Sebastian Raschka’s blog, Andrej Karpathy’s blog, Lilian Weng’s blog, and many more.\nSummarize interesting papers - Distilling key ideas from research into accessible explanations (in the spirit of Two Minute Papers but in longer written form). This is inspired by Jay Alammar’s Illustrated Transformer his Illustrated Deep Seek R1 and overall his Illustrated* series now available at Substack.\nBuild in public - Showing both successes and the inevitable struggles, and also post project walkthroughs.\nLink to resources - Occasionally, I’ll be posting links to useful resources - though I don’t want to make this another newslettter or news aggregator.\n\n\n\n\nI’m Daniel Pickem, a researcher and engineer with a background in robotics and multi-agent systems. I completed my PhD at Georgia Tech where I developed the Robotarium, a remotely accessible swarm robotics testbed. My swarm robotics days are somewhat behind me - or maybe just on pause until swarm robotics has more practical applications. Until then, I am really excited and curious about machine learning - most of all foundation models of all modalities.\n\n\n\nRobotarium\n\n\n\n\n\nI hope you’ll find the content here useful, whether you’re just starting out in ML or are already well into your own journey. Feel free to reach out with questions, corrections, or suggestions for topics you’d like to see covered.\nLet’s explore the fascinating world of machine learning together, one layer at a time.\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html#why-i-started-this-blog",
    "href": "posts/2025_05_05_welcome/index.html#why-i-started-this-blog",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "The machine learning landscape is evolving at a breathtaking pace. From transformers to diffusion models, from reinforcement learning to neural radiance fields - there’s an overwhelming amount of innovation happening. As someone navigating this field, I often find myself wanting to document my learning journey, break down complex ideas, and create resources I wish had existed when I was starting out.\nThis blog is my attempt to:\n\nDocument my learning process - Writing helps me solidify my understanding\nDevelop and share practical tutorials - With code examples that actually work. This is inspired by a few of the ML blogging greats: Sebastian Raschka’s blog, Andrej Karpathy’s blog, Lilian Weng’s blog, and many more.\nSummarize interesting papers - Distilling key ideas from research into accessible explanations (in the spirit of Two Minute Papers but in longer written form). This is inspired by Jay Alammar’s Illustrated Transformer his Illustrated Deep Seek R1 and overall his Illustrated* series now available at Substack.\nBuild in public - Showing both successes and the inevitable struggles, and also post project walkthroughs.\nLink to resources - Occasionally, I’ll be posting links to useful resources - though I don’t want to make this another newslettter or news aggregator."
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html#about-me",
    "href": "posts/2025_05_05_welcome/index.html#about-me",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "I’m Daniel Pickem, a researcher and engineer with a background in robotics and multi-agent systems. I completed my PhD at Georgia Tech where I developed the Robotarium, a remotely accessible swarm robotics testbed. My swarm robotics days are somewhat behind me - or maybe just on pause until swarm robotics has more practical applications. Until then, I am really excited and curious about machine learning - most of all foundation models of all modalities.\n\n\n\nRobotarium"
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html#join-me-on-this-journey",
    "href": "posts/2025_05_05_welcome/index.html#join-me-on-this-journey",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "I hope you’ll find the content here useful, whether you’re just starting out in ML or are already well into your own journey. Feel free to reach out with questions, corrections, or suggestions for topics you’d like to see covered.\nLet’s explore the fascinating world of machine learning together, one layer at a time.\nStay curious,\nDaniel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "Welcome to Layer by Layer, where I try to peel back the complexity of modern machine learning to reveal insights, patterns, and understanding - or mostly just share things in the ML space that I find interesting and also document my own learning journey, one layer at a time.\nI’m fascinated (though also intimidated and sometimes overwhelmed) by how rapidly the machine learning field is evolving - just reading AI newsletters is often tough to fit into a busy work schedule (let alone drinking from the firehose of all the papers that are coming out). My goal here is simple: to explore interesting ideas and ML fundamentals, break down complex concepts, and post by post build an ML engineering foundation that will hopefully make it easier to break into this exciting field. This blog is meant as an extension of my own learning journey.\nYou’ll find a mix of content here (or at least that is the aspiration):\n\nIn-depth tutorials that break down complex ML techniques into digestible steps - tutorials that I wish existed when I was learning certain techniques\nPractical guides for implementing state-of-the-art models and methodologies\nAccessible summaries of recent research papers that highlight key contributions but also notes I’ve found helpful or interesting\nThoughts on tools and frameworks I’ve tried, and the ecosystem powering modern ML\nDiscussions about trends and where ML might be heading\n\nI’m writing for fellow enthusiasts - people who are curious and excited about machine learning and want to understand it better, whether you’re just starting out or have been in the field for a while.\nThis isn’t about presenting myself as an expert but about becoming one - and sharing what I’m learning and thinking about along the way. I hope you’ll join me on this journey as we build knowledge together — layer by layer."
  },
  {
    "objectID": "disclaimer/index.html",
    "href": "disclaimer/index.html",
    "title": "Disclaimer",
    "section": "",
    "text": "Opinions expressed on this Site are the author’s own in his personal capacity. They do not reflect the views of the United States Government, NVIDIA Inc. or of any organisation, company or board he is associated with."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Currently, I am a Staff Software Engineer at NVIDIA, where I focus on defining and developing metrics for autonomous vehicle (AV) software, building data-driven evaluation products, and incorporating LLMs and VLMs into evaluation workflows. Just like AV planning systems move from rule-based systems to end-to-end learned planners, I am convinvced the systems tasked with evaluating AV planning stacks need to move to a data-driven approach that incorporates the nuanced world understanding and context AV systems themselves need to be able to reason about.\nPrior to NVIDIA (and fresh out of grad school), I was a Senior Machine Learning Engineer at Apple, working on robotics and decision making for autonomous vehicles. That is a fairly coarse summary for the 7+ years I’ve been with Apple’s SPG project tasked to work on autonomous systems. I’ve worked on everything from machine learned semantic map annotation, to the the initial rule-based planning system (and its transition to a hybrid rule-based/learned planner), to high signal-to-noise evaluation systems for said planner (with the goal of identifying test progressions and preventing regressions).\nI’ve received a B.S. degree in Electrical Engineering from the Vienna University of Technology, an M.S. degree in Electrical and Computer Engineering, and a Ph.D. degree in Robotics from the Georgia Institute of Technology. My doctoral research focused on self-reconfigurable multi-robot systems.\nI’m a recipient of the Fulbright scholarship and have held research positions at Carnegie Mellon University, Georgia Institute of Technology, and industry roles at Qualcomm, BMW, and Apple. My work on the Robotarium received the Best Paper Award on Multi-Robot Systems at the IEEE International Conference on Robotics and Automation (ICRA) in 2017, and I was a Student Best Paper Finalist at the IEEE Conference on Decision and Control in 2015.\n\n\n\nFoundation models: LLMs, VLMs, multi-modal models, diffusion models (for image, video, and audio generation), mechanistic interpretability and alignment via RLHF (reinforcement learning from human feedback), RLAIF (reinforcement learning from AI feedback), RLVR (reinforcement learning from verifiable rewards)\nMachine Learning: Reinforcement learning\nAutonomous Vehicles: Building the next generation of AV evaluation systems based on foundation models that enable detailed scene understanding and reasoning, adverse scenario generation, scene similarity computation based on embeddings, VLA models that enable interpretable reasoning about AV behavior\nRobotics: Multi-robot systems, self-reconfigurable robotics, swarm robotics (though this is a field I haven’t had the chance to work in for a while now)\n\n\n\n\nNVIDIA | Staff Software Engineer | 2024-Present\nFocus on defining and developing metrics for autonomous vehicle software, building data-driven evaluation products, and incorporating LLMs into evaluation workflows\nApple | Senior Machine Learning Engineer | 2017-2024 Worked on robotics and decision making for autonomous vehicles\nVarious Research Positions | 2012-2017\nResearch positions at Carnegie Mellon University and Georgia Institute of Technology\n\n\n\nGeorgia Institute of Technology | Ph.D., Robotics | 2016 Dissertation: Self-reconfigurable multi-robot systems\nGeorgia Institute of Technology | M.S., Electrical and Computer Engineering | 2012\nVienna University of Technology | B.S., Electrical Engineering | 2010\n\n\n\n\nBest Paper Award on Multi-Robot Systems | IEEE ICRA | 2017\nStudent Best Paper Finalist | IEEE CDC | 2015\nFulbright Scholarship | 2010-2012"
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Foundation models: LLMs, VLMs, multi-modal models, diffusion models (for image, video, and audio generation), mechanistic interpretability and alignment via RLHF (reinforcement learning from human feedback), RLAIF (reinforcement learning from AI feedback), RLVR (reinforcement learning from verifiable rewards)\nMachine Learning: Reinforcement learning\nAutonomous Vehicles: Building the next generation of AV evaluation systems based on foundation models that enable detailed scene understanding and reasoning, adverse scenario generation, scene similarity computation based on embeddings, VLA models that enable interpretable reasoning about AV behavior\nRobotics: Multi-robot systems, self-reconfigurable robotics, swarm robotics (though this is a field I haven’t had the chance to work in for a while now)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Daniel Pickem",
    "section": "",
    "text": "NVIDIA | Staff Software Engineer | 2024-Present\nFocus on defining and developing metrics for autonomous vehicle software, building data-driven evaluation products, and incorporating LLMs into evaluation workflows\nApple | Senior Machine Learning Engineer | 2017-2024 Worked on robotics and decision making for autonomous vehicles\nVarious Research Positions | 2012-2017\nResearch positions at Carnegie Mellon University and Georgia Institute of Technology"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Georgia Institute of Technology | Ph.D., Robotics | 2016 Dissertation: Self-reconfigurable multi-robot systems\nGeorgia Institute of Technology | M.S., Electrical and Computer Engineering | 2012\nVienna University of Technology | B.S., Electrical Engineering | 2010"
  },
  {
    "objectID": "about.html#awards-honors",
    "href": "about.html#awards-honors",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Best Paper Award on Multi-Robot Systems | IEEE ICRA | 2017\nStudent Best Paper Finalist | IEEE CDC | 2015\nFulbright Scholarship | 2010-2012"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "A full list of my publications can be found on Google Scholar"
  },
  {
    "objectID": "publications/index.html#conference-proceedings",
    "href": "publications/index.html#conference-proceedings",
    "title": "Publications",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\nThe robotarium: A remotely accessible swarm robotics research testbed\nD. Pickem, P. Glotfelter, L. Wang, M. Mote, et al.\nIEEE International Conference on Robotics and Automation (ICRA), 2017\nThe GRITSBot in its natural habitat-a multi-robot testbed\nD. Pickem, M. Lee, M. Egerstedt\nIEEE International Conference on Robotics and Automation (ICRA), 2015\nRealizing simultaneous lane keeping and adaptive speed regulation on accessible mobile robot testbeds\nX. Xu, T. Waters, D. Pickem, P. Glotfelter, et al.\nIEEE Conference on Control Technology and Applications (CCTA), 2017\nA game-theoretic formulation of the homogeneous self-reconfiguration problem\nD. Pickem, M. Egerstedt\nIEEE Conference on Decision and Control (CDC), 2015\nSelf-reconfiguration using graph grammars for modular robotics\nD. Pickem, M. Egerstedt\nIFAC Proceedings Volumes, 2012\nComplete heterogeneous self-reconfiguration: Deadlock avoidance using hole-free assemblies\nD. Pickem, M. Egerstedt, J.S. Shamma\nIFAC Proceedings Volumes, 2013"
  },
  {
    "objectID": "publications/index.html#various",
    "href": "publications/index.html#various",
    "title": "Publications",
    "section": "Various",
    "text": "Various\n\nCaptain hindsight: An autonomous surface vessel\nD. Pickem, D. Morioniti, C. Taylor, et al.\nGeorgia Institute of Technology Technical Report, 2012"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "",
    "text": "This notebook explores full LLM architecture for GPT based on Sebastian Raschka’s book (Chapter 4), implementing normalization layers, shortcut connections, and transformer blocks. This notebook also shows how to compute the parameter count as well as the storage requirements of GPT-like models.\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 4\n\nGPT-2 original paper: “Language Models Are Unsupervised Multitask Learners”\n\n\n\n\nGPT architecture\n\n\nThe components of a GPT-like architecture are shown in the image below.\n\n\n\nGPT components\n\n\nThe following figure shows the flow of data through the model and how they are transformed at each stage.\n\n\n\nGPT data flow\n\n\n\nimport dataclasses\nfrom typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\n\nimport tiktoken"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#resources",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 4\n\nGPT-2 original paper: “Language Models Are Unsupervised Multitask Learners”\n\n\n\n\nGPT architecture\n\n\nThe components of a GPT-like architecture are shown in the image below.\n\n\n\nGPT components\n\n\nThe following figure shows the flow of data through the model and how they are transformed at each stage.\n\n\n\nGPT data flow\n\n\n\nimport dataclasses\nfrom typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\n\nimport tiktoken"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#example",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#example",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "Example",
    "text": "Example\n\n# Example of layer normalization.\ntorch.set_printoptions(sci_mode=False)\ntorch.manual_seed(123)\n\n# Create two training examples with 5 features each.\nbatch_example = torch.randn(2, 5)\n\n# Create a basic neural network layer consisting of a Linear layer followed by a non-linear\n# activation function, ReLU (short for rectified linear unit).\nlayer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\nout = layer(batch_example)\n\n# Compute mean and variance along the feature dimension (i.e. the last dimension).\n# NOTE: Using keepdim=True in operations like mean or variance calculation ensures that the output\n#       tensor retains the same number of dimensions as the input tensor, even though the operation\n#       reduces the tensor along the dimension specified via dim. Here, without keepdim=True, the\n#       output tensor would be a two-dimensional vector (e.g. [1, 2]) rather than a 2x1-dimensional\n#       matrix (e.g. [[1], [2]]).\nmean = out.mean(dim=-1, keepdim=True)\nvar = out.var(dim=-1, keepdim=True)\nprint(f\"\\nRaw layer outputs:\\n{'-' * 18}\\n{out}\")\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n\n# Apply layer normalization.\nout_norm = (out - mean) / torch.sqrt(var)\nmean = out_norm.mean(dim=-1, keepdim=True)\nvar = out_norm.var(dim=-1, keepdim=True)\nprint(f\"\\nNormalized layer outputs:\\n{'-' * 25}\\n{out_norm}\")\nprint(f\"\\nMean:\\n{mean}\")\nprint(f\"\\nVariance:\\n{var}\")\n\n\nRaw layer outputs:\n------------------\ntensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n       grad_fn=&lt;ReluBackward0&gt;)\nMean:\n tensor([[0.1324],\n        [0.2170]], grad_fn=&lt;MeanBackward1&gt;)\nVariance:\n tensor([[0.0231],\n        [0.0398]], grad_fn=&lt;VarBackward0&gt;)\n\nNormalized layer outputs:\n-------------------------\ntensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n       grad_fn=&lt;DivBackward0&gt;)\n\nMean:\ntensor([[    0.0000],\n        [    0.0000]], grad_fn=&lt;MeanBackward1&gt;)\n\nVariance:\ntensor([[1.0000],\n        [1.0000]], grad_fn=&lt;VarBackward0&gt;)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#a-layer-normalization-class",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#a-layer-normalization-class",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "A layer normalization class",
    "text": "A layer normalization class\n\nclass LayerNorm(nn.Module):\n    \"\"\"Layer normalization (https://arxiv.org/abs/1607.06450).\n\n    This specific implementation of layer normalization operates on the last dimension of\n    the input tensor x, which represents the embedding dimension (emb_dim).\n    \"\"\"\n\n    def __init__(self, emb_dim: int):\n        super().__init__()\n\n        # Use a small constant which will be added to the variance to prevent division by zero.\n        self.eps = 1e-5\n\n        # The scale and shift are two trainable parameters (of the same dimension as the input)\n        # that the LLM automatically adjusts during training.\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n\n\n# Test the layer normalization class.\nln = LayerNorm(emb_dim=5)\nout_ln = ln(batch_example)\nmean = out_ln.mean(dim=-1, keepdim=True)\nvar = out_ln.var(dim=-1, unbiased=False, keepdim=True)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n\nMean:\n tensor([[    -0.0000],\n        [     0.0000]], grad_fn=&lt;MeanBackward1&gt;)\nVariance:\n tensor([[1.0000],\n        [1.0000]], grad_fn=&lt;VarBackward0&gt;)\n\n\nGELU activation function\nThis section is skipped in this notebook since torch already implements to full version of GELU as well as the curve fitting approximation.\nSee GELU activation function\nThe below figure shows a comparison of GELU and ReLU.\n\n\n\nFeed forward network"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#analyzing-the-model-architecture",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#analyzing-the-model-architecture",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "Analyzing the model architecture",
    "text": "Analyzing the model architecture\n\n# NOTE: The reason for the total number of parameters to be 163M instead of 124M is a concept called\n#       weight tying, which was used in the original GPT-2 architecture. It means that the original\n#       GPT-2 architecture reuses the weights from the token embedding layer in its output layer.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")\nprint(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\nprint(\"Output layer shape:\", model.out_head.weight.shape)\n\nTotal number of parameters: 163,009,536\nToken embedding layer shape: torch.Size([50257, 768])\nOutput layer shape: torch.Size([50257, 768])\n\n\n\ntotal_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\nprint(\n    f\"Number of trainable parameters \"\n    f\"considering weight tying: {total_params_gpt2:,}\"\n)\n\nNumber of trainable parameters considering weight tying: 124,412,160\n\n\n\n# Calculates the total size in bytes (assuming float32, 4 bytes per parameter).\ntotal_size_bytes = total_params * 4\ntotal_size_mb = total_size_bytes / (1024 * 1024)\nprint(f\"Total size of the model: {total_size_mb:.2f} MB\")\n\nTotal size of the model: 621.83 MB"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking text into smaller units called “tokens.” These tokens serve as the basic building blocks that machine learning models can process.\nFor Large Language Models (LLMs), tokenization is a critical first step that converts human-readable text into numerical formats the model can understand. When you send a prompt to an LLM such as GPT or Claude, the model doesn’t directly read your text - it processes sequences of tokens that represent your text.\nThere are several approaches to tokenization:\n\nWord tokenization: Splitting text by words (separated by spaces or punctuation)\nSubword tokenization: Breaking words into meaningful subunits (most common in modern LLMs)\nCharacter tokenization: Dividing text into individual characters\n\nTokenization presents various challenges, including handling punctuation, contractions, compound words, and rare words. The choice of tokenization method significantly impacts an LLM’s performance, vocabulary size, and ability to handle different languages.\nThis notebook explores tokenization techniques based on Sebastian Raschka’s book (Chapter 2), implementing various tokenization approaches and analyzing their effects.\n\n\n\nBorrowed from Manning’s Live Books\n\n\n\nTokenization Process\n\n\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\n\n# Install dependencies.\n%pip install tiktoken\n\n\nimport re\nfrom typing import Dict, List\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#what-is-tokenization",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#what-is-tokenization",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking text into smaller units called “tokens.” These tokens serve as the basic building blocks that machine learning models can process.\nFor Large Language Models (LLMs), tokenization is a critical first step that converts human-readable text into numerical formats the model can understand. When you send a prompt to an LLM such as GPT or Claude, the model doesn’t directly read your text - it processes sequences of tokens that represent your text.\nThere are several approaches to tokenization:\n\nWord tokenization: Splitting text by words (separated by spaces or punctuation)\nSubword tokenization: Breaking words into meaningful subunits (most common in modern LLMs)\nCharacter tokenization: Dividing text into individual characters\n\nTokenization presents various challenges, including handling punctuation, contractions, compound words, and rare words. The choice of tokenization method significantly impacts an LLM’s performance, vocabulary size, and ability to handle different languages.\nThis notebook explores tokenization techniques based on Sebastian Raschka’s book (Chapter 2), implementing various tokenization approaches and analyzing their effects."
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#tokenization-process",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#tokenization-process",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Borrowed from Manning’s Live Books\n\n\n\nTokenization Process"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#acknowledgment",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#resources",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#resources",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\n\n# Install dependencies.\n%pip install tiktoken\n\n\nimport re\nfrom typing import Dict, List\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#algorithm-explained-via-a-simple-example",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#algorithm-explained-via-a-simple-example",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "Algorithm explained via a simple example",
    "text": "Algorithm explained via a simple example\n\n\n\nBPE algorithm explained via a simple example"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#example-of-bpe-tokenization-for-unknown-words",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#example-of-bpe-tokenization-for-unknown-words",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "Example of BPE tokenization for unknown words",
    "text": "Example of BPE tokenization for unknown words\n\n\n\nBPE tokenization for unknown words\n\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntext = (\n    \"Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces\"\n    \"of someunknownPlace.\"\n)\nprint(text)\nintegers = tokenizer.encode(text, allowed_special={\"&lt;|endoftext|&gt;\"})\nprint(integers)\nstrings = tokenizer.decode(integers)\nprint(strings)\n\n\ntokenizer.decode(tokenizer.encode(\"Akwirw ier\"))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "",
    "text": "This notebook explores the fine-tuning process of LLMs with the purpose of creating instruction fine-tuned model based on Sebastian Raschka’s book (Chapter 7). In particular, it discusses the following:\n\nThe instruction fine-tuning process of LLMs\nPreparing a dataset for supervised instruction fine-tuning\nOrganizing instruction data in training batches\nLoading a pretrained LLM and fine-tuning it to follow human instructions\nExtracting LLM-generated instruction responses for evaluation\nEvaluating an instruction-fine-tuned LLM\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 7\n\n\n\n\n\nTopic overview\n\n\n\nimport dataclasses\nimport json\nimport functools\nimport os\nimport pathlib\nimport psutil\nfrom pprint import pprint\nimport time\nfrom typing import Any, Dict, List, Optional, Tuple\nimport urllib.request\nimport urllib\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\n\n# Chapter 4 dependencies.\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n)\n\n# Chapter 5 dependencies.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n    load_weights_into_gpt,\n    calc_loss_loader,\n    train_model_simple,\n    plot_losses,\n)\n\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#resources",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 7\n\n\n\n\n\nTopic overview\n\n\n\nimport dataclasses\nimport json\nimport functools\nimport os\nimport pathlib\nimport psutil\nfrom pprint import pprint\nimport time\nfrom typing import Any, Dict, List, Optional, Tuple\nimport urllib.request\nimport urllib\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\n\n# Chapter 4 dependencies.\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n)\n\n# Chapter 5 dependencies.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n    load_weights_into_gpt,\n    calc_loss_loader,\n    train_model_simple,\n    plot_losses,\n)\n\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#download-and-load-the-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#download-and-load-the-dataset",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Download and load the dataset",
    "text": "Download and load the dataset\nThe dataset consists of 1,100 instruction–response pairs. This dataset was created specifically for this book. The following code implements and executes a function to download this dataset, which is a relatively small file (only 204 KB) in JSON forma.\nAs we can see, the example entries are Python dictionary objects containing an instruction, input, and output.\nThe input field may occasionally be empty.\n\ndef download_and_load_file(file_path: pathlib.Path, url: str) -&gt; Dict[str, Any]:\n    \"\"\"Download and load a file from a URL.\n\n    Args:\n        file_path: The path to the file to download.\n        url: The URL to download the file from.\n\n    Returns:\n        The loaded data.\n    \"\"\"\n    # Skips download if file was already downloaded\n    if not os.path.exists(file_path):\n        with urllib.request.urlopen(url) as response:\n            text_data = response.read().decode(\"utf-8\")\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(text_data)\n    else:\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            text_data = file.read()\n\n    # Load and decode the data from the file.\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n\n    return data\n\n\nfile_path = pathlib.Path(\"data/instruction-data.json\")\nurl = (\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n)\ndata = download_and_load_file(file_path, url)\nprint(\"Number of entries:\", len(data))\npprint(data[50])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#prompt-formatting",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#prompt-formatting",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Prompt formatting",
    "text": "Prompt formatting\nInstruction fine-tuning involves training a model on a dataset where the input-output pairs, like those we extracted from the JSON file, are explicitly provided. There are various methods to format these entries for LLMs.\nThere are various methods to format these entries for LLMs, often referred to as prompt styles. The most commonly used ones are the following:\n\nAlpaca prompt style\nPhi-3 prompt style\n\nAlpaca was one of the early LLMs to publicly detail its instruction fine-tuning process. Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt styles. The rest of this notebook uses the Alpaca prompt style since it is one of the most popular ones, largely because it helped define the original approach to fine-tuning.\n\n\n\nPrompt styles\n\n\n\ndef format_input(entry: Dict[str, Any], add_output: bool = False) -&gt; str:\n    \"\"\"Format an entry for the Alpaca prompt style.\n\n    Args:\n        entry: A dictionary containing an `instruction` and `input` key.\n        add_output: Whether to add the `output` key to the formatted string.\n\n    Returns:\n        The formatted string.\n    \"\"\"\n    # Add the 'system' prompt and the entry's instruction.\n    instruction_text = (\n        f\"Below is an instruction that describes a task. \"\n        f\"Write a response that appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    # Add the entry's input if it exists.\n    # NOTE: The 'input' section is skipped if the field is empty.\n    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n\n    # Optionally add the desired response.\n    desired_response = f\"\\n\\n### Response:\\n{entry['output']}\" if add_output else \"\"\n\n    return instruction_text + input_text + desired_response\n\n\n# Format the example entry.\nmodel_input = format_input(data[50], add_output=True)\nprint(model_input)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#splitting-the-datast",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#splitting-the-datast",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Splitting the datast",
    "text": "Splitting the datast\n\n# TODO: This section should reuse functions from chapter 6.\n\n# Use 85% of the data for training, 10% for testing, and 5% for validation.\ntrain_portion = int(len(data) * 0.85)\ntest_portion = int(len(data) * 0.1)\nval_portion = len(data) - train_portion - test_portion\n\ntrain_data = data[:train_portion]\ntest_data = data[train_portion : train_portion + test_portion]\nval_data = data[train_portion + test_portion :]\n\nprint(\"Training set length:\", len(train_data))\nprint(\"Validation set length:\", len(val_data))\nprint(\"Test set length:\", len(test_data))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#organizing-data-into-training-batches",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#organizing-data-into-training-batches",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Organizing data into training batches",
    "text": "Organizing data into training batches\nIn this section, we learn how to efficiently pad the data samples to equal lengths so we can assemble multiple instruction examples in a batch.\nIn the previous chapter, the training batches were created automatically by the PyTorch DataLoader class, which employs a default collate function to combine lists of samples into batches. A collate function is responsible for taking a list of individual data samples and merging them into a single batch that can be processed efficiently by the model during training. Here, we create our own custom collate function to handle specific requirements and formatting (pre-tokenization and formatting of inputs) of our instruction dataset.\n\n\n\nBatching overview\n\n\n\nCreate a dataset class\nSimilar to the approach used for classification fine-tuning, we want to accelerate training by collecting multiple training examples in a batch, which necessitates padding all inputs to a similar length. As with classification fine-tuning, we use the &lt;|endoftext|&gt; token as a padding token.\n\n\n\nPrompt formatting and tokenization\n\n\n\nclass InstructionDataset(Dataset):\n    \"\"\"Dataset class for instruction fine-tuning.\"\"\"\n\n    def __init__(self, data: List[Dict[str, Any]], tokenizer: tiktoken.Encoding):\n        # Cache the raw and encoded texts.\n        self.data = data\n        self.encoded_texts = []\n\n        # Pretokenizes texts\n        for entry in data:\n            full_text = format_input(entry=entry, add_output=True)\n            self.encoded_texts.append(tokenizer.encode(full_text))\n\n    def __getitem__(self, index: int) -&gt; List[int]:\n        return self.encoded_texts[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ndataset = InstructionDataset(train_data, tokenizer)\nprint(f\"Length of dataset: {len(dataset)}\")\n\n\n# Instead of appending the &lt;|endoftext|&gt; tokens to the text inputs, we can append the token ID\n# corresponding to &lt;|endoftext|&gt; to the pretokenized inputs directly. We can use the tokenizer’s\n# .encode method on an &lt;|endoftext|&gt; token to remind us which token ID we should use:\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nprint(tokenizer.encode(\"&lt;|endoftext|&gt;\", allowed_special={\"&lt;|endoftext|&gt;\"}))\n\n\n\nCustom collate function\nThis custom collate function pads the training examples in each batch to the same length while allowing different batches to have different lengths, as demonstrated in the figure below. This approach minimizes unnecessary padding by only extending sequences to match the longest one in each batch, not the whole dataset.\n\n\n\nCustom collate function\n\n\n\ndef custom_collate_draft_1(\n    batch: List[List[int]], pad_token_id: int = 50256, device: str = \"cpu\"\n) -&gt; torch.Tensor:\n    \"\"\"Custom collate function for instruction fine-tuning.\n\n    Args:\n        batch: A list of lists of integers representing the training examples.\n        pad_token_id: The token ID to use for padding.\n        device: The device to move the resulting tensor to.\n\n    Returns:\n        A tensor of the padded inputs.\n    \"\"\"\n    # Find the longest sequence in the batch.\n    batch_max_length = max(len(item) + 1 for item in batch)\n\n    # Pad and prepare the inputs.\n    inputs_lst = []\n    for item in batch:\n        # Copy the item and append a single padding token.\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        # Pad the sequence to the longest sequence in the batch.\n        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n\n        # Convert the padded sequence to a tensor, remove the extra padded token added earlier, and\n        # add it to the list of inputs.\n        # NOTE: The purpose of this will become clear later in the second draft of this collate\n        #       function.\n        inputs = torch.tensor(padded[:-1])\n        inputs_lst.append(inputs)\n\n    # Stack the inputs into a single tensor and move it to the specified device.\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    return inputs_tensor\n\n\n# Test the custom collate function.\n# NOTE: This output shows all inputs have been padded to the length of the longest input list,\n#       inputs_1, containing five token IDs.\ninputs_1 = [0, 1, 2, 3, 4]\ninputs_2 = [5, 6]\ninputs_3 = [7, 8, 9]\nbatch = (\n    inputs_1,\n    inputs_2,\n    inputs_3,\n)\nprint(custom_collate_draft_1(batch))\n\n\n\nAdding target tokens to the collate function\nWe also need to create batches with the target token IDs corresponding to the batch of input IDs. These target IDs, as shown in the figure below, are crucial because they represent what we want the model to generate and what we need during training to calculate the loss for the weight updates. That is, we modify our custom collate function to return the target token IDs in addition to the input token IDs.\n\n\n\nTarget tokens in custom collate function\n\n\nSimilar to the process we used to pretrain an LLM, the target token IDs match the input token IDs but are shifted one position to the right.\n\n\n\nTarget tokens in custom collate function\n\n\n\ndef custom_collate_draft_2(\n    batch: List[List[int]], pad_token_id: int = 50256, device: str = \"cpu\"\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Custom collate function for instruction fine-tuning.\n\n    Args:\n        batch: A list of lists of integers representing the training examples.\n        pad_token_id: The token ID to use for padding.\n        device: The device to move the resulting tensor to.\n\n    Returns:\n        A tuple of tensors of the padded inputs and targets.\n    \"\"\"\n    # Find the longest sequence in the batch.\n    batch_max_length = max(len(item) + 1 for item in batch)\n\n    # Initialize the outputs.\n    inputs_lst, targets_lst = [], []\n\n    # Pad and prepare the inputs.\n    for item in batch:\n        # Copy the item and append a single padding token.\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        # Pad the sequence to the longest sequence in the batch.\n        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n\n        # Truncates the last token for inputs.\n        inputs = torch.tensor(padded[:-1])\n        # Shifts +1 to the right for targets.\n        targets = torch.tensor(padded[1:])\n\n        # Append the inputs and targets to the lists.\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    # Stack the inputs into a single tensor and move it to the specified device.\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor, targets_tensor\n\n\n# Test the custom collate function.\ninputs, targets = custom_collate_draft_2(batch)\nprint(inputs)\nprint(targets)\n\n\n\nReplace padding tokens with -100\nWe assign a -100 placeholder value to all padding tokens. This special value allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.\nThe default setting of the cross entropy function in PyTorch is cross_entropy(..., ignore_index=-100). This means that it ignores targets labeled with -100.\nHowever, note that we retain one end-of-text token, ID 50256, in the target list. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated response is complete.\n\n\n\nPadding tokens\n\n\nIn addition to masking out padding tokens, it is also common to mask out the target token IDs that correspond to the instruction, as illustrated in figure 7.13. By masking out the LLM’s target token IDs corresponding to the instruction, the cross entropy loss is only computed for the generated response target IDs. Thus, the model is trained to focus on generating accurate responses rather than memorizing instructions, which can help reduce overfitting.\n\n\n\nMasking out instructions\n\n\nAs of this writing, researchers are divided on whether masking the instructions is universally beneficial during instruction fine-tuning. For instance, the 2024 paper by Shi et al., Instruction Tuning With Loss Over Instructions, demonstrated that not masking the instructions benefits the LLM performance (see appendix B for more details). Here, we will not apply instruction masking.\n\ndef custom_collate_fn(\n    batch: List[List[int]],\n    pad_token_id: int = 50256,\n    ignore_index=-100,\n    allowed_max_length=None,\n    device: str = \"cpu\",\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Custom collate function for instruction fine-tuning.\n\n    Args:\n        batch: A list of lists of integers representing the training examples.\n        pad_token_id: The token ID to use for padding.\n        ignore_index: The value to use for padding tokens.\n        allowed_max_length: The maximum length of the input sequences.\n        device: The device to move the resulting tensor to.\n\n    Returns:\n        A tuple of tensors of the padded inputs and targets.\n    \"\"\"\n    # Find the longest sequence in the batch.\n    batch_max_length = max(len(item) + 1 for item in batch)\n\n    # Initialize the outputs.\n    inputs_lst, targets_lst = [], []\n\n    # Pad and prepare the inputs.\n    for item in batch:\n        # Copy the item and append a single padding token.\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        # Pad the sequence to the longest sequence in the batch.\n        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n\n        # Truncates the last token for inputs.\n        inputs = torch.tensor(padded[:-1])\n        # Shifts +1 to the right for targets.\n        targets = torch.tensor(padded[1:])\n\n        # Replaces all but the first padding tokens in targets by ignore_index.\n        mask = targets == pad_token_id\n        indices = torch.nonzero(mask).squeeze()\n        if indices.numel() &gt; 1:\n            targets[indices[1:]] = ignore_index\n\n        # Optionally truncates to the maximum sequence length.\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]\n            targets = targets[:allowed_max_length]\n\n        # Append the inputs and targets to the lists.\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    # Stack the inputs into a single tensor and move it to the specified device.\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor, targets_tensor\n\n\n# Test the custom collate function.\ninputs, targets = custom_collate_fn(batch)\nprint(inputs)\nprint(targets)\n\n\n\nCreating the data loader\n\ncustomized_collate_fn = functools.partial(\n    custom_collate_fn, device=device, allowed_max_length=1024\n)\n\n\n# You can try to increase this number if parallel Python processes are supported by your operating\n# system.\nnum_workers = 0\nbatch_size = 8\n\n# Set the seed for reproducibility.\ntorch.manual_seed(123)\n\n# Create the datasets.\ntrain_dataset = InstructionDataset(train_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=True,\n    drop_last=True,\n    num_workers=num_workers,\n)\nval_dataset = InstructionDataset(val_data, tokenizer)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n)\ntest_dataset = InstructionDataset(test_data, tokenizer)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n)\n\n# Print the shape of all batches in the train loader.\n# NOTE: Each batch contains 8 examples but the length of the sequences can vary from batch to batch.\nprint(f\"Train loader (length: {len(train_loader)}):\")\nfor inputs, targets in train_loader:\n    print(inputs.shape, targets.shape)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#loading-a-pre-trained-llm",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#loading-a-pre-trained-llm",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Loading a pre-trained LLM",
    "text": "Loading a pre-trained LLM\n\n# Load the base config.\nGPT_CONFIG = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Update the model configuration to conform to the model size.\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Instantiate a base config.\ntmp_config = dataclasses.asdict(GPT_CONFIG)\n\n# Load the overlay parameters.\nmodel_name = \"gpt2-medium (355M)\"\ntmp_config.update(model_configs[model_name])\n\n# Update the context length to match OpenAI's GPT-2 models.\ntmp_config.update({\"context_length\": 1024})\n\n# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n# working with pretrained weights, we need to match the settings for consistency and enable these\n# bias vectors.\ntmp_config.update({\"qkv_bias\": True})\n\n# Instantiate the new configuration.\nNEW_CONFIG = GPTConfig(**tmp_config)\n\n# Download the pretrained weights.\nmodel_size = model_name.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\nprint(f\"Downloading pretrained weights for {model_size} model...\")\nsettings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n\n# Initialize the model with the new configuration.\nmodel = GPTModel(NEW_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval()\n\n\n# Sanity check the model outputs on a random example.\ntorch.manual_seed(123)\n\n# Print an example from the validation set.\ninput_text = format_input(val_data[0])\nprint(input_text)\n\n\n# Generate a response from the model.\n# NOTE: The generate function returns the combined input and output text. This behavior was\n#       previously convenient since pretrained LLMs are primarily designed as text-completion\n#       models, where the input and output are concatenated to create coherent and legible\n#       text. However, when evaluating the model’s performance on a specific task, we often\n#       want to focus solely on the model’s generated response.\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(input_text, tokenizer),\n    max_new_tokens=35,\n    context_size=NEW_CONFIG.context_length,\n    eos_id=50256,\n)\n\n# Print the model's response without the input text (i.e. the instruction).\n# NOTE: To isolate the model’s response text, we need to subtract the length of the input\n#       instruction from the start of the generated_text.\ngenerated_text = token_ids_to_text(token_ids, tokenizer)\nresponse_text = generated_text[len(input_text) :].strip()\nprint(response_text)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#instruction-fine-tuning-the-llm",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#instruction-fine-tuning-the-llm",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Instruction fine-tuning the LLM",
    "text": "Instruction fine-tuning the LLM\n\nExercise 7.3 Fine-tuning on the original Alpaca dataset\n\nThe Alpaca dataset, by researchers at Stanford, is one of the earliest and most popular openly shared instruction datasets, consisting of 52,002 entries. As an alternative to the instruction-data.json file we use here, consider fine-tuning an LLM on this dataset. The dataset is available at https://mng.bz/NBnE. This dataset contains 52,002 entries, which is approximately 50 times more than those we used here, and most entries are longer. Thus, I highly recommend using a GPU to conduct the training, which will accelerate the fine-tuning process. If you encounter out-of-memory errors, consider reducing the batch_size from 8 to 4, 2, or even 1. Lowering the allowed_max_length from 1,024 to 512 or 256 can also help manage memory problems.\n\n# Calculate baseline train and validation loss (before any fine-tuning).\nmodel.to(device)\ntorch.manual_seed(123)\n\nwith torch.no_grad():\n    train_loss = calc_loss_loader(\n        data_loader=train_loader, model=model, device=device, num_batches=5\n    )\n    val_loss = calc_loss_loader(\n        data_loader=val_loader, model=model, device=device, num_batches=5\n    )\n\nprint(f\"Training loss: {train_loss}\")\nprint(f\"Validation loss: {val_loss}\")\n\n\nA note on weight decay\nAdamW implements weight decay by subtracting a scaled version of the weights from the parameter update, rather than modifying the loss function like L2 regularization. This decoupling of weight decay from the gradient calculation ensures that momentum and adaptive learning rates in Adam are not affected by weight decay, leading to more consistent and effective regularization (see this post or this post or this post)\n\nWeight Decay vs. L2 Regularization:\n\nWeight Decay: Modifies the parameter update step to penalize large weights. It directly subtracts a portion of the weights from the update, effectively shrinking them towards zero.\nL2 Regularization: Modifies the loss function by adding a penalty term proportional to the squared magnitude of the weights. This penalty term increases the loss for large weights, making it more difficult for the model to learn large values.\n\nAdamW’s Approach:\n\nAdamW implements weight decay by directly subtracting a scaled version of the weights from the parameter update, without changing the loss function.\nThis ensures that the momentum and adaptive learning rates in Adam, which are crucial for efficient training, are not affected by the weight decay process.\n\nMathematical Representation: Let’s consider the following:\n\n\n\\(\\theta_t\\): The weights at iteration \\(t\\).\n\\(\\nabla L(\\theta_t)\\): The gradient of the loss function with respect to the weights at iteration \\(t\\).\n\\(\\eta_t\\): The learning rate at iteration \\(t\\).\n\\(\\lambda\\): The weight decay hyperparameter.\n\nAdamW Update Rule: \\[\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\nabla L(\\theta_t) - \\eta_t \\cdot \\lambda \\cdot \\theta_t\\]\nExplanation: - The first term (\\(- \\eta_t \\cdot \\nabla L(\\theta_t)\\)) is the standard gradient descent update. - The second term (\\(- \\eta_t \\cdot \\lambda \\cdot \\theta_t\\)) is the weight decay term. It subtracts a portion of the weights (\\(\\lambda \\cdot \\theta_t\\)) from the update, scaled by the learning rate (\\(\\eta_t\\)). - This direct subtraction ensures that the weights are gradually shrunk towards zero during training.\n\nAdvantages of AdamW:\n\nConsistent Regularization: AdamW applies weight decay directly to the parameters, ensuring consistent regularization regardless of the magnitude of the gradients.\nImproved Generalization: By effectively shrinking weights towards zero, AdamW can help prevent overfitting and improve the model’s ability to generalize to unseen data.\nBetter Convergence: AdamW can lead to faster and more stable convergence during training, especially when dealing with large models and datasets.\n\n\n\n# Set the seed for reproducibility.\ntorch.manual_seed(123)\n\n# Initialize the optimizer.\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n\n# Set the number of epochs.\nnum_epochs = 2\n\n# Start the timer.\nstart_time = time.time()\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model,\n    train_loader,\n    val_loader,\n    optimizer,\n    device,\n    num_epochs=num_epochs,\n    eval_freq=5,\n    eval_iter=5,\n    start_context=format_input(val_data[0]),\n    tokenizer=tokenizer,\n)\n\n# Calculate the execution time.\n# NOTE: With an NVIDIA NVIDIA RTX 5000 GPU (32GB VRAM), this should take about 0.70 minutes.\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#trainingvalidation-loss-curves",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#trainingvalidation-loss-curves",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Training/validation loss curves",
    "text": "Training/validation loss curves\nFrom the loss plot shown in figure 7.17, we can see that the model’s performance on both the training and validation sets improves substantially over the course of training. The rapid decrease in losses during the initial phase indicates that the model quickly learns meaningful patterns and representations from the data. Then, as training progresses to the second epoch, the losses continue to decrease but at a slower rate, suggesting that the model is fine-tuning its learned representations and converging to a stable solution.\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#extracting-and-saving-responses",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#extracting-and-saving-responses",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Extracting and saving responses",
    "text": "Extracting and saving responses\nThis is for evaluating the model’s performance on the hold-out dataset which requires generating a response for each input in the test dataset.\n\n\n\nEvaluation\n\n\n\nSpot-checking examples\n\n# Spot check a few examples.\ntorch.manual_seed(123)\n\n# Iterates over the first three test set samples\nfor entry in test_data[:3]:\n    # Format the input instructions.\n    input_text = format_input(entry)\n\n    # Generate a response from the model.\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=NEW_CONFIG.context_length,\n        eos_id=50256,\n    )\n\n    # Decode the generated token IDs into text.\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n\n    # Remove the \"### Response:\" prefix and strip any leading or trailing whitespace.\n    response_text = (\n        generated_text[len(input_text) :].replace(\"### Response:\", \"\").strip()\n    )\n\n    print(\"---------- INPUT ----------------------\")\n    print(input_text)\n    print(\"---------- CORRECT RESPONSE ------------\")\n    print(f\"\\nCorrect response:\\n&gt;&gt; {entry['output']}\")\n    print(\"---------- MODEL RESPONSE --------------\")\n    print(f\"\\nModel response:\\n&gt;&gt; {response_text.strip()}\")\n    print(\"-------------------------------------\")\n\n\n\nGenerate responses for the entire test set\n\n# Generate responses for the entire test set.\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n    # Generate the response.\n    input_text = format_input(entry)\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=NEW_CONFIG.context_length,\n        eos_id=50256,\n    )\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n    response_text = (\n        generated_text[len(input_text) :].replace(\"### Response:\", \"\").strip()\n    )\n\n    # Update the test data with the model's response.\n    test_data[i][\"model_response\"] = response_text\n\n# Save the test data with the model's responses for later use.\nwith open(\"instruction-data-with-response.json\", \"w\") as file:\n    json.dump(test_data, file, indent=4)\n\n\n\nSave the fine-tuned model\n\nimport re\n\n# Removes white spaces and parentheses from file name.\nfile_name = f\"{re.sub(r'[ ()]', '', model_name) }-sft.pth\"\ntorch.save(model.state_dict(), file_name)\nprint(f\"Model saved as {file_name}\")\n\n\n\nLoad the fine-tuned model\n\nmodel.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))\nmodel.eval()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#evaluating-the-fine-tuned-model",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#evaluating-the-fine-tuned-model",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Evaluating the fine-tuned model",
    "text": "Evaluating the fine-tuned model\nThis section details the implementation of a method to automate the response evaluation of the fine-tuned LLM using another, larger LLM. To evaluate test set responses in an automated fashion, we utilize an existing instruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI. This model can be run locally using the open source Ollama application (https://ollama.com).\nNOTE: Ollama is an efficient application for running LLMs on a laptop. It serves as a wrapper around the open source llama.cpp library (https://github.com/ggerganov/llama.cpp), which implements LLMs in pure C/C++ to maximize efficiency. However, Ollama is only a tool for generating text using LLMs (inference) and does not support training or fine-tuning LLMs.\nThe 8-billion-parameter Llama 3 model is a very capable LLM that runs locally. However, it’s not as capable as large proprietary LLMs such as GPT-4 offered by OpenAI. For readers interested in exploring how to utilize GPT-4 through the OpenAI API to assess generated model responses, an optional code notebook is available within the supplementary materials accompanying this book at https://mng.bz/BgEv.\n\n\n\nRunning Ollama\n\n\n\nUtility functions for Ollama\n\n# Utility function to verify Ollama is running.\ndef check_if_running(process_name: str) -&gt; bool:\n    \"\"\"Check if the specified process is running.\"\"\"\n    running = False\n    for proc in psutil.process_iter([\"name\"]):\n        if process_name in proc.info[\"name\"]:\n            running = True\n            break\n\n    return running\n\n\nollama_running = check_if_running(\"ollama\")\nif not ollama_running:\n    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n\nprint(\"Ollama running:\", check_if_running(\"ollama\"))\n\n\n# REST API-based query function for Ollama.\nLOCAL_HOST_OLLAMA_URL = \"http://localhost:11434/api/chat\"\n\n\ndef query_model(prompt: str, model: str = \"llama3\", url: str = LOCAL_HOST_OLLAMA_URL):\n    \"\"\"Query the Ollama model using the REST API.\n\n    Args:\n        prompt: The prompt to query the model with.\n        model: The model to query.\n        url: The URL of the Ollama server.\n\n    Returns:\n        The response from the model.\n    \"\"\"\n    # Creates the data payload as a dictionary\n    data = {\n        \"model\": model,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        # Settings for deterministic responses\n        \"options\": {\n            \"seed\": 123,\n            \"temperature\": 0,\n            \"num_ctx\": 2048,\n        },\n    }\n\n    # Converts the dictionary to a JSON-formatted string and encodes it to bytes.\n    payload = json.dumps(data).encode(\"utf-8\")\n\n    # Creates a request object, setting the method to POST and adding necessary headers.\n    request = urllib.request.Request(url, data=payload, method=\"POST\")\n    request.add_header(\"Content-Type\", \"application/json\")\n\n    # Sends the request and captures the response.\n    response_data = \"\"\n    with urllib.request.urlopen(request) as response:\n        while True:\n            # Reads the response line by line.\n            line = response.readline().decode(\"utf-8\")\n            if not line:\n                break\n\n            # Parses the JSON-formatted line into a dictionary.\n            response_json = json.loads(line)\n\n            # Appends the response content to the response data.\n            response_data += response_json[\"message\"][\"content\"]\n\n    return response_data\n\n\n\nTest the REST API call to Ollama\n\nmodel = \"llama3\"\nresult = query_model(\"What do Llamas eat?\", model)\nprint(result)\n\n\n\nScore the instruction fine-tuned responses via Ollama\n\n# Evaluate the fine-tuned model and score some examples.\n\nfor entry in test_data[:3]:\n    prompt = (\n        f\"Given the input `{format_input(entry)}` \"\n        f\"and correct output `{entry['output']}`, \"\n        f\"score the model response `{entry['model_response']}`\"\n        f\" on a scale from 0 to 100, where 100 is the best score. \"\n    )\n    print(\"\\nDataset response:\")\n    print(\"&gt;&gt;\", entry[\"output\"])\n    print(\"\\nModel response:\")\n    print(\"&gt;&gt;\", entry[\"model_response\"])\n    print(\"\\nScore:\")\n    print(\"&gt;&gt;\", query_model(prompt))\n    print(\"\\n-------------------------\")\n\n\n\nNumeric model scoring\nIt’s worth noting that Ollama is not entirely deterministic across operating systems at the time of this writing, which means that the scores you obtain might vary slightly from the previous scores. To obtain more robust results, you can repeat the evaluation multiple times and average the resulting scores.\nTo further improve our model’s performance, we can explore various strategies, such as: - Adjusting the hyperparameters during fine-tuning, such as the learning rate, batch size, or number of epochs - Increasing the size of the training dataset or diversifying the examples to cover a broader range of topics and styles - Experimenting with different prompts or instruction formats to guide the model’s responses more effectively - Using a larger pretrained model, which may have greater capacity to capture complex patterns and generate more accurate responses\n\ndef generate_model_scores(\n    json_data: List[Dict[str, Any]], json_key: str, model: str = \"llama3\"\n) -&gt; List[int]:\n    \"\"\"Generate scores for a model based on a JSON dataset.\n\n    Args:\n        json_data: The JSON dataset to score.\n        json_key: The key in the JSON dataset to score.\n        model: The model to score with.\n\n    Returns:\n        A list of scores.\n    \"\"\"\n    scores = []\n    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n        # Modified instruction line to only return the numeric score (without any explanation).\n        prompt = (\n            f\"Given the input `{format_input(entry)}` \"\n            f\"and correct output `{entry['output']}`, \"\n            f\"score the model response `{entry[json_key]}`\"\n            f\" on a scale from 0 to 100, where 100 is the best score. \"\n            f\"Respond with the integer number only.\"\n        )\n\n        # Query the model and get the score.\n        score = query_model(prompt, model)\n\n        # Try to convert the score to an integer.\n        try:\n            scores.append(int(score))\n        except ValueError:\n            print(f\"Could not convert score: {score}\")\n            continue\n\n    return scores\n\n\nscores = generate_model_scores(test_data, \"model_response\")\nprint(f\"Number of scores: {len(scores)} of {len(test_data)}\")\nprint(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "",
    "text": "This notebook explores attention mechanisms (including self-attention) based on Sebastian Raschka’s book (Chapter 3), implementing basic self-attention, causal self-attention, and multi-headed self-attention (as shown in the figure below).\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 3\n\n\n\n\n\nAttention mechanisms\n\n\n\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#resources",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 3\n\n\n\n\n\nAttention mechanisms\n\n\n\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A simple example (single query vector)",
    "text": "A simple example (single query vector)\n\n\n\nSelf-attention example\n\n\nThese attention scores \\(\\omega_{ij}\\) are then normalized to arrive at attention weights \\(\\alpha_{ij}\\). Normalization helps with interpretability and maintaining stability during the training process of LLMs.\n\n\n\nSelf-attention example continued\n\n\nComputing the context vector is simply the attention-weighted sum of all input elements.\n\n\n\nContext vector computation\n\n\n\n# Define the input sequence (T = 6).\ninputs = torch.tensor(\n    [\n        [0.43, 0.15, 0.89],  # Your    (x^1)\n        [0.55, 0.87, 0.66],  # journey (x^2)\n        [0.57, 0.85, 0.64],  # starts  (x^3)\n        [0.22, 0.58, 0.33],  # with    (x^4)\n        [0.77, 0.25, 0.10],  # one     (x^5)\n        [0.05, 0.80, 0.55],  # step    (x^6)\n    ]\n)\n\n# Compute the attention scores (for the second element of the sequence x^2)\n# NOTE: The attention score computes similarity based on the dot product of the query and key vectors,\n#       which measures how aligned the query and key vectors are (a higher dot product indicates a\n#       greater degree of alignment, i.e. similarity between two vectors). A dot product is essentially\n#       a concise way of multiplying two vectors element-wise and summing the result.\n# NOTE: In the context of self-attention, the dot product determines the amount of attention the query\n#       should \"pay\" to each key in the input sequence.\n\n# 1. Via basic for-loops.\nquery = inputs[1]  # Python uses 0-based indexing.\nattn_scores = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores[i] = torch.dot(x_i, query)\nprint(attn_scores)\n\n\n# 2. Via matrix multiplication.\nattn_scores_mm = (\n    query @ inputs.T\n)  # The @ operator is syntactic sugar for matrix multiplication.\nprint(attn_scores_mm)\n\n# Verify that the two methods yield the same attention scores.\nassert torch.allclose(attn_scores, attn_scores_mm)\n\n# Normalize the attention scores to get the attention weights.\n# 1. Via a naive approach.\nattn_weights = attn_scores / torch.sum(attn_scores)\nprint(f\"Attention weights: {attn_weights} (sum: {torch.sum(attn_weights)})\")\n\n# 2. Via the softmax function.\n# NOTE: Softmax handles extreme values more gracefully and offers more favorable gradient properties\n#       during training.\n# NOTE: Since the softmax function ensures that attention weights are always positive and sum to 1,\n#       we can interpret the attention weights as probabilities.\nattn_weights_softmax = torch.nn.functional.softmax(attn_scores, dim=0)\nprint(\n    f\"Attention weights: {attn_weights_softmax} (sum: {torch.sum(attn_weights_softmax)})\"\n)\n\n# Compute the context vector z(2) for the query vector x(2).\n# 1. Via a naive approach via a for-loop.\ncontext_vector_2 = torch.zeros(inputs.shape[1])\nfor i, x_i in enumerate(inputs):\n    context_vector_2 += attn_weights_softmax[i] * x_i\nprint(f\"Context vector: {context_vector_2}\")\n\n# 2. Via matrix multiplication.\ncontext_vector_2_mm = attn_weights_softmax @ inputs\nprint(f\"Context vector: {context_vector_2_mm}\")\n\n# Verify that the two methods yield the same attention scores.\nassert torch.allclose(context_vector_2, context_vector_2_mm)\n\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\nAttention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656]) (sum: 1.0000001192092896)\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]) (sum: 1.0)\nContext vector: tensor([0.4419, 0.6515, 0.5683])\nContext vector: tensor([0.4419, 0.6515, 0.5683])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-batch-query",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-batch-query",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A simple example (batch query)",
    "text": "A simple example (batch query)\n\n\n\nBatched attention weight computation\n\n\nA small side-note on tensor initialization.\n\ntorch.empty\n\nCreates a tensor with uninitialized data - the tensor will be allocated in memory but the values are not initialized\nThe tensor contains whatever values were already in the allocated memory block (garbage values)\nIt’s faster than torch.zeros because it skips the step of initializing all values\n\ntorch.zeros\n\nCreates a tensor filled with the scalar value 0\nExplicitly initializes all elements of the tensor to zero\nSlightly slower than torch.empty because it needs to set all values to zero\n\n\nWhen to use which: - Use torch.zeros when you need a tensor initialized with zeros (most common use case) - Use torch.empty when: - You’ll immediately overwrite all values in the tensor - Performance is critical and you don’t care about initial values - You’re creating a buffer that will be filled later\n\n\n\nComputation flow\n\n\n\n# Initialize the full attention weights matrix (a square matrix of shape (T, T)).\nprint(f\"Inputs: 'Your journey starts with one step'\")\nprint(f\"Inputs shape: {inputs.shape}\")\n\n# Compute the unnormalized attention scores.\n# 1. Via a naive approach via nested for-loops.\nattn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\nfor i, x_i in enumerate(inputs):  # Iterate over the rows of the inputs tensor\n    for j, x_j in enumerate(inputs):  # Iterate over the columns of the inputs tensor\n        attn_scores[i, j] = torch.dot(\n            x_i, x_j\n        )  # Compute the dot product of the row and column vectors\n\n# 2. Via matrix multiplication.\nattn_scores_mm = inputs @ inputs.T\nassert torch.allclose(attn_scores, attn_scores_mm)\nprint(f\"Unnormalized attention scores:\\n{attn_scores_mm}\\n\")\n\n# Normalize the attention scores to get the attention weights.\nattn_weights = torch.nn.functional.softmax(attn_scores_mm, dim=1)\nprint(f\"Normalized attention scores:\\n{attn_weights}\")\nprint(f\"Sum of attention weights for each row: {attn_weights.sum(dim=1)}\")\n\n# Compute the context vectors for all query vectors.\ncontext_vectors = attn_weights @ inputs\nprint(f\"Context vectors:\\n{context_vectors}\")\n\nInputs: 'Your journey starts with one step'\nInputs shape: torch.Size([6, 3])\nUnnormalized attention scores:\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n\nNormalized attention scores:\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\nSum of attention weights for each row: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\nContext vectors:\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector-1",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector-1",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A simple example (single query vector)",
    "text": "A simple example (single query vector)\nUnlike in the simplified self-attention mechanism, scaled dot-product attention computes attention scores not on raw token embeddings but on the tokens projected into key and value space (via weight matrices \\(W_k\\) and \\(W_v\\)).\n\n\n\nScaled dot-product attention example\n\n\nComputing the normalized attention weights \\(\\alpha_{ij}\\) from unnormalized attention scores \\(\\omega_{ij}\\) is done via the softmax function as before. This time, however, we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (hence the name scaled dot-product attention).\nNOTE: The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate (see page 69 in Sebastian Raschka’s book).\n\n\n\nScaled dot-product attention example continued\n\n\nThe last step is to compute the context vector for \\(x^{(2)}\\), which is the weighted sum of all value vectors of the input sequence (i.e. the input tokens embedded via the \\(W_v\\) matrix).\n\n\n\nScaled dot-product attention example continued\n\n\n\n# Define input and output embedding size of the W_i embedding matrices.\n# NOTE: For GPT-like models, the input embedding size is typically equal to the output embedding\n#       size.\nx_2 = inputs[1]  # Python uses 0-based indexing.\nd_in = inputs.shape[1]  # The size of the input embedding dimension.\nd_out = 2  # The size of the output embedding dimension.\nprint(f\"Input token / shape: {x_2} ({x_2.shape})\")\n\n# Instantiate the trainable weight matrices.\n# NOTE: requires_grad=False is done here to reduce visual clutter in the outputs. When training the\n#       model, requires_grad obviously has to be set to True to update the weights during training.\ntorch.manual_seed(123)\nW_q = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\nW_k = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\nW_v = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n\n# Project the query input token into query, key, and value vectors.\nquery_2 = x_2 @ W_q\nkey_2 = x_2 @ W_k\nvalue_2 = x_2 @ W_v\nprint(f\"Weight matrix shape: {W_q.shape}\")\nprint(f\"Projected query vector: {query_2}\")\n\n# Compute key and value vectors for all input tokens.\n# NOTE: Computing the context vector for the query vector x(2) requires the key and value vectors of\n#       all input tokens.\nkeys = inputs @ W_k\nvalues = inputs @ W_v\nprint(f\"Keys shape: {keys.shape}\")\nprint(f\"Values shape: {values.shape}\")\n\n# Compute the unnormalized attention scores (for the query vector x(2) first).\nkeys_2 = keys[1]\nattn_scores_2 = query_2.dot(keys_2)\nprint(f\"Unnormalized attention score for x^2: {attn_scores_2}\")\n\n# Compute the unnormalized attention scores for all input tokens.\nattn_scores = query_2 @ keys.T\nprint(f\"Unnormalized attention scores: {attn_scores}\")\n\n# Normalize the attention scores to get the attention weights.\nd_k = keys.shape[1]\nattn_weights = torch.nn.functional.softmax(attn_scores / d_k**0.5, dim=-1)\nprint(f\"Attention weights: {attn_weights}\")\n\n# Compute the context vector for the query vector x(2).\ncontext_vector_2 = torch.zeros(d_out)\nfor i, v_i in enumerate(values):\n    context_vector_2 += attn_weights[i] * v_i\n\ncontext_vector_2_mm = attn_weights @ values\nassert torch.allclose(context_vector_2, context_vector_2_mm)\nprint(f\"Context vector: {context_vector_2}\")\n\nInput token / shape: tensor([0.5500, 0.8700, 0.6600]) (torch.Size([3]))\nWeight matrix shape: torch.Size([3, 2])\nProjected query vector: tensor([-1.1729, -0.0048])\nKeys shape: torch.Size([6, 2])\nValues shape: torch.Size([6, 2])\nUnnormalized attention score for x^2: 0.13763877749443054\nUnnormalized attention scores: tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809])\nAttention weights: tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117])\nContext vector: tensor([0.2854, 0.4081])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-self-attention-class",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-self-attention-class",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A self-attention class",
    "text": "A self-attention class\nIn self-attention, we transform the input vectors in the input matrix X with the three weight matrices, \\(W_q\\), \\(W_k\\), and \\(W_v\\). The new compute the attention weight matrix based on the resulting queries (Q) and keys (K). Using the attention weights and values (V), we then compute the context vectors (Z).\n\n\n\nA Python class implementing self-attention\n\n\n\nclass SelfAttentionV1(nn.Module):\n    def __init__(self, d_in: int, d_out: int):\n        super().__init__()\n        self.W_q = nn.Parameter(torch.randn(d_in, d_out))\n        self.W_k = nn.Parameter(torch.randn(d_in, d_out))\n        self.W_v = nn.Parameter(torch.randn(d_in, d_out))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Project the input tokens into query, key, and value vectors.\n        query = x @ self.W_q\n        key = x @ self.W_k\n        value = x @ self.W_v\n\n        # Compute the unnormalized attention scores, i.e. the omegas.\n        attn_scores = query @ key.T\n\n        # Normalize the attention scores to get the attention weights, i.e. the alphas.\n        d_k = key.shape[-1]\n        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n\n        # Compute the full set of context vectors.\n        context_vectors = attn_weights @ value\n\n        return context_vectors\n\n\nclass SelfAttentionV2(nn.Module):\n    \"\"\"A Python class implementing self-attention.\n\n    V2 replaces the nn.Parameter objects with nn.Linear objects which effectively perform matrix\n    multiplication when the bias units are disabled.\n\n    One significant advantage of using nn.Linear objects is that nn.Linear has an optimized weight\n    initialization scheme that helps with stabilizing the training process and making it more\n    effective.\n    \"\"\"\n\n    def __init__(self, d_in: int, d_out: int, qkv_bias: bool = False):\n        super().__init__()\n        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Project the input tokens into query, key, and value vectors.\n        query = self.W_q(x)\n        key = self.W_k(x)\n        value = self.W_v(x)\n\n        # Compute the unnormalized attention scores, i.e. the omegas.\n        attn_scores = query @ key.T\n\n        # Normalize the attention scores to get the attention weights, i.e. the alphas.\n        d_k = key.shape[-1]\n        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n\n        # Compute the full set of context vectors.\n        context_vectors = attn_weights @ value\n\n        return context_vectors\n\n\n# Test the self-attention class.\ntorch.manual_seed(123)\nsa_v1 = SelfAttentionV1(d_in, d_out)\nsa_v2 = SelfAttentionV2(d_in, d_out)\n\n# NOTE: SelfAttentionV1 and SelfAttentionV2 give different outputs because they use different\n#       initial weights for the weight matrices since nn.Linear uses a more sophisticated weight\n#       initialization scheme.\nprint(f\"Context vector 2 (from before): {context_vector_2_mm}\")\nprint(f\"Context vectors (V1):\\n{sa_v1(inputs)}\")\nprint(f\"Context vectors (V2):\\n{sa_v2(inputs)}\")\n\nContext vector 2 (from before): tensor([0.2854, 0.4081])\nContext vectors (V1):\ntensor([[0.2845, 0.4071],\n        [0.2854, 0.4081],\n        [0.2854, 0.4075],\n        [0.2864, 0.3974],\n        [0.2863, 0.3910],\n        [0.2860, 0.4039]], grad_fn=&lt;MmBackward0&gt;)\nContext vectors (V2):\ntensor([[0.5322, 0.2491],\n        [0.5316, 0.2488],\n        [0.5316, 0.2488],\n        [0.5340, 0.2501],\n        [0.5331, 0.2497],\n        [0.5337, 0.2499]], grad_fn=&lt;MmBackward0&gt;)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#dropout-masking-additional-attention-weights",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#dropout-masking-additional-attention-weights",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "Dropout: Masking additional attention weights",
    "text": "Dropout: Masking additional attention weights\nDropout is a technique where randomly selected hidden layer units are ignored (or dropped out) which helps prevent overfitting during training because the model is not allowed to become overly reliant on any specific set of hidden layer units. Note that dropout is only used during training and disabled afterwards.\nDropout in self-attention is most commonly applied at two specific times: 1. after calculating the attention weights 2. after applying the attention weights to the value vectors\nHere we’ll apply dropout after applying the attention weights to the value vectors (which is the more common variant in practice).\n\n\n\nDropout in causal self-attention\n\n\n\ntorch.manual_seed(123)\n# Instantiate the dropout module (choose a dropout probability of 50%)\ndropout = torch.nn.Dropout(p=0.5)\n\n# Create some example data (a matrix of ones).\nexample = torch.ones(6, 6)\nprint(f\"Example:\\n{example}\")\n\n# NOTE: Applying dropout scales the outputs by a factor of 1/(1-p) during training. This means that\n#       during evaluation the module simply computes an identity function. This is done to compensate\n#       for the reduction of active elements and is crucial to maintain the overall balance of the\n#       attention weights as it ensures that the average influence of the attention mechanism remains\n#       consistent during both the training and inference phases.\n# See https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\nprint(f\"Dropout:\\n{dropout(example)}\")\n\n# Apply dropout to the attention weights.\nprint(f\"Dropout:\\n{dropout(attn_weights_masked_normalized)}\")\n\nExample:\ntensor([[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]])\nDropout:\ntensor([[2., 2., 2., 2., 2., 2.],\n        [0., 2., 0., 0., 0., 0.],\n        [0., 0., 2., 0., 2., 0.],\n        [2., 2., 0., 0., 0., 2.],\n        [2., 0., 0., 0., 0., 2.],\n        [0., 2., 0., 0., 0., 0.]])\nDropout:\ntensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.6622, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.4982, 0.0000, 0.5000, 0.0000, 0.0000],\n        [0.0000, 0.3974, 0.3975, 0.3993, 0.4024, 0.0000],\n        [0.3355, 0.3319, 0.0000, 0.0000, 0.3353, 0.3320]],\n       grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-compact-causal-attention-class",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-compact-causal-attention-class",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A compact causal attention class",
    "text": "A compact causal attention class\n\n# We want to ensure that our implementation works with batches of data (as produced by the\n# dataloader implemented in chapter 2).\nbatch = torch.stack([inputs, inputs], dim=0)\nprint(f\"Batch shape: {batch.shape}\")\n\n\nclass CausalAttention(nn.Module):\n    def __init__(\n        self,\n        d_in: int,\n        d_out: int,\n        context_length: int,\n        dropout_prob: float = 0.1,\n        qkv_bias: bool = False,\n    ):\n        super().__init__()\n        # Cache d_out for later use.\n        self.d_out = d_out\n\n        # Initialize the weight matrices.\n        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n        # Initialize the dropout module.\n        # Compared to the previous implementation, we now use a dropout layer.\n        self.dropout = torch.nn.Dropout(p=dropout_prob)\n\n        # Register a buffer for the mask.\n        # NOTE: Buffers are not trained and are not subject to gradient descent.\n        # NOTE: The use of register_buffer in PyTorch is not strictly necessary for all use cases\n        #       but offers several advantages here. For instance, when we use the CausalAttention\n        #       class in our LLM, buffers are automatically moved to the appropriate device (CPU or\n        #       GPU) along with our model, which will be relevant when training our LLM. This means\n        #       we don’t need to manually ensure these tensors are on the same device as your model\n        #       parameters, avoiding device mismatch errors.\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n\n    def forward(self, x: torch.Tensor, verbose: bool = False) -&gt; torch.Tensor:\n        # Extract input dimensions.\n        batch_size, num_tokens, d_in = x.shape\n\n        # Project input into query, key, and value vectors.\n        query = self.W_q(x)\n        key = self.W_k(x)\n        value = self.W_v(x)\n\n        # Compute the unnormalized attention scores.\n        # NOTE: We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n        attn_scores = query @ key.transpose(-2, -1)\n\n        # Apply the mask to the attention scores.\n        # NOTE: In PyTorch, operations with a trailing underscore are performed in-place, avoiding\n        #       unnecessary memory copies.\n        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n        if verbose:\n            print(\n                f\"Unnormalized causal attention scores (shape: {attn_scores.shape}):\\n{attn_scores}\"\n            )\n\n        # Normalize the attention scores.\n        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n        if verbose:\n            print(\n                f\"Normalized causal attention weights (shape: {attn_weights.shape}):\\n{attn_weights}\"\n            )\n\n        # Apply dropout to the attention weights.\n        attn_weights = self.dropout(attn_weights)\n\n        # Compute the context vectors.\n        context_vectors = attn_weights @ value\n        if verbose:\n            print(\n                f\"Context vectors (shape: {context_vectors.shape}):\\n{context_vectors}\"\n            )\n\n        return context_vectors\n\n\n# Test the causal attention class.\ntorch.manual_seed(123)\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\ncontext_vecs = ca(batch)\nprint(f\"Context vector shape: {context_vecs.shape}\")\n\nBatch shape: torch.Size([2, 6, 3])\nContext vector shape: torch.Size([2, 6, 2])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-views",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-views",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A note on views",
    "text": "A note on views\n\n# Example for reshaping a tensor from [2, 3, 4] to [2, 3, 2, 2] (via views)\nB, T, D = 2, 3, 4\nx = torch.randn((B, T, D))\nprint(x.shape)\nx_view = x.view(B, T, 2, 2)\nprint(x_view.shape)\n\ntorch.Size([2, 3, 4])\ntorch.Size([2, 3, 2, 2])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-batched-matrix-multiplications",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-batched-matrix-multiplications",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A note on batched matrix multiplications",
    "text": "A note on batched matrix multiplications\n\n# The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).\na = torch.tensor(\n    [\n        [\n            [\n                [0.2745, 0.6584, 0.2775, 0.8573],\n                [0.8993, 0.0390, 0.9268, 0.7388],\n                [0.7179, 0.7058, 0.9156, 0.4340],\n            ],\n            [\n                [0.0772, 0.3565, 0.1479, 0.5331],\n                [0.4066, 0.2318, 0.4545, 0.9737],\n                [0.4606, 0.5159, 0.4220, 0.5786],\n            ],\n        ]\n    ]\n)\n\n# Perform a batched matrix multiplication between a and a.transpose(2, 3), i.e. num_tokens and\n# head_dim are transposed.\n# NOTE: [1, 2, 3, 4] @ [1, 2, 4, 3] = [1, 2, 3, 3]\n# NOTE: In this case, the matrix multiplication implementation in PyTorch handles the\n#       four-dimensional input tensor so that the matrix multiplication is carried out between the\n#       two last dimensions (num_tokens, head_dim) and then repeated for the individual heads (as\n#       well as for each batch separately, i.e. the first dimension which here is just one element).\naat = a @ a.transpose(2, 3)\nprint(f\"Shape of aat: {aat.shape}\")\nprint(aat)\n\nShape of aat: torch.Size([1, 2, 3, 3])\ntensor([[[[1.3208, 1.1631, 1.2879],\n          [1.1631, 2.2150, 1.8424],\n          [1.2879, 1.8424, 2.0402]],\n\n         [[0.4391, 0.7003, 0.5903],\n          [0.7003, 1.3737, 1.0620],\n          [0.5903, 1.0620, 0.9912]]]])\n\n\n\n# A less compact version of the above operation is as follows:\nfirst_head = a[0, 0, :, :]\nfirst_res = first_head @ first_head.T\nprint(\"First head:\\n\", first_res)\n\nsecond_head = a[0, 1, :, :]\nsecond_res = second_head @ second_head.T\nprint(\"\\nSecond head:\\n\", second_res)\n\nFirst head:\n tensor([[1.3208, 1.1631, 1.2879],\n        [1.1631, 2.2150, 1.8424],\n        [1.2879, 1.8424, 2.0402]])\n\nSecond head:\n tensor([[0.4391, 0.7003, 0.5903],\n        [0.7003, 1.3737, 1.0620],\n        [0.5903, 1.0620, 0.9912]])"
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html",
    "href": "posts/2025_05_06_website_tech_stack/index.html",
    "title": "Tech Stack for This Website",
    "section": "",
    "text": "For the longest time I have been hosting some version of my website using WordPress, which was the style at the time (this is 2012 onwards but pre-ChatGPT). While I really enjoyed the flexibility and ease of use of WordPress, it’s always been cumbersome to actually host the website. My last setup was to use the most basic AWS EC2 instance and redirect my domain to it. That setup always came with a high maintenance burden given the fact that the container had to be kept up to date and the website content (i.e. the DB backing the website) had to be manually backed up."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#pros",
    "href": "posts/2025_05_06_website_tech_stack/index.html#pros",
    "title": "Tech Stack for This Website",
    "section": "Pros",
    "text": "Pros\n\nFull control, blazing fast, easy to integrate MDX (Markdown + React), perfect for code-heavy tutorials.\nCustom Domain: Easy to connect on both Vercel and Netlify."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#cons",
    "href": "posts/2025_05_06_website_tech_stack/index.html#cons",
    "title": "Tech Stack for This Website",
    "section": "Cons",
    "text": "Cons\n\nNeeds some dev setup. You write in Markdown or MDX."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#a-note-about-deployment",
    "href": "posts/2025_05_06_website_tech_stack/index.html#a-note-about-deployment",
    "title": "Tech Stack for This Website",
    "section": "A note about deployment",
    "text": "A note about deployment\nPublishing updates to the website is trivial with Quarto. Their publishing workflow is well-documented and well-integrated with Netlify. Overall, Quarto’s documenation is extensive which makes it easy to get started and get support. All that’s required is to create a _publish.yml file in the root directory of the repository and executing the following command:\nquarto publish netlify\nThe contents of the _publish.yml file look something like the following:\n- source: project\n  netlify:\n    - id: \"5f3abafe-68f9-4c1d-835b-9d668b892001\"\n      url: \"https://danielpickem.com\"\nThis tells Quarto to publish the website to Netlify and use the ID and URL from Netlify’s “Site settings” dashboard (see image below).\n\n\n\nNetlify Site Settings\n\n\nThis blog post is already a useful tutorial as I just had to look up how to deploy the updated website to Netlify!\nAnother useful resource was this blogpost about deploying a Github page via Netlify."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#quarto-templates",
    "href": "posts/2025_05_06_website_tech_stack/index.html#quarto-templates",
    "title": "Tech Stack for This Website",
    "section": "Quarto templates",
    "text": "Quarto templates\nQuarto also boasts an extensive gallery of templates for various use-cases (such as books, blogs, presentations, etc.). This website is using a blog template, in particular the one by Chris von Csefalvay - thanks Chris! Modifying an existing template gets you up and running really quickly, especially if you have an AI-assisted code editor like Cursor - but that’s a topic for another post.\n\n\n\nQuarto Blog Template\n\n\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Layer by Layer",
    "section": "",
    "text": "This blog is all about me learning in public and sharing what I discover along the way. If something I write helps even one person understand a concept better, I’ll consider this blog a success.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRobust Autonomy Emerges from Self-Play\n\n\n\npaper-review\n\nautonomous-driving\n\n\n\n\n\n\n\n\n\nJun 4, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following\n\n\n\nllms\n\nfine-tuning\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 6: Fine-tuning for Classification\n\n\n\nllms\n\nfine-tuning\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data\n\n\n\nllms\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 4: GPT from Scratch\n\n\n\nllms\n\ngpt\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 3: Attention Mechanisms\n\n\n\nllms\n\nattention\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 2: Dataloader Creation\n\n\n\nllms\n\ndataloader\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 1: Tokenization\n\n\n\nllms\n\npretraining\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a Large Language Model (From Scratch)\n\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nTech Stack for This Website\n\n\n\nwebsite\n\n\n\n\n\n\n\n\n\nMay 6, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Layer by Layer\n\n\n\nnews\n\nintroduction\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nDaniel Pickem\n\n\n\n\n\nNo matching items"
  }
]
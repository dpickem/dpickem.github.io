[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "A full list of my publications can be found on Google Scholar"
  },
  {
    "objectID": "publications/index.html#conference-proceedings",
    "href": "publications/index.html#conference-proceedings",
    "title": "Publications",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\nThe robotarium: A remotely accessible swarm robotics research testbed\nD. Pickem, P. Glotfelter, L. Wang, M. Mote, et al.\nIEEE International Conference on Robotics and Automation (ICRA), 2017\nThe GRITSBot in its natural habitat-a multi-robot testbed\nD. Pickem, M. Lee, M. Egerstedt\nIEEE International Conference on Robotics and Automation (ICRA), 2015\nRealizing simultaneous lane keeping and adaptive speed regulation on accessible mobile robot testbeds\nX. Xu, T. Waters, D. Pickem, P. Glotfelter, et al.\nIEEE Conference on Control Technology and Applications (CCTA), 2017\nA game-theoretic formulation of the homogeneous self-reconfiguration problem\nD. Pickem, M. Egerstedt\nIEEE Conference on Decision and Control (CDC), 2015\nSelf-reconfiguration using graph grammars for modular robotics\nD. Pickem, M. Egerstedt\nIFAC Proceedings Volumes, 2012\nComplete heterogeneous self-reconfiguration: Deadlock avoidance using hole-free assemblies\nD. Pickem, M. Egerstedt, J.S. Shamma\nIFAC Proceedings Volumes, 2013"
  },
  {
    "objectID": "publications/index.html#various",
    "href": "publications/index.html#various",
    "title": "Publications",
    "section": "Various",
    "text": "Various\n\nCaptain hindsight: An autonomous surface vessel\nD. Pickem, D. Morioniti, C. Taylor, et al.\nGeorgia Institute of Technology Technical Report, 2012"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "This notebook explores dataset/dataloader creation techniques based on Sebastian Raschka’s book (Chapter 2), implementing data sampling, batching, and other techniques.\n\n\nBorrowed from Manning’s Live Books\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\n\nimport re\nfrom typing import Any, Dict, List, Tuple\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0\n\ntiktoken version: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#dataset",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "Borrowed from Manning’s Live Books"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_2/chapter_02_dataset_creation.html#resources",
    "title": "LLMs From Scratch - Chapter 2: Dataloader Creation",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\n\nimport re\nfrom typing import Any, Dict, List, Tuple\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0\n\ntiktoken version: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "",
    "text": "This notebook explores pretraining process of LLMs based on Sebastian Raschka’s book (Chapter 5). In particular, it discusses the following:\n\nComputing the training and validation set losses to assess the quality of LLM-generated text during training\nImplementing a training function and pretraining the LLM\nSaving and loading model weights to continue training an LLM\nLoading pretrained weights from OpenAI\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 5\n\nPytorch Lightning - great tutorial collection\n\n\n\n\nTopic overview\n\n\n\n# This installs the ipynb package which enables importing functions defined in other notebooks.\n# %pip install ipynb\n\n\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nfrom ipynb.fs.full.chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\nfrom ipynb.fs.full.chapter_02_dataset_creation import create_dataloader_v1\n\n\n# Instantiate the GPT-2 configuration with shortened context length.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=256,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.1,\n    qkv_bias=False,\n)\n\n\n# Create two training examples in a batch.\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nbatch = []\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\n\n\n# Test the GPT model.\ntorch.manual_seed(123)\n\n# Run the model on the batch.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\nout = model(batch)\n\nprint(f\"Input batch: {batch}\")\nprint(f\"Output shape: {out.shape}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#resources",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 5\n\nPytorch Lightning - great tutorial collection\n\n\n\n\nTopic overview\n\n\n\n# This installs the ipynb package which enables importing functions defined in other notebooks.\n# %pip install ipynb\n\n\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nfrom ipynb.fs.full.chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\nfrom ipynb.fs.full.chapter_02_dataset_creation import create_dataloader_v1\n\n\n# Instantiate the GPT-2 configuration with shortened context length.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=256,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.1,\n    qkv_bias=False,\n)\n\n\n# Create two training examples in a batch.\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nbatch = []\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\n\n\n# Test the GPT model.\ntorch.manual_seed(123)\n\n# Run the model on the batch.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\nout = model(batch)\n\nprint(f\"Input batch: {batch}\")\nprint(f\"Output shape: {out.shape}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#text-to-token-conversion",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#text-to-token-conversion",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Text to token conversion",
    "text": "Text to token conversion\n\ndef text_to_token_ids(\n    text: str, tokenizer: Optional[tiktoken.Encoding] = None\n) -&gt; torch.Tensor:\n    \"\"\"Convert a text string to a tensor of token IDs.\n\n    Args:\n        text: The text to convert to token IDs.\n        tokenizer: The tokenizer to use.\n\n    Returns:\n        torch.Tensor: A tensor of token IDs.\n    \"\"\"\n    # Instantiate a default tokenizer (if non was provided).\n    # Tokenize the input text.\n    encoded = tokenizer.encode(text, allowed_special={\"&lt;|endoftext|&gt;\"})\n\n    # Convert the tokenized text to a tensor.\n    # NOTE: .unsqueeze(0) adds the batch dimension.\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n    return encoded_tensor"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#token-to-text-conversion",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#token-to-text-conversion",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Token to text conversion",
    "text": "Token to text conversion\n\ndef token_ids_to_text(\n    token_ids: torch.Tensor, tokenizer: Optional[tiktoken.Encoding] = None\n) -&gt; str:\n    \"\"\"Convert a tensor of token IDs to a text string.\n\n    Args:\n        token_ids: The tensor of token IDs to convert to text.\n        tokenizer: The tokenizer to use.\n\n    Returns:\n        str: The text string.\n    \"\"\"\n    # Instantiate a default tokenizer (if non was provided).\n    # NOTE: .squeeze(0) removes the batch dimension.\n    flat = token_ids.squeeze(0)\n    return tokenizer.decode(flat.tolist())\n\n\n# Test the text to token conversion.\nstart_context = \"Every effort moves you\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M.context_length,\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#example---step-by-step",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#example---step-by-step",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Example - step by step",
    "text": "Example - step by step\n\n# Develop the loss function using a batch of two simple examples.\ninputs = torch.tensor(\n    [[16833, 3626, 6100], [40, 1107, 588]],  # [\"every effort moves\", \"I really like\"]\n)\n\n# Define the targets, which are the next tokens in the sequences.\ntargets = torch.tensor(\n    [\n        [3626, 6100, 345],\n        [1107, 588, 11311],\n    ]  # [\" effort moves you\", \" really like chocolate\"]\n)\n\n# Compute the logits for the inputs.\n# NOTE: We disable gradient computation since gradients are only used for training.\nwith torch.no_grad():\n    logits = model(inputs)\n\n# Compute the probabilities of each token in the vocabulary.\n# NOTE: The shape of probas is [B, T, V] where\n#\n# B is the batch size\n# T is the sequence length\n# V is the vocabulary size.\nprobas = torch.softmax(logits, dim=-1)\nprint(f\"Probas shape: {probas.shape}\")\n\n# Step 3 and 4: Convert the probabilities to token IDs via a greedy decoding strategy.\ntoken_ids = torch.argmax(probas, dim=-1, keepdim=True)\n# Print both batches of token IDs.\nprint(\"Token IDs:\\n\", token_ids)\n\n# Step 5: Convert the token IDs back to text.\nprint(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\nprint(f\"Outputs batch 1:\" f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n\n\n# For each of the two input texts, we can print the initial softmax probability scores\n# corresponding to the target tokens using the following code:\n\nbatch_idx = 0\n# TODO: Why can't we just use probas[batch_idx, :, targets[batch_idx]] since T = 3?\ntarget_probas_1 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]\nprint(f\"probas.shape: {probas.shape}\")\nprint(\"Text 1:\", target_probas_1)\n\nbatch_idx = 1\ntarget_probas_2 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]\nprint(\"Text 2:\", target_probas_2)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#computing-the-loss-step-by-step",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#computing-the-loss-step-by-step",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Computing the loss step by step",
    "text": "Computing the loss step by step\n\n\n\nLoss computation\n\n\n\n# Compute the log probabilities of the target tokens.\n# NOTE: Working with logarithms of probability scores is more manageable in mathematical\n#       optimization than handling the scores directly.\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\nprint(f\"log_probas: {log_probas}\")\n\n# Compute the average log probability of the target tokens.\navg_log_probas = torch.mean(log_probas)\nprint(f\"avg_log_probas: {avg_log_probas}\")\n\n# The goal is to get the average log probability as close to 0 as possible by updating the model’s\n# weights as part of the training process. However, in deep learning, the common practice isn’t to\n# push the average log probability up to 0 but rather to bring the negative average log probability\n# down to 0. The negative average log probability is simply the average log probability multiplied\n# by –1.\nneg_avg_log_probas = avg_log_probas * -1\nprint(f\"neg_avg_log_probas: {neg_avg_log_probas}\")\n\n\n# As we can see, the logits tensor has three dimensions: batch size, number of tokens, and\n# vocabulary size. The targets tensor has two dimensions: batch size and number of tokens.\n# For the cross_entropy loss function in PyTorch, we want to flatten these tensors by combining\n# them over the batch dimension:\nprint(\"Logits shape:\", logits.shape)\nprint(\"Targets shape:\", targets.shape)\n\nlogits_flat = logits.flatten(0, 1)\ntargets_flat = targets.flatten()\nprint(\"Flattened logits:\", logits_flat.shape)\nprint(\"Flattened targets:\", targets_flat.shape)\n\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\nprint(loss)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#the-difference-between-cross-entropy-perplexity-and-kl-divergence",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#the-difference-between-cross-entropy-perplexity-and-kl-divergence",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "The difference between cross-entropy, perplexity, and KL-divergence",
    "text": "The difference between cross-entropy, perplexity, and KL-divergence\n\nCross-entropy\nCross-entropy measures how well a predicted probability distribution \\(q\\) matches a true distribution \\(p\\). It’s defined as:\n\\[\nH(p, q) = -\\sum_{x} p(x) \\log q(x)\n\\]\nwhere \\(x\\) runs over all possible events. Intuitively, it’s the average number of bits needed to encode samples from \\(p\\), if they’re encoded according to \\(q\\). The lower the cross-entropy, the closer \\(q\\) is to \\(p\\).\nAccording to Wikipedia, in information theory, the cross-entropy between two probability distributions \\({\\displaystyle p}\\) and \\({\\displaystyle q}\\), over the same underlying set of events, measures the average number of bits needed to identify an event drawn from the set when the coding scheme used for the set is optimized for an estimated probability distribution \\({\\displaystyle q}\\), rather than the true distribution \\({\\displaystyle p}\\).\nThis statement reflects a fundamental idea from information theory: cross-entropy measures the cost of encoding data from one distribution \\(p\\) under the assumptions of another distribution \\(q\\). The unit “bits” arises because we’re working in the context of binary information encoding. Intuitively, each bit represents a yes/no choice, and the cross-entropy tells us, on average, how many such choices we’d need to make to encode the true outcomes from \\(p\\), given that our model assigns probabilities according to \\(q\\).\n\nIf \\(q\\) perfectly matches \\(p\\), the encoding is as efficient as possible—this is essentially the entropy \\(H(p)\\) of the true distribution.\n\nIf \\(q\\) differs from \\(p\\), the encoder based on \\(q\\) will make less informed decisions, leading to longer or more error-prone codes on average.\n\nThe “lower” cross-entropy means we’re closer to the ideal scenario where \\(q \\approx p\\), which indicates our model (represented by \\(q\\)) is doing a better job of approximating the true distribution \\(p\\).\n\nConversely, a higher cross-entropy indicates that \\(q\\) diverges significantly from \\(p\\), causing inefficiencies and increasing the average number of bits needed.\n\nSo, the cross-entropy not only quantifies the difference between two distributions, but also translates that difference into the practical costs of encoding data.\nExample:\n- True distribution: \\(p = [0.7, 0.2, 0.1]\\) - Predicted distribution 1: \\(q_1 = [0.6, 0.3, 0.1]\\) - Predicted distribution 2: \\(q_2 = [0.9, 0.05, 0.05]\\) - \\(H(p, q_1)\\) will be lower than \\(H(p, q_2)\\), because \\(q_1\\) is closer to \\(p\\) than \\(q_2\\).\n\n\nPerplexity\nPerplexity is often used in language modeling and other probabilistic models to measure how well a model predicts a sample. It’s defined as the exponentiated average negative log-probability:\n\\[\n\\text{Perplexity}(p, q) = 2^{H(p, q)}\n\\]\nThis represents the effective number of choices the model assigns to each outcome. A lower perplexity means the model is more confident in its predictions. Perplexity is often viewed as a normalized measure of cross-entropy, expressed in terms of the equivalent branching factor. For instance, if a language model’s perplexity is 10, it implies the model is, on average, as uncertain as making a single choice out of 10 equally likely outcomes.\nAccording to Wikipedia, in information theory, perplexity is a measure of uncertainty in the value of a sample from a discrete probability distribution. The larger the perplexity, the less likely it is that an observer can guess the value which will be drawn from the distribution.\nFrom Sebastian Raschka’s book:\nPerplexity is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence.\nPerplexity measures how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset. Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution. Perplexity can be calculated as perplexity = torch.exp(loss), which returns tensor(48725.8203) when applied to the previously calculated loss.\nPerplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step. In the given example, this would translate to the model being unsure about which among 48,725 tokens in the vocabulary to generate as the next token.\nChatGPT provides a similar intuitive explanation. If we consider a language model predicting the next word in a sentence, perplexity provides a numerical summary of how uncertain or “perplexed” the model is, on average, when choosing among possible outcomes. A perplexity value of 10, for example, indicates that the model’s uncertainty is equivalent to having 10 equally likely choices for each word it predicts. In other words, lower perplexity means the model is more confident in its predictions, as it can narrow down the possible outcomes to a smaller, more focused set. Higher perplexity indicates greater uncertainty or poorer model performance, since the model must spread its probability mass across more outcomes, essentially “considering” a larger range of possibilities before making a prediction.\nThis interpretation of perplexity as a kind of “average branching factor” makes it particularly useful in evaluating the quality of language models. Instead of dealing with abstract bits or logarithms (as in cross-entropy), perplexity translates the model’s predictive efficiency into a form that’s more intuitive.\nExample:\n- Suppose a language model predicts a sentence like “The cat sat on the ____” with probabilities for possible words:\n- \\(p(\\text{mat})\\) = 0.8, \\(p(\\text{floor})\\) = 0.15, \\(p(\\text{roof})\\) = 0.05 - If the true word is “mat” and the model’s probabilities closely match this, the perplexity will be low.\n- If the model assigns much lower probability to “mat” and higher to other options, the perplexity will increase, indicating worse predictions.\n\n\nKL Divergence (Kullback-Leibler Divergence)\nKL divergence measures how one probability distribution ( q ) diverges from a reference distribution ( p ). It’s given by:\n\\[\nD_{KL}(p \\parallel q) = \\sum_{x} p(x) \\log\\frac{p(x)}{q(x)}\n\\]\nKL divergence is always non-negative and equals zero only when ( p = q ). Unlike cross-entropy, it explicitly quantifies the “distance” (in an information-theoretic sense) between the two distributions. While cross-entropy tells us how many bits are needed to encode ( p ) using ( q ), KL divergence tells us how many extra bits are needed compared to using the true distribution ( p ) itself.\nExample:\n- True distribution: p = [0.5, 0.5] - Predicted distribution 1: \\(q_1\\) = [0.6, 0.4] - Predicted distribution 2: \\(q_2\\) = [0.9, 0.1] - \\(D_{KL}(p \\parallel q_1)\\) is smaller than \\(D_{KL}(p \\parallel q_2)\\), because \\(q_1\\) is closer to \\(p\\).\n- If \\(q_1\\) becomes equal to \\(p\\), the KL divergence will be zero.\n\n\nComparing the Concepts\n\nCross-Entropy vs. KL Divergence:\n\nCross-entropy combines the entropy of \\(p\\), which is fixed for a given \\(p\\), and the KL divergence from \\(p\\) to \\(q\\):\n\\[\nH(p, q) = H(p) + D_{KL}(p \\parallel q)\n\\]\nWhile cross-entropy measures the total coding cost under \\(q\\), KL divergence isolates the inefficiency due to \\(q\\)’s divergence from \\(p\\).\n\nPerplexity and Cross-Entropy:\n\nPerplexity is derived directly from cross-entropy, converting the measure into an interpretable “average number of choices.” It essentially provides a more human-readable version of the model’s performance.\n\nBoth low perplexity and low cross-entropy indicate a better model fit, but perplexity is the exponential form and gives a more intuitive sense of the model’s uncertainty.\n\nPerplexity and KL Divergence:\n\nWhile perplexity is connected to cross-entropy, KL divergence is a more nuanced measure that focuses on how much \\(q\\) deviates from \\(p\\) rather than the raw efficiency of encoding.\n\nPerplexity doesn’t directly measure divergence; instead, it measures how well the model predicts, which can be related to divergence indirectly through the cross-entropy.\n\n\nIn summary, cross-entropy and perplexity are practical metrics for evaluating how well a predictive model matches a true distribution, with perplexity offering a more intuitive interpretation. KL divergence, on the other hand, is a more fundamental information-theoretic measure that quantifies how much one distribution differs from another, forming a building block for understanding the inefficiencies captured by cross-entropy."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-cross-entropy-loss-for-a-given-batch.",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-cross-entropy-loss-for-a-given-batch.",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Utility function to compute the cross-entropy loss for a given batch.",
    "text": "Utility function to compute the cross-entropy loss for a given batch.\n\ndef calc_loss_batch(\n    input_batch: torch.Tensor,\n    target_batch: torch.Tensor,\n    model: nn.Module,\n    device: torch.device,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the cross-entropy loss for a given batch.\n\n    Args:\n        input_batch: The input batch.\n        target_batch: The target batch.\n        model: The model.\n        device: The device to compute the loss on.\n\n    Returns:\n        The cross-entropy loss for the input batch.\n    \"\"\"\n    # Transfer the input and target batches to the device.\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    # Compute the logits for the input batch.\n    logits = model(input_batch)\n\n    # Compute the cross-entropy loss for the input batch.\n    # NOTE: We flatten the logits and targets to have a shape of B * T where B is the batch size.\n    #\n    # logits: [B, T, V] -&gt; [B * T, V]\n    # targets: [B, T] -&gt; [B * T]\n    loss = torch.nn.functional.cross_entropy(\n        logits.flatten(0, 1), target_batch.flatten()\n    )\n    return loss"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-loss-for-a-data-loader",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#utility-function-to-compute-the-loss-for-a-data-loader",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Utility function to compute the loss for a data loader",
    "text": "Utility function to compute the loss for a data loader\n\ndef calc_loss_loader(\n    data_loader: torch.utils.data.DataLoader,\n    model: nn.Module,\n    device: torch.device,\n    num_batches: Optional[int] = None,\n) -&gt; float:\n    \"\"\"Compute the cross-entropy loss for a given data loader.\n\n    Args:\n        data_loader: The data loader.\n        model: The model.\n        device: The device to compute the loss on.\n        num_batches: The number of batches to compute the loss on.\n\n    Returns:\n        The cross-entropy loss for the entire data loader.\n    \"\"\"\n    total_loss = 0.0\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        # Iteratives over all batches if no fixed num_batches is specified\n        num_batches = len(data_loader)\n    else:\n        # Reduces the number of batches to match the total number of batches in the data loader if\n        # num_batches exceeds the number of batches in the data loader.\n        num_batches = min(num_batches, len(data_loader))\n\n    # Iterate over all batches in the data loader (or a subset thereof).\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i &lt; num_batches:\n            # Compute the loss for the input batch.\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n\n            # Sum the loss for each batch.\n            total_loss += loss.item()\n        else:\n            break\n\n    # Return the average loss over the number of batches.\n    return total_loss / num_batches\n\n\n# Test the loss computation.\n\n# If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any\n# changes to the code.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"Using device: {device}\")\n\n# Disables gradient tracking for efficiency because we are not training yet.\nwith torch.no_grad():\n    # Via the “device” setting, we ensure the data is loaded onto the same device as the LLM model.\n    train_loss = calc_loss_loader(train_loader, model, device)\n    val_loss = calc_loss_loader(val_loader, model, device)\n\nprint(f\"Training loss  : {train_loss}\")\nprint(f\"Validation loss: {val_loss}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#evaluation-utilities",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#evaluation-utilities",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Evaluation utilities",
    "text": "Evaluation utilities\n\ndef evaluate_model(\n    model: nn.Module,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_iter: int,\n):\n    \"\"\"Evaluate the model on the training and validation sets.\"\"\"\n\n    # Set the model to evaluation mode.\n    # NOTE: In evaluation mode, certain layers like dropout are disabled to ensure stable,\n    #       reproducible results.\n    model.eval()\n\n    # Disables gradient tracking, which is not required during evaluation (to reduce the\n    # computational overhead).\n    with torch.no_grad():\n        train_loss = calc_loss_loader(\n            train_loader, model, device, num_batches=eval_iter\n        )\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n\n    # Sets the model back to training mode.\n    model.train()\n\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(\n    model: nn.Module,\n    tokenizer: tiktoken.core.Encoding,\n    device: torch.device,\n    start_context: str,\n) -&gt; None:\n    \"\"\"Generate and print a sample from the model.\"\"\"\n    # Set the model to evaluation mode.\n    model.eval()\n\n    # Get the context size from the model's positional embedding weight.\n    context_size = model.pos_emb.weight.shape[0]\n\n    # Encode the start context and move to the device.\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    # Generate the text.\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n        )\n\n    # Decode the generated text.\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(decoded_text.replace(\"\\n\", \" \"))\n\n    # Set the model back to training mode.\n    model.train()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#pretraining-function",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#pretraining-function",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Pretraining function",
    "text": "Pretraining function\n\ndef train_model_simple(\n    model: nn.Module,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    start_context: str,\n    tokenizer: tiktoken.core.Encoding,\n    num_epochs: int = 10,\n    eval_freq: int = 5,\n    eval_iter: int = 5,\n):\n    # Initializes lists to track losses and tokens seen.\n    # TODO: Tracking of training statistics can be done more efficiently and elegantly.\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    for epoch in range(num_epochs):\n        model.train()\n\n        # Start the main training loop.\n        # Use tqdm to show progress with epoch and local step information\n        for local_step, (input_batch, target_batch) in enumerate(\n            tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n        ):\n            # Resets loss gradients from the previous batch iteration.\n            optimizer.zero_grad()\n\n            # Compute the loss for the input batch.\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n\n            # Calculates loss gradients.\n            loss.backward()\n\n            # Updates model weights using loss gradients\n            optimizer.step()\n\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            # Optional evaluation step.\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(\n                    f\"Ep {epoch+1} (Step {global_step:06d}, Batch {local_step+1}): \"\n                    f\"Train loss {train_loss:.3f}, \"\n                    f\"Val loss {val_loss:.3f}\"\n                )\n\n        # Prints a sample text after each epoch.\n        generate_and_print_sample(model, tokenizer, device, start_context)\n\n    return train_losses, val_losses, track_tokens_seen\n\n\n# Test the training loop.\ntorch.manual_seed(123)\n\n# Instantiate the model and move it to the device.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\n\n# Instantiate the optimizer.\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n\n# Train the model.\nnum_epochs = 10\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model,\n    train_loader,\n    val_loader,\n    optimizer,\n    device,\n    start_context=\"Every effort moves you\",\n    tokenizer=tokenizer,\n)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#plot-losses",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#plot-losses",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Plot losses",
    "text": "Plot losses\n\nfrom typing import Tuple\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\ndef plot_losses(\n    epochs_seen: torch.Tensor,\n    tokens_seen: torch.Tensor,\n    train_losses: torch.Tensor,\n    val_losses: torch.Tensor,\n    figsize: Tuple[int, int] = (8, 6),\n):\n    \"\"\"Plot the training and validation losses.\"\"\"\n    fig, ax1 = plt.subplots(figsize=figsize)\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2 = ax1.twiny()\n    ax2.plot(tokens_seen, train_losses, alpha=0)\n    ax2.set_xlabel(\"Tokens seen\")\n    fig.tight_layout()\n    plt.show()\n\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#temperature-sampling",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#temperature-sampling",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Temperature sampling",
    "text": "Temperature sampling\nTemperature scaling is a technique that adds a probabilistic selection process to the next-token generation task. Instead of always sampling the token with the highest probability as the next token using torch.argmax, also known as greedy decoding, we can replace argmax with a function that samples from a probability distribution (to generate text with more variety).\n\n\n\nTemperature sampling\n\n\n\n# Use a small sample vocabulary to illustrate temperature sampling.\nvocab = {\n    \"closer\": 0,\n    \"every\": 1,\n    \"effort\": 2,\n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5,\n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8,\n}\ninverse_vocab = {v: k for k, v in vocab.items()}\n\n# Assume the LLM generated the following logits for the next token, i.e. \"every effort moves you\".\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n\n# We convert the logits into probabilities via the softmax function and obtain the token ID\n# corresponding to the generated token via the argmax function, which we can then map back\n# into text via the inverse vocabulary:\nprobas = torch.softmax(next_token_logits, dim=0)\nnext_token_id = torch.argmax(probas).item()\nprint(f\"Greedy decoding: {inverse_vocab[next_token_id]}\")\n\n\n# Instead of greedy decoding via argmax, we can sample from the probability distribution\n# to generate text with more variety (via a multinomial distribution).\n\n# This is done via replacing the argmax with a sampling process from an multinomial\n# distribution.\ntorch.manual_seed(123)\nnext_token_id = torch.multinomial(probas, num_samples=1).item()\nprint(f\"Temperature sampling: {inverse_vocab[next_token_id]}\")\n\n\ndef print_sampled_tokens(probas: torch.Tensor, num_samples: int = 1_000):\n    \"\"\"Print the sampled tokens from the probability distribution.\"\"\"\n    torch.manual_seed(123)\n    sample = [\n        torch.multinomial(probas, num_samples=1).item() for i in range(num_samples)\n    ]\n\n    sampled_ids = torch.bincount(torch.tensor(sample))\n    for i, freq in enumerate(sampled_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\n\n\nprint_sampled_tokens(probas)\n\n\n# We can further control the distribution and selection process via a concept called temperature\n# scaling. Temperature scaling is just a fancy description for dividing the logits by a number\n# greater than 0.\ndef softmax_with_temperature(logits: torch.Tensor, temperature: float) -&gt; torch.Tensor:\n    \"\"\"Apply softmax with temperature scaling.\"\"\"\n    scaled_logits = logits / temperature\n    return torch.softmax(scaled_logits, dim=0)\n\n\n# Sample with original, lower, and higher confidence.\n# NOTE: In the plot below, temperature scaling manifests itself with sharper (lower temperatures)\n#       or more diffuse (higher temperatures) probability distributions.\n# NOTE: A temperature of 1 corresponds to no temperature scaling.\n# NOTE: Decreasing the temperature to 0.1 sharpens the distribution, so the most likely token\n#       (here, “forward”) will have an even higher probability score. Likewise, increasing the\n#       temperature to 5 makes the distribution more uniform.\n# NOTE: A temperature of 5 results in a more uniform distribution where other tokens are selected\n#       more often. This can add more variety to the generated texts but also more often results\n#       in nonsensical text.\ntemperatures = [1, 0.1, 5]\n\n# Temperature scaling the logits.\nscaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n\n# Plot the results.\nx = torch.arange(len(vocab))\nbar_width = 0.15\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i, T in enumerate(temperatures):\n    rects = ax.bar(\n        x + i * bar_width, scaled_probas[i], bar_width, label=f\"Temperature = {T}\"\n    )\n\nax.set_ylabel(\"Probability\")\nax.set_xticks(x)\nax.set_xticklabels(vocab.keys(), rotation=90)\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#top-k-sampling",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#top-k-sampling",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Top-k sampling",
    "text": "Top-k sampling\nNaive temperature sampling with higher temperatures leads to potentially more interesting and creative outputs. However, one downside of this approach is that it sometimes leads to grammatically incorrect or completely nonsensical outputs.\nTop-k sampling, when combined with probabilistic sampling and temperature scaling, can improve the text generation results. In top-k sampling, we can restrict the sampled tokens to the top-k most likely tokens and exclude all other tokens from the selection process by masking their probability scores.\nThe top-k approach replaces all nonselected logits with negative infinity value (-inf), such that when computing the softmax values, the probability scores of the non-top-k tokens are 0, and the remaining probabilities sum up to 1 (this is a similar masking trick as in the causal attention module).\n\n\n\nTop-k sampling\n\n\n\n# Assume the LLM generated the following logits for the next token, i.e. \"every effort moves you\".\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n\n# Define the top-k value.\ntop_k = 3\n\n# Get the top-k logits and their positions.\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\nprint(f\"Top logits: {top_logits}\")\nprint(f\"Top positions: {top_pos}\")\n\n# Subsequently, we apply PyTorch’s where function to set the logit values of tokens that are below\n# the lowest logit value within our top-three selection to negative infinity (-inf):\n\nnew_logits = torch.where(\n    # Identifies logits less than the minimum in the top 3.\n    condition=next_token_logits &lt; top_logits[-1],\n    # Assigns –inf to these lower logits.\n    input=torch.tensor(float(\"-inf\")),\n    # Keeps the original logits for the top-k tokens.\n    other=next_token_logits,\n)\nprint(f\"New logits (top-k sampling): {new_logits}\")\n\n# Apply the softmax function to turn these into next-token probabilities.\n# NOTE: We can now apply the temperature scaling and multinomial function for probabilistic\n#       sampling to select the next token among these three non-zero probability scores to\n#       generate the next token.\ntopk_probas = torch.softmax(new_logits, dim=0)\nprint(f\"Top-k probabilities: {topk_probas}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#an-updated-text-generation-function",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#an-updated-text-generation-function",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "An updated text generation function",
    "text": "An updated text generation function\n\ndef generate(\n    model: nn.Module,\n    idx: torch.Tensor,\n    max_new_tokens: int,\n    context_size: int,\n    temperature: float = 0.0,\n    top_k: Optional[int] = None,\n    eos_id: Optional[int] = None,\n):\n    \"\"\"Generate text with the model.\n\n    Args:\n        model: The model to use for generation.\n        idx: The input tokens.\n        max_new_tokens: The maximum number of tokens to generate.\n        context_size: The size of the context window.\n        temperature: The temperature to use for sampling.\n        top_k: The top-k value to use for sampling.\n        eos_id: The end-of-sequence token.\n\n    Returns:\n        The generated tokens.\n    \"\"\"\n    # The for loop is the same as before: gets logits and only focuses on the last time step.\n    # NOTE: Generate at most max_new_tokens tokens.\n    for _ in range(max_new_tokens):\n        # Only consider the last context_size tokens (this is typically informed by the model's\n        # supported context length or length of the positional embedding weight).\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n\n        # Only consider the last time step (i.e. the next token).\n        logits = logits[:, -1, :]\n\n        # Optionally filter logits with top_k sampling.\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits &lt; min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits\n            )\n\n        # Optionally apply temperature scaling.\n        if temperature &gt; 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            # If temperature is 0, use greedy decoding.\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        # Stop generating if we encounter the EOS (end of sequence) token.\n        if idx_next == eos_id:\n            break\n\n        # Concatenate the next token to the running sequence.\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n\n# Test the generation function.\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=model.to(device),  # Ensure model is on the correct device\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n    max_new_tokens=15,\n    context_size=GPT_CONFIG_124M.context_length,\n    top_k=15,\n    temperature=1.4,\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#without-the-optimizer-state",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#without-the-optimizer-state",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Without the optimizer state",
    "text": "Without the optimizer state\n\n# Saving a PyTorch model is relatively straightforward. The recommended way is to save a model’s\n# state_dict, a dictionary mapping each layer to its parameters, using the torch.save function:\n# NOTE: For the GPT2-124M model, this results in a file of roughly 623M.\ntorch.save(model.state_dict(), \"model.pth\")\n\n\n# Loading the model is equally straightforward. Note, however, that one needs to reinitialize the\n# model architecture first and then load the state_dict into an existing model instance:\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\n\n# NOTE: Set the model to evaluation mode since a model is most likely loaded for inference tasks\n#       (since the optimizer state is not saved/loaded to/from disk).\nmodel.eval()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#with-the-optimizer-state",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#with-the-optimizer-state",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "With the optimizer state",
    "text": "With the optimizer state\n\n# If we plan to continue pre-training a model later—for example, using the train_model_simple\n# function we defined earlier in this chapter—saving the optimizer state is also recommended.\n\n# Adaptive optimizers such as AdamW store additional parameters for each model weight. AdamW uses\n# historical data to adjust learning rates for each model parameter dynamically. Without it, the\n# optimizer resets, and the model may learn suboptimally or even fail to converge properly, which\n# means it will lose the ability to generate coherent text.\n# NOTE: For the GPT2-124M model, this results in a file of roughly 1.9G (or almost 3x the size of\n#       the model weights alone).\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"model_and_optimizer.pth\",\n)\n\n\n# Define the checkpoint to load the model and optimizer states from disk.\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n\n# Load the model and optimizer states from the checkpoint.\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n# NOTE: Set the model to training mode since a model is most likely loaded for further training\n#       (since the optimizer state is saved/loaded to/from disk).\nmodel.train()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#creating-the-right-config",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#creating-the-right-config",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Creating the right config",
    "text": "Creating the right config\n\nfrom dataclasses import asdict\n\nasdict(GPT_CONFIG_124M)\n\n\n# Update the model configuration to conform to the model size.\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Instantiate a base config.\ntmp_config = asdict(GPT_CONFIG_124M)\n\n# Load the overlay parameters.\nmodel_name = \"gpt2-small (124M)\"\ntmp_config.update(model_configs[model_name])\n\n# Update the context length to match OpenAI's GPT-2 models.\ntmp_config.update({\"context_length\": 1024})\n\n# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n# working with pretrained weights, we need to match the settings for consistency and enable these\n# bias vectors.\ntmp_config.update({\"qkv_bias\": True})\n\n# Instantiate the new configuration.\nNEW_CONFIG = GPTConfig(**tmp_config)\n\n# Initialize the model with the new configuration.\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#loading-weights-into-the-model",
    "href": "posts/2025_05_16_llms_from_scratch_part_5/chapter_05_pretraining_on_unlabeled_data.html#loading-weights-into-the-model",
    "title": "LLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data",
    "section": "Loading weights into the model",
    "text": "Loading weights into the model\n\ndef assign(left, right):\n    \"\"\"Safely assign the right weight tensor to the left layer.\n\n    Checks whether two tensors or arrays (left and right) have the same dimensions or shape and\n    returns the right tensor as trainable PyTorch parameters.\n    \"\"\"\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n\n    return torch.nn.Parameter(torch.tensor(right))\n\n\ndef load_weights_into_gpt(gpt: GPTModel, params: dict):\n    # Sets the model’s positional and token embedding weights to those specified in params.\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n\n    # Iterates over each transformer block in the model.\n    for b in range(len(params[\"blocks\"])):\n        # The np.split function is used to divide the attention and bias weights into three equal\n        # parts for the query, key, and value components.\n        q_w, k_w, v_w = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.weight = assign(\n            gpt.trf_blocks[b].mha.W_q.weight, q_w.T\n        )\n        gpt.trf_blocks[b].mha.W_k.weight = assign(\n            gpt.trf_blocks[b].mha.W_k.weight, k_w.T\n        )\n        gpt.trf_blocks[b].mha.W_v.weight = assign(\n            gpt.trf_blocks[b].mha.W_v.weight, v_w.T\n        )\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.bias = assign(gpt.trf_blocks[b].mha.W_q.bias, q_b)\n        gpt.trf_blocks[b].mha.W_k.bias = assign(gpt.trf_blocks[b].mha.W_k.bias, k_b)\n        gpt.trf_blocks[b].mha.W_v.bias = assign(gpt.trf_blocks[b].mha.W_v.bias, v_b)\n        gpt.trf_blocks[b].mha.out_proj.weight = assign(\n            gpt.trf_blocks[b].mha.out_proj.weight,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].mha.out_proj.bias = assign(\n            gpt.trf_blocks[b].mha.out_proj.bias,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n        )\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].pre_attention_norm.scale = assign(\n            gpt.trf_blocks[b].pre_attention_norm.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_attention_norm.shift = assign(\n            gpt.trf_blocks[b].pre_attention_norm.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.scale = assign(\n            gpt.trf_blocks[b].pre_ff_norm.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.shift = assign(\n            gpt.trf_blocks[b].pre_ff_norm.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n        )\n\n        gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n        gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n\n        # The original GPT-2 model by OpenAI reused the token embedding weights in the output layer\n        # to reduce the total number of parameters, which is a concept known as weight tying.\n        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n\n\n# Load the weights into the model.\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\n\n\n# Test the model to verify that it can generate coherent text.\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=gpt,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=NEW_CONFIG.context_length,\n    top_k=50,\n    temperature=1.5,\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "",
    "text": "This notebook explores the fine-tuning process of LLMs with the purpose of creating instruction fine-tuned model based on Sebastian Raschka’s book (Chapter 7). In particular, it discusses the following:\n\nThe instruction fine-tuning process of LLMs\nPreparing a dataset for supervised instruction fine-tuning\nOrganizing instruction data in training batches\nLoading a pretrained LLM and fine-tuning it to follow human instructions\nExtracting LLM-generated instruction responses for evaluation\nEvaluating an instruction-fine-tuned LLM\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 7\n\n\n\n\n\nTopic overview\n\n\n\nimport dataclasses\nimport json\nimport functools\nimport os\nimport pathlib\nimport psutil\nfrom pprint import pprint\nimport time\nfrom typing import Any, Dict, List, Optional, Tuple\nimport urllib.request\nimport urllib\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\n\n# Chapter 4 dependencies.\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n)\n\n# Chapter 5 dependencies.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n    load_weights_into_gpt,\n    calc_loss_loader,\n    train_model_simple,\n    plot_losses,\n)\n\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#resources",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 7\n\n\n\n\n\nTopic overview\n\n\n\nimport dataclasses\nimport json\nimport functools\nimport os\nimport pathlib\nimport psutil\nfrom pprint import pprint\nimport time\nfrom typing import Any, Dict, List, Optional, Tuple\nimport urllib.request\nimport urllib\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\n\n# Chapter 4 dependencies.\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n)\n\n# Chapter 5 dependencies.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n    load_weights_into_gpt,\n    calc_loss_loader,\n    train_model_simple,\n    plot_losses,\n)\n\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#download-and-load-the-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#download-and-load-the-dataset",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Download and load the dataset",
    "text": "Download and load the dataset\nThe dataset consists of 1,100 instruction–response pairs. This dataset was created specifically for this book. The following code implements and executes a function to download this dataset, which is a relatively small file (only 204 KB) in JSON forma.\nAs we can see, the example entries are Python dictionary objects containing an instruction, input, and output.\nThe input field may occasionally be empty.\n\ndef download_and_load_file(file_path: pathlib.Path, url: str) -&gt; Dict[str, Any]:\n    \"\"\"Download and load a file from a URL.\n\n    Args:\n        file_path: The path to the file to download.\n        url: The URL to download the file from.\n\n    Returns:\n        The loaded data.\n    \"\"\"\n    # Skips download if file was already downloaded\n    if not os.path.exists(file_path):\n        with urllib.request.urlopen(url) as response:\n            text_data = response.read().decode(\"utf-8\")\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(text_data)\n    else:\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            text_data = file.read()\n\n    # Load and decode the data from the file.\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n\n    return data\n\n\nfile_path = pathlib.Path(\"data/instruction-data.json\")\nurl = (\n    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n)\ndata = download_and_load_file(file_path, url)\nprint(\"Number of entries:\", len(data))\npprint(data[50])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#prompt-formatting",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#prompt-formatting",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Prompt formatting",
    "text": "Prompt formatting\nInstruction fine-tuning involves training a model on a dataset where the input-output pairs, like those we extracted from the JSON file, are explicitly provided. There are various methods to format these entries for LLMs.\nThere are various methods to format these entries for LLMs, often referred to as prompt styles. The most commonly used ones are the following:\n\nAlpaca prompt style\nPhi-3 prompt style\n\nAlpaca was one of the early LLMs to publicly detail its instruction fine-tuning process. Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt styles. The rest of this notebook uses the Alpaca prompt style since it is one of the most popular ones, largely because it helped define the original approach to fine-tuning.\n\n\n\nPrompt styles\n\n\n\ndef format_input(entry: Dict[str, Any], add_output: bool = False) -&gt; str:\n    \"\"\"Format an entry for the Alpaca prompt style.\n\n    Args:\n        entry: A dictionary containing an `instruction` and `input` key.\n        add_output: Whether to add the `output` key to the formatted string.\n\n    Returns:\n        The formatted string.\n    \"\"\"\n    # Add the 'system' prompt and the entry's instruction.\n    instruction_text = (\n        f\"Below is an instruction that describes a task. \"\n        f\"Write a response that appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    # Add the entry's input if it exists.\n    # NOTE: The 'input' section is skipped if the field is empty.\n    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n\n    # Optionally add the desired response.\n    desired_response = f\"\\n\\n### Response:\\n{entry['output']}\" if add_output else \"\"\n\n    return instruction_text + input_text + desired_response\n\n\n# Format the example entry.\nmodel_input = format_input(data[50], add_output=True)\nprint(model_input)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#splitting-the-datast",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#splitting-the-datast",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Splitting the datast",
    "text": "Splitting the datast\n\n# TODO: This section should reuse functions from chapter 6.\n\n# Use 85% of the data for training, 10% for testing, and 5% for validation.\ntrain_portion = int(len(data) * 0.85)\ntest_portion = int(len(data) * 0.1)\nval_portion = len(data) - train_portion - test_portion\n\ntrain_data = data[:train_portion]\ntest_data = data[train_portion : train_portion + test_portion]\nval_data = data[train_portion + test_portion :]\n\nprint(\"Training set length:\", len(train_data))\nprint(\"Validation set length:\", len(val_data))\nprint(\"Test set length:\", len(test_data))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#organizing-data-into-training-batches",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#organizing-data-into-training-batches",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Organizing data into training batches",
    "text": "Organizing data into training batches\nIn this section, we learn how to efficiently pad the data samples to equal lengths so we can assemble multiple instruction examples in a batch.\nIn the previous chapter, the training batches were created automatically by the PyTorch DataLoader class, which employs a default collate function to combine lists of samples into batches. A collate function is responsible for taking a list of individual data samples and merging them into a single batch that can be processed efficiently by the model during training. Here, we create our own custom collate function to handle specific requirements and formatting (pre-tokenization and formatting of inputs) of our instruction dataset.\n\n\n\nBatching overview\n\n\n\nCreate a dataset class\nSimilar to the approach used for classification fine-tuning, we want to accelerate training by collecting multiple training examples in a batch, which necessitates padding all inputs to a similar length. As with classification fine-tuning, we use the &lt;|endoftext|&gt; token as a padding token.\n\n\n\nPrompt formatting and tokenization\n\n\n\nclass InstructionDataset(Dataset):\n    \"\"\"Dataset class for instruction fine-tuning.\"\"\"\n\n    def __init__(self, data: List[Dict[str, Any]], tokenizer: tiktoken.Encoding):\n        # Cache the raw and encoded texts.\n        self.data = data\n        self.encoded_texts = []\n\n        # Pretokenizes texts\n        for entry in data:\n            full_text = format_input(entry=entry, add_output=True)\n            self.encoded_texts.append(tokenizer.encode(full_text))\n\n    def __getitem__(self, index: int) -&gt; List[int]:\n        return self.encoded_texts[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ndataset = InstructionDataset(train_data, tokenizer)\nprint(f\"Length of dataset: {len(dataset)}\")\n\n\n# Instead of appending the &lt;|endoftext|&gt; tokens to the text inputs, we can append the token ID\n# corresponding to &lt;|endoftext|&gt; to the pretokenized inputs directly. We can use the tokenizer’s\n# .encode method on an &lt;|endoftext|&gt; token to remind us which token ID we should use:\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nprint(tokenizer.encode(\"&lt;|endoftext|&gt;\", allowed_special={\"&lt;|endoftext|&gt;\"}))\n\n\n\nCustom collate function\nThis custom collate function pads the training examples in each batch to the same length while allowing different batches to have different lengths, as demonstrated in the figure below. This approach minimizes unnecessary padding by only extending sequences to match the longest one in each batch, not the whole dataset.\n\n\n\nCustom collate function\n\n\n\ndef custom_collate_draft_1(\n    batch: List[List[int]], pad_token_id: int = 50256, device: str = \"cpu\"\n) -&gt; torch.Tensor:\n    \"\"\"Custom collate function for instruction fine-tuning.\n\n    Args:\n        batch: A list of lists of integers representing the training examples.\n        pad_token_id: The token ID to use for padding.\n        device: The device to move the resulting tensor to.\n\n    Returns:\n        A tensor of the padded inputs.\n    \"\"\"\n    # Find the longest sequence in the batch.\n    batch_max_length = max(len(item) + 1 for item in batch)\n\n    # Pad and prepare the inputs.\n    inputs_lst = []\n    for item in batch:\n        # Copy the item and append a single padding token.\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        # Pad the sequence to the longest sequence in the batch.\n        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n\n        # Convert the padded sequence to a tensor, remove the extra padded token added earlier, and\n        # add it to the list of inputs.\n        # NOTE: The purpose of this will become clear later in the second draft of this collate\n        #       function.\n        inputs = torch.tensor(padded[:-1])\n        inputs_lst.append(inputs)\n\n    # Stack the inputs into a single tensor and move it to the specified device.\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    return inputs_tensor\n\n\n# Test the custom collate function.\n# NOTE: This output shows all inputs have been padded to the length of the longest input list,\n#       inputs_1, containing five token IDs.\ninputs_1 = [0, 1, 2, 3, 4]\ninputs_2 = [5, 6]\ninputs_3 = [7, 8, 9]\nbatch = (\n    inputs_1,\n    inputs_2,\n    inputs_3,\n)\nprint(custom_collate_draft_1(batch))\n\n\n\nAdding target tokens to the collate function\nWe also need to create batches with the target token IDs corresponding to the batch of input IDs. These target IDs, as shown in the figure below, are crucial because they represent what we want the model to generate and what we need during training to calculate the loss for the weight updates. That is, we modify our custom collate function to return the target token IDs in addition to the input token IDs.\n\n\n\nTarget tokens in custom collate function\n\n\nSimilar to the process we used to pretrain an LLM, the target token IDs match the input token IDs but are shifted one position to the right.\n\n\n\nTarget tokens in custom collate function\n\n\n\ndef custom_collate_draft_2(\n    batch: List[List[int]], pad_token_id: int = 50256, device: str = \"cpu\"\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Custom collate function for instruction fine-tuning.\n\n    Args:\n        batch: A list of lists of integers representing the training examples.\n        pad_token_id: The token ID to use for padding.\n        device: The device to move the resulting tensor to.\n\n    Returns:\n        A tuple of tensors of the padded inputs and targets.\n    \"\"\"\n    # Find the longest sequence in the batch.\n    batch_max_length = max(len(item) + 1 for item in batch)\n\n    # Initialize the outputs.\n    inputs_lst, targets_lst = [], []\n\n    # Pad and prepare the inputs.\n    for item in batch:\n        # Copy the item and append a single padding token.\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        # Pad the sequence to the longest sequence in the batch.\n        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n\n        # Truncates the last token for inputs.\n        inputs = torch.tensor(padded[:-1])\n        # Shifts +1 to the right for targets.\n        targets = torch.tensor(padded[1:])\n\n        # Append the inputs and targets to the lists.\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    # Stack the inputs into a single tensor and move it to the specified device.\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor, targets_tensor\n\n\n# Test the custom collate function.\ninputs, targets = custom_collate_draft_2(batch)\nprint(inputs)\nprint(targets)\n\n\n\nReplace padding tokens with -100\nWe assign a -100 placeholder value to all padding tokens. This special value allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.\nThe default setting of the cross entropy function in PyTorch is cross_entropy(..., ignore_index=-100). This means that it ignores targets labeled with -100.\nHowever, note that we retain one end-of-text token, ID 50256, in the target list. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated response is complete.\n\n\n\nPadding tokens\n\n\nIn addition to masking out padding tokens, it is also common to mask out the target token IDs that correspond to the instruction, as illustrated in figure 7.13. By masking out the LLM’s target token IDs corresponding to the instruction, the cross entropy loss is only computed for the generated response target IDs. Thus, the model is trained to focus on generating accurate responses rather than memorizing instructions, which can help reduce overfitting.\n\n\n\nMasking out instructions\n\n\nAs of this writing, researchers are divided on whether masking the instructions is universally beneficial during instruction fine-tuning. For instance, the 2024 paper by Shi et al., Instruction Tuning With Loss Over Instructions, demonstrated that not masking the instructions benefits the LLM performance (see appendix B for more details). Here, we will not apply instruction masking.\n\ndef custom_collate_fn(\n    batch: List[List[int]],\n    pad_token_id: int = 50256,\n    ignore_index=-100,\n    allowed_max_length=None,\n    device: str = \"cpu\",\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Custom collate function for instruction fine-tuning.\n\n    Args:\n        batch: A list of lists of integers representing the training examples.\n        pad_token_id: The token ID to use for padding.\n        ignore_index: The value to use for padding tokens.\n        allowed_max_length: The maximum length of the input sequences.\n        device: The device to move the resulting tensor to.\n\n    Returns:\n        A tuple of tensors of the padded inputs and targets.\n    \"\"\"\n    # Find the longest sequence in the batch.\n    batch_max_length = max(len(item) + 1 for item in batch)\n\n    # Initialize the outputs.\n    inputs_lst, targets_lst = [], []\n\n    # Pad and prepare the inputs.\n    for item in batch:\n        # Copy the item and append a single padding token.\n        new_item = item.copy()\n        new_item += [pad_token_id]\n\n        # Pad the sequence to the longest sequence in the batch.\n        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n\n        # Truncates the last token for inputs.\n        inputs = torch.tensor(padded[:-1])\n        # Shifts +1 to the right for targets.\n        targets = torch.tensor(padded[1:])\n\n        # Replaces all but the first padding tokens in targets by ignore_index.\n        mask = targets == pad_token_id\n        indices = torch.nonzero(mask).squeeze()\n        if indices.numel() &gt; 1:\n            targets[indices[1:]] = ignore_index\n\n        # Optionally truncates to the maximum sequence length.\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]\n            targets = targets[:allowed_max_length]\n\n        # Append the inputs and targets to the lists.\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    # Stack the inputs into a single tensor and move it to the specified device.\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor, targets_tensor\n\n\n# Test the custom collate function.\ninputs, targets = custom_collate_fn(batch)\nprint(inputs)\nprint(targets)\n\n\n\nCreating the data loader\n\ncustomized_collate_fn = functools.partial(\n    custom_collate_fn, device=device, allowed_max_length=1024\n)\n\n\n# You can try to increase this number if parallel Python processes are supported by your operating\n# system.\nnum_workers = 0\nbatch_size = 8\n\n# Set the seed for reproducibility.\ntorch.manual_seed(123)\n\n# Create the datasets.\ntrain_dataset = InstructionDataset(train_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=True,\n    drop_last=True,\n    num_workers=num_workers,\n)\nval_dataset = InstructionDataset(val_data, tokenizer)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n)\ntest_dataset = InstructionDataset(test_data, tokenizer)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    collate_fn=customized_collate_fn,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n)\n\n# Print the shape of all batches in the train loader.\n# NOTE: Each batch contains 8 examples but the length of the sequences can vary from batch to batch.\nprint(f\"Train loader (length: {len(train_loader)}):\")\nfor inputs, targets in train_loader:\n    print(inputs.shape, targets.shape)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#loading-a-pre-trained-llm",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#loading-a-pre-trained-llm",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Loading a pre-trained LLM",
    "text": "Loading a pre-trained LLM\n\n# Load the base config.\nGPT_CONFIG = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Update the model configuration to conform to the model size.\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Instantiate a base config.\ntmp_config = dataclasses.asdict(GPT_CONFIG)\n\n# Load the overlay parameters.\nmodel_name = \"gpt2-medium (355M)\"\ntmp_config.update(model_configs[model_name])\n\n# Update the context length to match OpenAI's GPT-2 models.\ntmp_config.update({\"context_length\": 1024})\n\n# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n# working with pretrained weights, we need to match the settings for consistency and enable these\n# bias vectors.\ntmp_config.update({\"qkv_bias\": True})\n\n# Instantiate the new configuration.\nNEW_CONFIG = GPTConfig(**tmp_config)\n\n# Download the pretrained weights.\nmodel_size = model_name.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\nprint(f\"Downloading pretrained weights for {model_size} model...\")\nsettings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n\n# Initialize the model with the new configuration.\nmodel = GPTModel(NEW_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval()\n\n\n# Sanity check the model outputs on a random example.\ntorch.manual_seed(123)\n\n# Print an example from the validation set.\ninput_text = format_input(val_data[0])\nprint(input_text)\n\n\n# Generate a response from the model.\n# NOTE: The generate function returns the combined input and output text. This behavior was\n#       previously convenient since pretrained LLMs are primarily designed as text-completion\n#       models, where the input and output are concatenated to create coherent and legible\n#       text. However, when evaluating the model’s performance on a specific task, we often\n#       want to focus solely on the model’s generated response.\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(input_text, tokenizer),\n    max_new_tokens=35,\n    context_size=NEW_CONFIG.context_length,\n    eos_id=50256,\n)\n\n# Print the model's response without the input text (i.e. the instruction).\n# NOTE: To isolate the model’s response text, we need to subtract the length of the input\n#       instruction from the start of the generated_text.\ngenerated_text = token_ids_to_text(token_ids, tokenizer)\nresponse_text = generated_text[len(input_text) :].strip()\nprint(response_text)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#instruction-fine-tuning-the-llm",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#instruction-fine-tuning-the-llm",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Instruction fine-tuning the LLM",
    "text": "Instruction fine-tuning the LLM\n\nExercise 7.3 Fine-tuning on the original Alpaca dataset\n\nThe Alpaca dataset, by researchers at Stanford, is one of the earliest and most popular openly shared instruction datasets, consisting of 52,002 entries. As an alternative to the instruction-data.json file we use here, consider fine-tuning an LLM on this dataset. The dataset is available at https://mng.bz/NBnE. This dataset contains 52,002 entries, which is approximately 50 times more than those we used here, and most entries are longer. Thus, I highly recommend using a GPU to conduct the training, which will accelerate the fine-tuning process. If you encounter out-of-memory errors, consider reducing the batch_size from 8 to 4, 2, or even 1. Lowering the allowed_max_length from 1,024 to 512 or 256 can also help manage memory problems.\n\n# Calculate baseline train and validation loss (before any fine-tuning).\nmodel.to(device)\ntorch.manual_seed(123)\n\nwith torch.no_grad():\n    train_loss = calc_loss_loader(\n        data_loader=train_loader, model=model, device=device, num_batches=5\n    )\n    val_loss = calc_loss_loader(\n        data_loader=val_loader, model=model, device=device, num_batches=5\n    )\n\nprint(f\"Training loss: {train_loss}\")\nprint(f\"Validation loss: {val_loss}\")\n\n\nA note on weight decay\nAdamW implements weight decay by subtracting a scaled version of the weights from the parameter update, rather than modifying the loss function like L2 regularization. This decoupling of weight decay from the gradient calculation ensures that momentum and adaptive learning rates in Adam are not affected by weight decay, leading to more consistent and effective regularization (see this post or this post or this post)\n\nWeight Decay vs. L2 Regularization:\n\nWeight Decay: Modifies the parameter update step to penalize large weights. It directly subtracts a portion of the weights from the update, effectively shrinking them towards zero.\nL2 Regularization: Modifies the loss function by adding a penalty term proportional to the squared magnitude of the weights. This penalty term increases the loss for large weights, making it more difficult for the model to learn large values.\n\nAdamW’s Approach:\n\nAdamW implements weight decay by directly subtracting a scaled version of the weights from the parameter update, without changing the loss function.\nThis ensures that the momentum and adaptive learning rates in Adam, which are crucial for efficient training, are not affected by the weight decay process.\n\nMathematical Representation: Let’s consider the following:\n\n\n\\(\\theta_t\\): The weights at iteration \\(t\\).\n\\(\\nabla L(\\theta_t)\\): The gradient of the loss function with respect to the weights at iteration \\(t\\).\n\\(\\eta_t\\): The learning rate at iteration \\(t\\).\n\\(\\lambda\\): The weight decay hyperparameter.\n\nAdamW Update Rule: \\[\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\nabla L(\\theta_t) - \\eta_t \\cdot \\lambda \\cdot \\theta_t\\]\nExplanation: - The first term (\\(- \\eta_t \\cdot \\nabla L(\\theta_t)\\)) is the standard gradient descent update. - The second term (\\(- \\eta_t \\cdot \\lambda \\cdot \\theta_t\\)) is the weight decay term. It subtracts a portion of the weights (\\(\\lambda \\cdot \\theta_t\\)) from the update, scaled by the learning rate (\\(\\eta_t\\)). - This direct subtraction ensures that the weights are gradually shrunk towards zero during training.\n\nAdvantages of AdamW:\n\nConsistent Regularization: AdamW applies weight decay directly to the parameters, ensuring consistent regularization regardless of the magnitude of the gradients.\nImproved Generalization: By effectively shrinking weights towards zero, AdamW can help prevent overfitting and improve the model’s ability to generalize to unseen data.\nBetter Convergence: AdamW can lead to faster and more stable convergence during training, especially when dealing with large models and datasets.\n\n\n\n# Set the seed for reproducibility.\ntorch.manual_seed(123)\n\n# Initialize the optimizer.\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n\n# Set the number of epochs.\nnum_epochs = 2\n\n# Start the timer.\nstart_time = time.time()\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model,\n    train_loader,\n    val_loader,\n    optimizer,\n    device,\n    num_epochs=num_epochs,\n    eval_freq=5,\n    eval_iter=5,\n    start_context=format_input(val_data[0]),\n    tokenizer=tokenizer,\n)\n\n# Calculate the execution time.\n# NOTE: With an NVIDIA NVIDIA RTX 5000 GPU (32GB VRAM), this should take about 0.70 minutes.\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#trainingvalidation-loss-curves",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#trainingvalidation-loss-curves",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Training/validation loss curves",
    "text": "Training/validation loss curves\nFrom the loss plot shown in figure 7.17, we can see that the model’s performance on both the training and validation sets improves substantially over the course of training. The rapid decrease in losses during the initial phase indicates that the model quickly learns meaningful patterns and representations from the data. Then, as training progresses to the second epoch, the losses continue to decrease but at a slower rate, suggesting that the model is fine-tuning its learned representations and converging to a stable solution.\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#extracting-and-saving-responses",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#extracting-and-saving-responses",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Extracting and saving responses",
    "text": "Extracting and saving responses\nThis is for evaluating the model’s performance on the hold-out dataset which requires generating a response for each input in the test dataset.\n\n\n\nEvaluation\n\n\n\nSpot-checking examples\n\n# Spot check a few examples.\ntorch.manual_seed(123)\n\n# Iterates over the first three test set samples\nfor entry in test_data[:3]:\n    # Format the input instructions.\n    input_text = format_input(entry)\n\n    # Generate a response from the model.\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=NEW_CONFIG.context_length,\n        eos_id=50256,\n    )\n\n    # Decode the generated token IDs into text.\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n\n    # Remove the \"### Response:\" prefix and strip any leading or trailing whitespace.\n    response_text = (\n        generated_text[len(input_text) :].replace(\"### Response:\", \"\").strip()\n    )\n\n    print(\"---------- INPUT ----------------------\")\n    print(input_text)\n    print(\"---------- CORRECT RESPONSE ------------\")\n    print(f\"\\nCorrect response:\\n&gt;&gt; {entry['output']}\")\n    print(\"---------- MODEL RESPONSE --------------\")\n    print(f\"\\nModel response:\\n&gt;&gt; {response_text.strip()}\")\n    print(\"-------------------------------------\")\n\n\n\nGenerate responses for the entire test set\n\n# Generate responses for the entire test set.\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n    # Generate the response.\n    input_text = format_input(entry)\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=NEW_CONFIG.context_length,\n        eos_id=50256,\n    )\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n    response_text = (\n        generated_text[len(input_text) :].replace(\"### Response:\", \"\").strip()\n    )\n\n    # Update the test data with the model's response.\n    test_data[i][\"model_response\"] = response_text\n\n# Save the test data with the model's responses for later use.\nwith open(\"instruction-data-with-response.json\", \"w\") as file:\n    json.dump(test_data, file, indent=4)\n\n\n\nSave the fine-tuned model\n\nimport re\n\n# Removes white spaces and parentheses from file name.\nfile_name = f\"{re.sub(r'[ ()]', '', model_name) }-sft.pth\"\ntorch.save(model.state_dict(), file_name)\nprint(f\"Model saved as {file_name}\")\n\n\n\nLoad the fine-tuned model\n\nmodel.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))\nmodel.eval()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#evaluating-the-fine-tuned-model",
    "href": "posts/2025_05_16_llms_from_scratch_part_7/chapter_07_fine_tuning_to_follow_instructions.html#evaluating-the-fine-tuned-model",
    "title": "LLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following",
    "section": "Evaluating the fine-tuned model",
    "text": "Evaluating the fine-tuned model\nThis section details the implementation of a method to automate the response evaluation of the fine-tuned LLM using another, larger LLM. To evaluate test set responses in an automated fashion, we utilize an existing instruction-fine-tuned 8-billion-parameter Llama 3 model developed by Meta AI. This model can be run locally using the open source Ollama application (https://ollama.com).\nNOTE: Ollama is an efficient application for running LLMs on a laptop. It serves as a wrapper around the open source llama.cpp library (https://github.com/ggerganov/llama.cpp), which implements LLMs in pure C/C++ to maximize efficiency. However, Ollama is only a tool for generating text using LLMs (inference) and does not support training or fine-tuning LLMs.\nThe 8-billion-parameter Llama 3 model is a very capable LLM that runs locally. However, it’s not as capable as large proprietary LLMs such as GPT-4 offered by OpenAI. For readers interested in exploring how to utilize GPT-4 through the OpenAI API to assess generated model responses, an optional code notebook is available within the supplementary materials accompanying this book at https://mng.bz/BgEv.\n\n\n\nRunning Ollama\n\n\n\nUtility functions for Ollama\n\n# Utility function to verify Ollama is running.\ndef check_if_running(process_name: str) -&gt; bool:\n    \"\"\"Check if the specified process is running.\"\"\"\n    running = False\n    for proc in psutil.process_iter([\"name\"]):\n        if process_name in proc.info[\"name\"]:\n            running = True\n            break\n\n    return running\n\n\nollama_running = check_if_running(\"ollama\")\nif not ollama_running:\n    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n\nprint(\"Ollama running:\", check_if_running(\"ollama\"))\n\n\n# REST API-based query function for Ollama.\nLOCAL_HOST_OLLAMA_URL = \"http://localhost:11434/api/chat\"\n\n\ndef query_model(prompt: str, model: str = \"llama3\", url: str = LOCAL_HOST_OLLAMA_URL):\n    \"\"\"Query the Ollama model using the REST API.\n\n    Args:\n        prompt: The prompt to query the model with.\n        model: The model to query.\n        url: The URL of the Ollama server.\n\n    Returns:\n        The response from the model.\n    \"\"\"\n    # Creates the data payload as a dictionary\n    data = {\n        \"model\": model,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        # Settings for deterministic responses\n        \"options\": {\n            \"seed\": 123,\n            \"temperature\": 0,\n            \"num_ctx\": 2048,\n        },\n    }\n\n    # Converts the dictionary to a JSON-formatted string and encodes it to bytes.\n    payload = json.dumps(data).encode(\"utf-8\")\n\n    # Creates a request object, setting the method to POST and adding necessary headers.\n    request = urllib.request.Request(url, data=payload, method=\"POST\")\n    request.add_header(\"Content-Type\", \"application/json\")\n\n    # Sends the request and captures the response.\n    response_data = \"\"\n    with urllib.request.urlopen(request) as response:\n        while True:\n            # Reads the response line by line.\n            line = response.readline().decode(\"utf-8\")\n            if not line:\n                break\n\n            # Parses the JSON-formatted line into a dictionary.\n            response_json = json.loads(line)\n\n            # Appends the response content to the response data.\n            response_data += response_json[\"message\"][\"content\"]\n\n    return response_data\n\n\n\nTest the REST API call to Ollama\n\nmodel = \"llama3\"\nresult = query_model(\"What do Llamas eat?\", model)\nprint(result)\n\n\n\nScore the instruction fine-tuned responses via Ollama\n\n# Evaluate the fine-tuned model and score some examples.\n\nfor entry in test_data[:3]:\n    prompt = (\n        f\"Given the input `{format_input(entry)}` \"\n        f\"and correct output `{entry['output']}`, \"\n        f\"score the model response `{entry['model_response']}`\"\n        f\" on a scale from 0 to 100, where 100 is the best score. \"\n    )\n    print(\"\\nDataset response:\")\n    print(\"&gt;&gt;\", entry[\"output\"])\n    print(\"\\nModel response:\")\n    print(\"&gt;&gt;\", entry[\"model_response\"])\n    print(\"\\nScore:\")\n    print(\"&gt;&gt;\", query_model(prompt))\n    print(\"\\n-------------------------\")\n\n\n\nNumeric model scoring\nIt’s worth noting that Ollama is not entirely deterministic across operating systems at the time of this writing, which means that the scores you obtain might vary slightly from the previous scores. To obtain more robust results, you can repeat the evaluation multiple times and average the resulting scores.\nTo further improve our model’s performance, we can explore various strategies, such as: - Adjusting the hyperparameters during fine-tuning, such as the learning rate, batch size, or number of epochs - Increasing the size of the training dataset or diversifying the examples to cover a broader range of topics and styles - Experimenting with different prompts or instruction formats to guide the model’s responses more effectively - Using a larger pretrained model, which may have greater capacity to capture complex patterns and generate more accurate responses\n\ndef generate_model_scores(\n    json_data: List[Dict[str, Any]], json_key: str, model: str = \"llama3\"\n) -&gt; List[int]:\n    \"\"\"Generate scores for a model based on a JSON dataset.\n\n    Args:\n        json_data: The JSON dataset to score.\n        json_key: The key in the JSON dataset to score.\n        model: The model to score with.\n\n    Returns:\n        A list of scores.\n    \"\"\"\n    scores = []\n    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n        # Modified instruction line to only return the numeric score (without any explanation).\n        prompt = (\n            f\"Given the input `{format_input(entry)}` \"\n            f\"and correct output `{entry['output']}`, \"\n            f\"score the model response `{entry[json_key]}`\"\n            f\" on a scale from 0 to 100, where 100 is the best score. \"\n            f\"Respond with the integer number only.\"\n        )\n\n        # Query the model and get the score.\n        score = query_model(prompt, model)\n\n        # Try to convert the score to an integer.\n        try:\n            scores.append(int(score))\n        except ValueError:\n            print(f\"Could not convert score: {score}\")\n            continue\n\n    return scores\n\n\nscores = generate_model_scores(test_data, \"model_response\")\nprint(f\"Number of scores: {len(scores)} of {len(test_data)}\")\nprint(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "",
    "text": "This notebook explores attention mechanisms (including self-attention) based on Sebastian Raschka’s book (Chapter 3), implementing basic self-attention, causal self-attention, and multi-headed self-attention (as shown in the figure below).\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 3\n\n\n\n\n\nAttention mechanisms\n\n\n\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#resources",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 3\n\n\n\n\n\nAttention mechanisms\n\n\n\nimport torch\nimport torch.nn as nn"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A simple example (single query vector)",
    "text": "A simple example (single query vector)\n\n\n\nSelf-attention example\n\n\nThese attention scores \\(\\omega_{ij}\\) are then normalized to arrive at attention weights \\(\\alpha_{ij}\\). Normalization helps with interpretability and maintaining stability during the training process of LLMs.\n\n\n\nSelf-attention example continued\n\n\nComputing the context vector is simply the attention-weighted sum of all input elements.\n\n\n\nContext vector computation\n\n\n\n# Define the input sequence (T = 6).\ninputs = torch.tensor(\n    [\n        [0.43, 0.15, 0.89],  # Your    (x^1)\n        [0.55, 0.87, 0.66],  # journey (x^2)\n        [0.57, 0.85, 0.64],  # starts  (x^3)\n        [0.22, 0.58, 0.33],  # with    (x^4)\n        [0.77, 0.25, 0.10],  # one     (x^5)\n        [0.05, 0.80, 0.55],  # step    (x^6)\n    ]\n)\n\n# Compute the attention scores (for the second element of the sequence x^2)\n# NOTE: The attention score computes similarity based on the dot product of the query and key vectors,\n#       which measures how aligned the query and key vectors are (a higher dot product indicates a\n#       greater degree of alignment, i.e. similarity between two vectors). A dot product is essentially\n#       a concise way of multiplying two vectors element-wise and summing the result.\n# NOTE: In the context of self-attention, the dot product determines the amount of attention the query\n#       should \"pay\" to each key in the input sequence.\n\n# 1. Via basic for-loops.\nquery = inputs[1]  # Python uses 0-based indexing.\nattn_scores = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores[i] = torch.dot(x_i, query)\nprint(attn_scores)\n\n\n# 2. Via matrix multiplication.\nattn_scores_mm = (\n    query @ inputs.T\n)  # The @ operator is syntactic sugar for matrix multiplication.\nprint(attn_scores_mm)\n\n# Verify that the two methods yield the same attention scores.\nassert torch.allclose(attn_scores, attn_scores_mm)\n\n# Normalize the attention scores to get the attention weights.\n# 1. Via a naive approach.\nattn_weights = attn_scores / torch.sum(attn_scores)\nprint(f\"Attention weights: {attn_weights} (sum: {torch.sum(attn_weights)})\")\n\n# 2. Via the softmax function.\n# NOTE: Softmax handles extreme values more gracefully and offers more favorable gradient properties\n#       during training.\n# NOTE: Since the softmax function ensures that attention weights are always positive and sum to 1,\n#       we can interpret the attention weights as probabilities.\nattn_weights_softmax = torch.nn.functional.softmax(attn_scores, dim=0)\nprint(\n    f\"Attention weights: {attn_weights_softmax} (sum: {torch.sum(attn_weights_softmax)})\"\n)\n\n# Compute the context vector z(2) for the query vector x(2).\n# 1. Via a naive approach via a for-loop.\ncontext_vector_2 = torch.zeros(inputs.shape[1])\nfor i, x_i in enumerate(inputs):\n    context_vector_2 += attn_weights_softmax[i] * x_i\nprint(f\"Context vector: {context_vector_2}\")\n\n# 2. Via matrix multiplication.\ncontext_vector_2_mm = attn_weights_softmax @ inputs\nprint(f\"Context vector: {context_vector_2_mm}\")\n\n# Verify that the two methods yield the same attention scores.\nassert torch.allclose(context_vector_2, context_vector_2_mm)\n\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\nAttention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656]) (sum: 1.0000001192092896)\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]) (sum: 1.0)\nContext vector: tensor([0.4419, 0.6515, 0.5683])\nContext vector: tensor([0.4419, 0.6515, 0.5683])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-batch-query",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-batch-query",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A simple example (batch query)",
    "text": "A simple example (batch query)\n\n\n\nBatched attention weight computation\n\n\nA small side-note on tensor initialization.\n\ntorch.empty\n\nCreates a tensor with uninitialized data - the tensor will be allocated in memory but the values are not initialized\nThe tensor contains whatever values were already in the allocated memory block (garbage values)\nIt’s faster than torch.zeros because it skips the step of initializing all values\n\ntorch.zeros\n\nCreates a tensor filled with the scalar value 0\nExplicitly initializes all elements of the tensor to zero\nSlightly slower than torch.empty because it needs to set all values to zero\n\n\nWhen to use which: - Use torch.zeros when you need a tensor initialized with zeros (most common use case) - Use torch.empty when: - You’ll immediately overwrite all values in the tensor - Performance is critical and you don’t care about initial values - You’re creating a buffer that will be filled later\n\n\n\nComputation flow\n\n\n\n# Initialize the full attention weights matrix (a square matrix of shape (T, T)).\nprint(f\"Inputs: 'Your journey starts with one step'\")\nprint(f\"Inputs shape: {inputs.shape}\")\n\n# Compute the unnormalized attention scores.\n# 1. Via a naive approach via nested for-loops.\nattn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\nfor i, x_i in enumerate(inputs):  # Iterate over the rows of the inputs tensor\n    for j, x_j in enumerate(inputs):  # Iterate over the columns of the inputs tensor\n        attn_scores[i, j] = torch.dot(\n            x_i, x_j\n        )  # Compute the dot product of the row and column vectors\n\n# 2. Via matrix multiplication.\nattn_scores_mm = inputs @ inputs.T\nassert torch.allclose(attn_scores, attn_scores_mm)\nprint(f\"Unnormalized attention scores:\\n{attn_scores_mm}\\n\")\n\n# Normalize the attention scores to get the attention weights.\nattn_weights = torch.nn.functional.softmax(attn_scores_mm, dim=1)\nprint(f\"Normalized attention scores:\\n{attn_weights}\")\nprint(f\"Sum of attention weights for each row: {attn_weights.sum(dim=1)}\")\n\n# Compute the context vectors for all query vectors.\ncontext_vectors = attn_weights @ inputs\nprint(f\"Context vectors:\\n{context_vectors}\")\n\nInputs: 'Your journey starts with one step'\nInputs shape: torch.Size([6, 3])\nUnnormalized attention scores:\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n\nNormalized attention scores:\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\nSum of attention weights for each row: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\nContext vectors:\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector-1",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-simple-example-single-query-vector-1",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A simple example (single query vector)",
    "text": "A simple example (single query vector)\nUnlike in the simplified self-attention mechanism, scaled dot-product attention computes attention scores not on raw token embeddings but on the tokens projected into key and value space (via weight matrices \\(W_k\\) and \\(W_v\\)).\n\n\n\nScaled dot-product attention example\n\n\nComputing the normalized attention weights \\(\\alpha_{ij}\\) from unnormalized attention scores \\(\\omega_{ij}\\) is done via the softmax function as before. This time, however, we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (hence the name scaled dot-product attention).\nNOTE: The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate (see page 69 in Sebastian Raschka’s book).\n\n\n\nScaled dot-product attention example continued\n\n\nThe last step is to compute the context vector for \\(x^{(2)}\\), which is the weighted sum of all value vectors of the input sequence (i.e. the input tokens embedded via the \\(W_v\\) matrix).\n\n\n\nScaled dot-product attention example continued\n\n\n\n# Define input and output embedding size of the W_i embedding matrices.\n# NOTE: For GPT-like models, the input embedding size is typically equal to the output embedding\n#       size.\nx_2 = inputs[1]  # Python uses 0-based indexing.\nd_in = inputs.shape[1]  # The size of the input embedding dimension.\nd_out = 2  # The size of the output embedding dimension.\nprint(f\"Input token / shape: {x_2} ({x_2.shape})\")\n\n# Instantiate the trainable weight matrices.\n# NOTE: requires_grad=False is done here to reduce visual clutter in the outputs. When training the\n#       model, requires_grad obviously has to be set to True to update the weights during training.\ntorch.manual_seed(123)\nW_q = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\nW_k = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\nW_v = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n\n# Project the query input token into query, key, and value vectors.\nquery_2 = x_2 @ W_q\nkey_2 = x_2 @ W_k\nvalue_2 = x_2 @ W_v\nprint(f\"Weight matrix shape: {W_q.shape}\")\nprint(f\"Projected query vector: {query_2}\")\n\n# Compute key and value vectors for all input tokens.\n# NOTE: Computing the context vector for the query vector x(2) requires the key and value vectors of\n#       all input tokens.\nkeys = inputs @ W_k\nvalues = inputs @ W_v\nprint(f\"Keys shape: {keys.shape}\")\nprint(f\"Values shape: {values.shape}\")\n\n# Compute the unnormalized attention scores (for the query vector x(2) first).\nkeys_2 = keys[1]\nattn_scores_2 = query_2.dot(keys_2)\nprint(f\"Unnormalized attention score for x^2: {attn_scores_2}\")\n\n# Compute the unnormalized attention scores for all input tokens.\nattn_scores = query_2 @ keys.T\nprint(f\"Unnormalized attention scores: {attn_scores}\")\n\n# Normalize the attention scores to get the attention weights.\nd_k = keys.shape[1]\nattn_weights = torch.nn.functional.softmax(attn_scores / d_k**0.5, dim=-1)\nprint(f\"Attention weights: {attn_weights}\")\n\n# Compute the context vector for the query vector x(2).\ncontext_vector_2 = torch.zeros(d_out)\nfor i, v_i in enumerate(values):\n    context_vector_2 += attn_weights[i] * v_i\n\ncontext_vector_2_mm = attn_weights @ values\nassert torch.allclose(context_vector_2, context_vector_2_mm)\nprint(f\"Context vector: {context_vector_2}\")\n\nInput token / shape: tensor([0.5500, 0.8700, 0.6600]) (torch.Size([3]))\nWeight matrix shape: torch.Size([3, 2])\nProjected query vector: tensor([-1.1729, -0.0048])\nKeys shape: torch.Size([6, 2])\nValues shape: torch.Size([6, 2])\nUnnormalized attention score for x^2: 0.13763877749443054\nUnnormalized attention scores: tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809])\nAttention weights: tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117])\nContext vector: tensor([0.2854, 0.4081])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-self-attention-class",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-self-attention-class",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A self-attention class",
    "text": "A self-attention class\nIn self-attention, we transform the input vectors in the input matrix X with the three weight matrices, \\(W_q\\), \\(W_k\\), and \\(W_v\\). The new compute the attention weight matrix based on the resulting queries (Q) and keys (K). Using the attention weights and values (V), we then compute the context vectors (Z).\n\n\n\nA Python class implementing self-attention\n\n\n\nclass SelfAttentionV1(nn.Module):\n    def __init__(self, d_in: int, d_out: int):\n        super().__init__()\n        self.W_q = nn.Parameter(torch.randn(d_in, d_out))\n        self.W_k = nn.Parameter(torch.randn(d_in, d_out))\n        self.W_v = nn.Parameter(torch.randn(d_in, d_out))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Project the input tokens into query, key, and value vectors.\n        query = x @ self.W_q\n        key = x @ self.W_k\n        value = x @ self.W_v\n\n        # Compute the unnormalized attention scores, i.e. the omegas.\n        attn_scores = query @ key.T\n\n        # Normalize the attention scores to get the attention weights, i.e. the alphas.\n        d_k = key.shape[-1]\n        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n\n        # Compute the full set of context vectors.\n        context_vectors = attn_weights @ value\n\n        return context_vectors\n\n\nclass SelfAttentionV2(nn.Module):\n    \"\"\"A Python class implementing self-attention.\n\n    V2 replaces the nn.Parameter objects with nn.Linear objects which effectively perform matrix\n    multiplication when the bias units are disabled.\n\n    One significant advantage of using nn.Linear objects is that nn.Linear has an optimized weight\n    initialization scheme that helps with stabilizing the training process and making it more\n    effective.\n    \"\"\"\n\n    def __init__(self, d_in: int, d_out: int, qkv_bias: bool = False):\n        super().__init__()\n        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Project the input tokens into query, key, and value vectors.\n        query = self.W_q(x)\n        key = self.W_k(x)\n        value = self.W_v(x)\n\n        # Compute the unnormalized attention scores, i.e. the omegas.\n        attn_scores = query @ key.T\n\n        # Normalize the attention scores to get the attention weights, i.e. the alphas.\n        d_k = key.shape[-1]\n        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n\n        # Compute the full set of context vectors.\n        context_vectors = attn_weights @ value\n\n        return context_vectors\n\n\n# Test the self-attention class.\ntorch.manual_seed(123)\nsa_v1 = SelfAttentionV1(d_in, d_out)\nsa_v2 = SelfAttentionV2(d_in, d_out)\n\n# NOTE: SelfAttentionV1 and SelfAttentionV2 give different outputs because they use different\n#       initial weights for the weight matrices since nn.Linear uses a more sophisticated weight\n#       initialization scheme.\nprint(f\"Context vector 2 (from before): {context_vector_2_mm}\")\nprint(f\"Context vectors (V1):\\n{sa_v1(inputs)}\")\nprint(f\"Context vectors (V2):\\n{sa_v2(inputs)}\")\n\nContext vector 2 (from before): tensor([0.2854, 0.4081])\nContext vectors (V1):\ntensor([[0.2845, 0.4071],\n        [0.2854, 0.4081],\n        [0.2854, 0.4075],\n        [0.2864, 0.3974],\n        [0.2863, 0.3910],\n        [0.2860, 0.4039]], grad_fn=&lt;MmBackward0&gt;)\nContext vectors (V2):\ntensor([[0.5322, 0.2491],\n        [0.5316, 0.2488],\n        [0.5316, 0.2488],\n        [0.5340, 0.2501],\n        [0.5331, 0.2497],\n        [0.5337, 0.2499]], grad_fn=&lt;MmBackward0&gt;)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#dropout-masking-additional-attention-weights",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#dropout-masking-additional-attention-weights",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "Dropout: Masking additional attention weights",
    "text": "Dropout: Masking additional attention weights\nDropout is a technique where randomly selected hidden layer units are ignored (or dropped out) which helps prevent overfitting during training because the model is not allowed to become overly reliant on any specific set of hidden layer units. Note that dropout is only used during training and disabled afterwards.\nDropout in self-attention is most commonly applied at two specific times: 1. after calculating the attention weights 2. after applying the attention weights to the value vectors\nHere we’ll apply dropout after applying the attention weights to the value vectors (which is the more common variant in practice).\n\n\n\nDropout in causal self-attention\n\n\n\ntorch.manual_seed(123)\n# Instantiate the dropout module (choose a dropout probability of 50%)\ndropout = torch.nn.Dropout(p=0.5)\n\n# Create some example data (a matrix of ones).\nexample = torch.ones(6, 6)\nprint(f\"Example:\\n{example}\")\n\n# NOTE: Applying dropout scales the outputs by a factor of 1/(1-p) during training. This means that\n#       during evaluation the module simply computes an identity function. This is done to compensate\n#       for the reduction of active elements and is crucial to maintain the overall balance of the\n#       attention weights as it ensures that the average influence of the attention mechanism remains\n#       consistent during both the training and inference phases.\n# See https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\nprint(f\"Dropout:\\n{dropout(example)}\")\n\n# Apply dropout to the attention weights.\nprint(f\"Dropout:\\n{dropout(attn_weights_masked_normalized)}\")\n\nExample:\ntensor([[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]])\nDropout:\ntensor([[2., 2., 2., 2., 2., 2.],\n        [0., 2., 0., 0., 0., 0.],\n        [0., 0., 2., 0., 2., 0.],\n        [2., 2., 0., 0., 0., 2.],\n        [2., 0., 0., 0., 0., 2.],\n        [0., 2., 0., 0., 0., 0.]])\nDropout:\ntensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.6622, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.4982, 0.0000, 0.5000, 0.0000, 0.0000],\n        [0.0000, 0.3974, 0.3975, 0.3993, 0.4024, 0.0000],\n        [0.3355, 0.3319, 0.0000, 0.0000, 0.3353, 0.3320]],\n       grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-compact-causal-attention-class",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-compact-causal-attention-class",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A compact causal attention class",
    "text": "A compact causal attention class\n\n# We want to ensure that our implementation works with batches of data (as produced by the\n# dataloader implemented in chapter 2).\nbatch = torch.stack([inputs, inputs], dim=0)\nprint(f\"Batch shape: {batch.shape}\")\n\n\nclass CausalAttention(nn.Module):\n    def __init__(\n        self,\n        d_in: int,\n        d_out: int,\n        context_length: int,\n        dropout_prob: float = 0.1,\n        qkv_bias: bool = False,\n    ):\n        super().__init__()\n        # Cache d_out for later use.\n        self.d_out = d_out\n\n        # Initialize the weight matrices.\n        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n        # Initialize the dropout module.\n        # Compared to the previous implementation, we now use a dropout layer.\n        self.dropout = torch.nn.Dropout(p=dropout_prob)\n\n        # Register a buffer for the mask.\n        # NOTE: Buffers are not trained and are not subject to gradient descent.\n        # NOTE: The use of register_buffer in PyTorch is not strictly necessary for all use cases\n        #       but offers several advantages here. For instance, when we use the CausalAttention\n        #       class in our LLM, buffers are automatically moved to the appropriate device (CPU or\n        #       GPU) along with our model, which will be relevant when training our LLM. This means\n        #       we don’t need to manually ensure these tensors are on the same device as your model\n        #       parameters, avoiding device mismatch errors.\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n\n    def forward(self, x: torch.Tensor, verbose: bool = False) -&gt; torch.Tensor:\n        # Extract input dimensions.\n        batch_size, num_tokens, d_in = x.shape\n\n        # Project input into query, key, and value vectors.\n        query = self.W_q(x)\n        key = self.W_k(x)\n        value = self.W_v(x)\n\n        # Compute the unnormalized attention scores.\n        # NOTE: We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n        attn_scores = query @ key.transpose(-2, -1)\n\n        # Apply the mask to the attention scores.\n        # NOTE: In PyTorch, operations with a trailing underscore are performed in-place, avoiding\n        #       unnecessary memory copies.\n        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n        if verbose:\n            print(\n                f\"Unnormalized causal attention scores (shape: {attn_scores.shape}):\\n{attn_scores}\"\n            )\n\n        # Normalize the attention scores.\n        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n        if verbose:\n            print(\n                f\"Normalized causal attention weights (shape: {attn_weights.shape}):\\n{attn_weights}\"\n            )\n\n        # Apply dropout to the attention weights.\n        attn_weights = self.dropout(attn_weights)\n\n        # Compute the context vectors.\n        context_vectors = attn_weights @ value\n        if verbose:\n            print(\n                f\"Context vectors (shape: {context_vectors.shape}):\\n{context_vectors}\"\n            )\n\n        return context_vectors\n\n\n# Test the causal attention class.\ntorch.manual_seed(123)\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\ncontext_vecs = ca(batch)\nprint(f\"Context vector shape: {context_vecs.shape}\")\n\nBatch shape: torch.Size([2, 6, 3])\nContext vector shape: torch.Size([2, 6, 2])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-views",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-views",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A note on views",
    "text": "A note on views\n\n# Example for reshaping a tensor from [2, 3, 4] to [2, 3, 2, 2] (via views)\nB, T, D = 2, 3, 4\nx = torch.randn((B, T, D))\nprint(x.shape)\nx_view = x.view(B, T, 2, 2)\nprint(x_view.shape)\n\ntorch.Size([2, 3, 4])\ntorch.Size([2, 3, 2, 2])"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-batched-matrix-multiplications",
    "href": "posts/2025_05_16_llms_from_scratch_part_3/chapter_03_attention_mechanisms.html#a-note-on-batched-matrix-multiplications",
    "title": "LLMs From Scratch - Chapter 3: Attention Mechanisms",
    "section": "A note on batched matrix multiplications",
    "text": "A note on batched matrix multiplications\n\n# The shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4).\na = torch.tensor(\n    [\n        [\n            [\n                [0.2745, 0.6584, 0.2775, 0.8573],\n                [0.8993, 0.0390, 0.9268, 0.7388],\n                [0.7179, 0.7058, 0.9156, 0.4340],\n            ],\n            [\n                [0.0772, 0.3565, 0.1479, 0.5331],\n                [0.4066, 0.2318, 0.4545, 0.9737],\n                [0.4606, 0.5159, 0.4220, 0.5786],\n            ],\n        ]\n    ]\n)\n\n# Perform a batched matrix multiplication between a and a.transpose(2, 3), i.e. num_tokens and\n# head_dim are transposed.\n# NOTE: [1, 2, 3, 4] @ [1, 2, 4, 3] = [1, 2, 3, 3]\n# NOTE: In this case, the matrix multiplication implementation in PyTorch handles the\n#       four-dimensional input tensor so that the matrix multiplication is carried out between the\n#       two last dimensions (num_tokens, head_dim) and then repeated for the individual heads (as\n#       well as for each batch separately, i.e. the first dimension which here is just one element).\naat = a @ a.transpose(2, 3)\nprint(f\"Shape of aat: {aat.shape}\")\nprint(aat)\n\nShape of aat: torch.Size([1, 2, 3, 3])\ntensor([[[[1.3208, 1.1631, 1.2879],\n          [1.1631, 2.2150, 1.8424],\n          [1.2879, 1.8424, 2.0402]],\n\n         [[0.4391, 0.7003, 0.5903],\n          [0.7003, 1.3737, 1.0620],\n          [0.5903, 1.0620, 0.9912]]]])\n\n\n\n# A less compact version of the above operation is as follows:\nfirst_head = a[0, 0, :, :]\nfirst_res = first_head @ first_head.T\nprint(\"First head:\\n\", first_res)\n\nsecond_head = a[0, 1, :, :]\nsecond_res = second_head @ second_head.T\nprint(\"\\nSecond head:\\n\", second_res)\n\nFirst head:\n tensor([[1.3208, 1.1631, 1.2879],\n        [1.1631, 2.2150, 1.8424],\n        [1.2879, 1.8424, 2.0402]])\n\nSecond head:\n tensor([[0.4391, 0.7003, 0.5903],\n        [0.7003, 1.3737, 1.0620],\n        [0.5903, 1.0620, 0.9912]])"
  },
  {
    "objectID": "posts/2025_06_04_robust_autonomy_emerges_from_self_play/index.html",
    "href": "posts/2025_06_04_robust_autonomy_emerges_from_self_play/index.html",
    "title": "Robust Autonomy Emerges from Self-Play",
    "section": "",
    "text": "A few weeks ago I came across a paper titled “Robust Autonomy Emerges from Self-Play” in the TLDR newsletter (which is worth subscribing to if you want to stay on top of the latest news in AI, but that’s not the point of this post).\nThe above paper was interesting for many reasons not the least of which was a sentimental one. It was published by former colleagues of mine at Apple and appears to be the latest (and last?) public artifact of Apples self-driving efforts.\nOver ten authors contributed to the paper, but I’ve only had the pleasure of (tangentially) working with the following:\n\nBrody Huval\nStuart Bowers\nPhilipp Krähenbühl\nVladlen Koltun\n\nIn my opinion, the approach in the paper stands out for several reasons:\n\nThe complete absence of transformers or any attention mechanism. The model only uses MLPs and pooling layers.\nThe relatively small size of the model - it uses a total of just 6M parameters (3M for the actor and 3M for the critic), which seems tiny for a model controlling autonomous vehicles.\nThe fairly minimalistic reward function formulation that combines a handful of components to encode good driving behavior.\nThe enormous throughput of interactions with the simulation environment (4.4 billion state transitions per hour on an 8-GPU node).\nLast but not least, the emergence of realistic and robust driving behavior for a model that has never seen human-driving data.\n\nMy favorite part about this paper, though, was the conditioning input C to the model which modulates the policy’s behavior and enables inference-time modifications of agent behavior by simply changing conditioning inputs. More aggressive driving? Simply modify the weights on some reward function components. You want a truck instead of a passenger vehicle behavior? Increase the agent’s dimensions and dynamics through C and it will behave like a truck. A single model can be used to simulate a diverse variety of agents and behaviors - which is a powerful capability for realistic agents in simulation (and possibly even for a policy that runs on-vehicle).\nI enjoyed reading the paper to the point where I put together the following slide deck and presented it for an autonomous driving reading group.\n\n\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Layer by Layer",
    "section": "",
    "text": "This blog is all about me learning in public and sharing what I discover along the way. If something I write helps even one person understand a concept better, I’ll consider this blog a success.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProject Second Brain\n\n\n\nProductivity\n\nLearning\n\nCentralized Knowledge Base\n\n\n\n\n\n\n\n\n\nFeb 1, 2026\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything\n\n\n\nProductivity\n\nNote-Taking\n\n\n\n\n\n\n\n\n\nJan 13, 2026\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nThe Curious World of MCP Servers\n\n\n\nMCP\n\nProductivity\n\n\n\n\n\n\n\n\n\nDec 10, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nOpenAI Dev Day 2025 Recap\n\n\n\nOpenAI\n\nDev Day\n\n\n\n\n\n\n\n\n\nOct 6, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nRobust Autonomy Emerges from Self-Play\n\n\n\npaper-review\n\nautonomous-driving\n\n\n\n\n\n\n\n\n\nJun 4, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 7: Fine-tuning for Instruction Following\n\n\n\nllms\n\nfine-tuning\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 6: Fine-tuning for Classification\n\n\n\nllms\n\nfine-tuning\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 5: Pretraining from Unlabeled Data\n\n\n\nllms\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 4: GPT from Scratch\n\n\n\nllms\n\ngpt\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 3: Attention Mechanisms\n\n\n\nllms\n\nattention\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 2: Dataloader Creation\n\n\n\nllms\n\ndataloader\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nLLMs From Scratch - Chapter 1: Tokenization\n\n\n\nllms\n\npretraining\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a Large Language Model (From Scratch)\n\n\n\nLLMs\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nTech Stack for This Website\n\n\n\nwebsite\n\n\n\n\n\n\n\n\n\nMay 6, 2025\n\n\nDaniel Pickem\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Layer by Layer\n\n\n\nnews\n\nintroduction\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nDaniel Pickem\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking text into smaller units called “tokens.” These tokens serve as the basic building blocks that machine learning models can process.\nFor Large Language Models (LLMs), tokenization is a critical first step that converts human-readable text into numerical formats the model can understand. When you send a prompt to an LLM such as GPT or Claude, the model doesn’t directly read your text - it processes sequences of tokens that represent your text.\nThere are several approaches to tokenization:\n\nWord tokenization: Splitting text by words (separated by spaces or punctuation)\nSubword tokenization: Breaking words into meaningful subunits (most common in modern LLMs)\nCharacter tokenization: Dividing text into individual characters\n\nTokenization presents various challenges, including handling punctuation, contractions, compound words, and rare words. The choice of tokenization method significantly impacts an LLM’s performance, vocabulary size, and ability to handle different languages.\nThis notebook explores tokenization techniques based on Sebastian Raschka’s book (Chapter 2), implementing various tokenization approaches and analyzing their effects.\n\n\n\nBorrowed from Manning’s Live Books\n\n\n\nTokenization Process\n\n\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\n\n# Install dependencies.\n%pip install tiktoken\n\n\nimport re\nfrom typing import Dict, List\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#what-is-tokenization",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#what-is-tokenization",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Tokenization is a fundamental process in Natural Language Processing (NLP) that involves breaking text into smaller units called “tokens.” These tokens serve as the basic building blocks that machine learning models can process.\nFor Large Language Models (LLMs), tokenization is a critical first step that converts human-readable text into numerical formats the model can understand. When you send a prompt to an LLM such as GPT or Claude, the model doesn’t directly read your text - it processes sequences of tokens that represent your text.\nThere are several approaches to tokenization:\n\nWord tokenization: Splitting text by words (separated by spaces or punctuation)\nSubword tokenization: Breaking words into meaningful subunits (most common in modern LLMs)\nCharacter tokenization: Dividing text into individual characters\n\nTokenization presents various challenges, including handling punctuation, contractions, compound words, and rare words. The choice of tokenization method significantly impacts an LLM’s performance, vocabulary size, and ability to handle different languages.\nThis notebook explores tokenization techniques based on Sebastian Raschka’s book (Chapter 2), implementing various tokenization approaches and analyzing their effects."
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#tokenization-process",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#tokenization-process",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Borrowed from Manning’s Live Books\n\n\n\nTokenization Process"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#acknowledgment",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#resources",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#resources",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\n\n# Install dependencies.\n%pip install tiktoken\n\n\nimport re\nfrom typing import Dict, List\nimport urllib.request\n\nfrom importlib.metadata import version\nimport tiktoken\n\n\n# Verify library versions.\nprint(\"tiktoken version:\", version(\"tiktoken\"))  # expected: 0.7.0"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#algorithm-explained-via-a-simple-example",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#algorithm-explained-via-a-simple-example",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "Algorithm explained via a simple example",
    "text": "Algorithm explained via a simple example\n\n\n\nBPE algorithm explained via a simple example"
  },
  {
    "objectID": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#example-of-bpe-tokenization-for-unknown-words",
    "href": "posts/2025_05_14_llms_from_scratch_part_1/chapter_01_tokenization.html#example-of-bpe-tokenization-for-unknown-words",
    "title": "LLMs From Scratch - Chapter 1: Tokenization",
    "section": "Example of BPE tokenization for unknown words",
    "text": "Example of BPE tokenization for unknown words\n\n\n\nBPE tokenization for unknown words\n\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntext = (\n    \"Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces\"\n    \"of someunknownPlace.\"\n)\nprint(text)\nintegers = tokenizer.encode(text, allowed_special={\"&lt;|endoftext|&gt;\"})\nprint(integers)\nstrings = tokenizer.decode(integers)\nprint(strings)\n\n\ntokenizer.decode(tokenizer.encode(\"Akwirw ier\"))"
  },
  {
    "objectID": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html",
    "href": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html",
    "title": "The Curious World of MCP Servers",
    "section": "",
    "text": "Using AI assistants for software engineering has practically become non-optional for tech workers. At NVIDIA, I routinely use AI assistants for software engineering - to the point where I have been warned about my excessive token usage. More specifically, I use Cursor in agent mode in combination with Claude 4.5 Opus on MAX mode (which according to benchmarks has the best performance for software engineering and coding tasks). But recently I unlocked another level of performance and productivity by enabling a number of MCP servers in Cursor. I’ve been somewhat skeptical of the power of MCP servers and have put off trying them out. But momentum for internal MCP servers at NVIDIA has been building for a while now, so I gave them a try.\nI started with a handful of servers (to focus on the most prominent use-cases and not overwhelm Cursor/Opus with too many tools):\n\n[Directory] A server for querying the internal employee directory\n[Slack] A server for sending and receiving messages from Slack (Slack is a messaging platform used at NVIDIA but also more widely used in the industry)\n[JIRA] A server for querying the internal JIRA instance, creating new issues, and updating issue statuses (JIRA is a project management tool used at NVIDIA)\n[Gerrit] A server for querying the internal Gerrit instance (Gerrit is used as a code review system in our project)\n[NVIDIA MCP Suite] A collection of servers for interacting with the internal knowledge bases (Google Drive, Confluence, Sharepoint, etc.). Out of these, the Google Drive server has been the most useful so far for my daily work.\n\nIn the rest of this post I’ll use Cursor / Sonnet / Opus / LLM interchangeably to refer to the same thing - an agentic AI assistant orchestrated by Cursor that uses a large language model (LLM) with access to MCP servers."
  },
  {
    "objectID": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#bug-triage-feature-development",
    "href": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#bug-triage-feature-development",
    "title": "The Curious World of MCP Servers",
    "section": "🐛 Bug Triage & Feature Development",
    "text": "🐛 Bug Triage & Feature Development\n\nAutomating On-Call User Requests to Production Code\nOftentimes, on-call user requests (be it bug reports or feature requests) are fairly contained in scope and can be implemented by an AI assistant mostly autonomously. The main body of work is triaging the request, understanding the scope and impact, and creating the required documentation (e.g. in the form of a JIRA ticket) and artifacts for the request (code, tests, Git commit message that also links to the JIRA ticket) and then executing the request.\nWhat enabled me to automate that entire workflow was a combination of Slack and JIRA MCP servers and the following prompt:\nTriage the issue reported in Slack thread &lt;slack-thread-id&gt; and propose \na solution to the issue. Before working on the fix, create a JIRA ticket \nand a step-by-step implementation plan. Only then start implementing the \nsolution and unit test the implementation. Commit the changes to the \nrepository and create a Git commit including a message and link to the \nnewly created JIRA ticket. Follow the instructions in jira.md to create\nnew JIRA tickets.\n\nThe new feature should be added to code_file_x while new tests should be\nadded to test_file_y.\nNote that the more context you can provide to the LLM, the more likely it is to generate a useful solution (for example in the prompt above, I provided the code file and test file to be modified). But additional context can be as little as a starting point (i.e. a seed code file or directory) for the LLM to explore the codebase from.\nI also want to point out the reference of a jira.md file which is a Cursor rules file that contains the instructions for creating JIRA tickets that comply with our project-specific, internal requirements.\n\n\n\nThe end-to-end workflow for automating on-call user requests to production code\n\n\nThe workflow:\nThe end-to-end workflow below includes all steps from identifying a problem reported by a user in a Slack thread to implementing, testing, and deploying a solution with full project management integration.\n\nProblem Discovery: Ask Cursor to analyze the Slack thread, and identify the problem.\nCode Analysis: Ask Cursor to analyze the relevant code file to understand the context (seed file plus adjacent files) and identify the problem / required feature.\nSolution Design: Ask Cursor to propose a solution design and break it down into a series of tasks to be executed.\nImplementation: Ask Cursor to implement the solution in the codebase and check off all tasks from the solution design. Using the newly implemented unit tests, Cursor can verify that the implementation is correct. Using the checklist, Cursor can verify that the design has been followed.\nGit Commit & Jira Integration: Use the Jira MCP server to automatically create a ticket documenting the change, then commit the code with a reference to the ticket.\n\nKey insights:\n\nThis workflow works well for small, contained feature requests or bug fixes (which on-call requests often are). This is most likely not the best approach for large scale feature development or refactoring - the LLM may not have the context to understand the overall architecture and impact of the changes.\nThe more context you can provide to the LLM, the more likely it is to generate a useful solution (pointing the LLM to a good starting point / seed file in the codebase is often sufficient).\n\n\n\nAutomated Gerrit Code Review\nThis use-case shows how MCP integration with a code review system (Gerrit) can provide comprehensive, structured code reviews with security and quality analysis. The MCP Gerrit integration approach is specifically useful for reviewing changes that are already on Gerrit (not in your local workspace) and analyzing CLs from other team members.\n\nNote: Cursor recently added a built-in code review agent (accessible via Command Palette → “Start Agent Review”) that can analyze local git diffs and uncommitted changes. The MCP Gerrit integration approach documented here is specifically useful for:\n\nReviewing changes that are already on Gerrit (not in your local workspace)\nAnalyzing CLs from other team members\nFetching existing review comments and context\nSystematic review of multi-file changes with structured output\n\nBoth approaches are complementary: use the built-in agent for pre-commit reviews, and MCP Gerrit integration for reviewing others’ CLs or post-commit analysis.\n\nChallenges:\n\nManual code review requires opening Gerrit UI, reading diffs, understanding context\nEasy to miss security issues (e.g. hardcoded secrets)\nTime-consuming to analyze large CLs with multiple files\nNeed to provide actionable, structured feedback\n\n\n\n\nThe end-to-end workflow for automated Gerrit code review\n\n\nThe workflow:\n\nData Gathering: Given a code review URL, the MCP Gerrit integration fetches the file list, existing review comments, and full diffs for all modified files.\nSecurity Analysis: The AI scans for security issues like hardcoded secrets.\nCode Quality Analysis: The review identifies patterns like reinventing existing utilities, error-prone code construction, and test organization issues. It also provides specific recommendations with code examples.\nPositive Feedback: The review acknowledges good practices - comprehensive test coverage, well-structured code, thorough documentation, and proper integration patterns.\nStructured Output: The final review follows a consistent format: Critical issues → Quality issues → Positive aspects → Suggestions → Checklist for the author.\n\nKey insights:\n\nUnderstanding why code exists is crucial. What appears as a security mistake may be a deliberate temporary workaround discussed with infrastructure teams. The AI helps surface issues, but human judgment is still needed for prioritization and context.\nContext from existing comments valuable: If somebody has already commented on the code, it is valuable to include that context in the review. A human reviewer could leave high-level comments that provide crucial context while the LLM can do a more detailed analysis of the code.\nThis workflow may need multiple passes for very large CLs (&gt;100 files) given the context window limitations of the LLM.\nThe LLM has limited understanding of business logic without broader context (i.e. without access to the broader system architecture, product requirements, etc.).\nReviewing code is a task that requires a lot more context than writing code. Previous design decisions, code architecture, context from conversations with other engineers, product managers, etc. are all crucial for a comprehensive review. Such comprehensive context is non-trivial to provide to the LLM, which is why code reviews are still mostly done manually. Just in recent months, I’ve seen a number of tools that aim to automate code reviews more (e.g. via OpenAI’s Codex or Cursor’s built in code review capabilities).\n\nFuture Enhancements:\nA number of improvements are possible to make this workflow more robust and effective - most of them related to context engineering, i.e. how to provide the LLM with enough context to perform the review well.\n\nIntegrate with static analysis tools (linters, security scanners)\nBuild knowledge base of common patterns/anti-patterns\nAdd automatic severity classification\nLink to coding standards documentation\nGenerate metrics (review time, issues found, etc.)\nCross-reference with Slack/Jira to understand context behind decisions (e.g., why temporary workarounds exist) - ideally this would be documented by engineers via in-line comments/links in the code that can be dereferenced via Slack or Jira MCP servers. That way, the LLM can get the context it needs to understand the code and the decisions made by the engineers.\nSurface related team discussions when security patterns are detected"
  },
  {
    "objectID": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#project-management",
    "href": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#project-management",
    "title": "The Curious World of MCP Servers",
    "section": "📋 Project Management",
    "text": "📋 Project Management\n\nAutomated Jira Ticket Creation\nThis use-case establishes an automated Jira ticket creation workflow via MCP with proper attribution and best practices. For this workflow, I am using the open-source Atlassian MCP server which provides an MCP server for Atlassian products such as Jira and Confluence.\nNote that this workflow requires write permissions to the Jira instance - use this workflow with caution to avoid creating tickets in the wrong project, with the wrong assignee, or creating large numbers of tickets and spamming the Jira instance.\n\n\n\nThe end-to-end workflow for automated Jira ticket creation\n\n\nBest practices established:\n\nAlways add “Ticket created by AI assistant” to descriptions for traceability - we want these auto-generated tickets to be clearly identifiable as such.\nKeep initial descriptions brief; add details via comments after creation\nUse a two-step process: create ticket with minimal info, then add detailed comments\nTest with a dummy ticket first to verify write access\n\nThis two-step ticket creation process (ticket first, then details in the comment) was my first stab at automating this (and done to avoid timeouts with the JIRA MCP server). However, from what I have seen, it is more common to add all details to the ticket in a single description (which in my opinion provides a better one-stop-shop overview for what the ticket is about).\nKey learnings:\n\nBrief description + detailed comment is more reliable than a single long description (a single long description has led to API timeouts in the past)\nClearly identifying auto-generated tickets as such is crucial for traceability and accountability.\n\n\n\nSlack Mention Summarization\nThis use-case automates searching and prioritizing Slack messages in which you’ve been tagged, categorizing them by urgency and action required.\n\n\n\nThe end-to-end workflow for Slack mention summarization\n\n\nThe workflow:\n\nSearch for Mentions: Use MCP Slack integration to search for messages mentioning your user ID across all accessible channels.\nCategorization: Messages are automatically categorized into three priority levels:\n\n🔴 Action Required: Direct questions, review requests, blockers, handoff requests\n🟡 Review/FYI: Deployments to review, meeting prep, awareness items\n🟢 Informational: Cross-posts, announcements, confirmations\n\nTime Filtering: Filter by recency—last 2 days for daily review, last week for weekly catchup, or custom date ranges.\nStructured Output: Generate a summary with action items at the top, tables for lower-priority items, and a final prioritized checklist.\n\nCategorization rules:\n\nMR/CL review requests → Action Required\nDirect questions to you → Action Required\nHandoff/takeover requests → Action Required\nStaging deployments for review → Review\nCross-posts of announcements → Informational\nAcknowledgments/thanks → Informational\n\nThe set of rules is work-in-progress and being updated as I (or Cursor) discover new patterns and categories of messages.\n\nVariations of This Workflow\nThere is a number of variations and extensions of this basic Slack workflow I use. Especially when I am juggling a large number of concurrent threads and conversations or when I have been out of office for a few days, being able to answer the following questions programmatically has been a huge time saver:\n\nWhich conversations was I part of today? What’s the summary of my Slack activity today?\nWhat do I need to follow up on in Slack tomorrow?\nWhat did I miss on Slack when I was out yesterday?\nWhich channels, threads was I active in yesterday?\nWhich projects did I push yesterday?\nWhich open questions did I follow up on yesterday?\nWhat has person X been working on today / yesterday / over the last (N) days / weeks?\n\nGiven the public nature of Slack, it is easy to imagine that this workflow could be used to track the activity of other engineers in the organization (which is what the last question above demonstrates). This is perhaps the most questionable use-case of this workflow. Not everybody is comfortable with the idea of being tracked like this - especially since Slack provides just a partial picture of a person’s daily work and is bound to skew the picture towards more publicly visible activities.\nFuture Enhancements:\n\nCombining Slack with other MCP servers like JIRA could enable powerful bots that, for example, send a daily digest of tasks I should complete or identify open questions I should follow up on. This could be a powerful tool for a manager to delegate tasks (assuming engineers and PMs are diligent in filing JIRA tickets - which is far from guaranteed).\nSimilarly, based on Slack activity alone, this workflow could be used to send yourself a daily digest of tasks to complete (by also providing write permissions to the Slack MCP server).\nIdeally, this workflow could be extended to other communication channels like email and most notably meetings (Teams, Zoom, Webex, etc.). Given the availability of AI-powered transcription tools, it should be possible to transcribe meetings and then use the same workflow to summarize the meeting notes (and pick out any mentions or questions that need to be followed up on).\n\n\n\n\nCreating a New JIRA Issue"
  },
  {
    "objectID": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#people-performance-management",
    "href": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#people-performance-management",
    "title": "The Curious World of MCP Servers",
    "section": "👥 People & Performance Management",
    "text": "👥 People & Performance Management\n\nPerformance Review Data Gathering\nThis use-case demonstrates automated collection and analysis of employee contributions from Slack to support performance reviews and promotion decisions. I have used this to gather material for my own self-evaluation for the annual performance review (in addition to scouring my daily work logs, Gerrit code contributions, etc.).\nThe challenge:\n\nManual review of months of Slack activity across dozens of channels is extremely time-intensive\nNeed to identify specific examples demonstrating key competencies\nMust synthesize scattered information into a coherent narrative\n\n\n\n\nThe end-to-end workflow for performance review data gathering\n\n\nThe workflow:\n\nData Collection: Use MCP Slack integration to search for an engineer’s messages over a review period (e.g., 2 months), filtering by user ID and date range.\nEvidence Gathering: The AI extracts threads showing collaboration, technical discussions, incident response ownership, and cross-team coordination.\nCompetency Mapping: Contributions are grouped and mapped to competency areas like technical depth, ownership, collaboration, and communication.\nReport Generation: Generate a structured report with executive summary, major achievements, competency assessment, areas for improvement, and supporting links (documents, Slack threads, tickets).\n\nQuality improvements:\n\nComprehensive coverage (hundreds of messages analyzed)\nNo recency bias (equal weight to entire review period)\nSpecific evidence cited with quotes, links, and dates\nConsistent competency framework application\n\nThe same considerations discussed in the Slack mention summarization workflow apply here — specifically, that not everybody is comfortable with the idea of being tracked / evaluated like this. When in doubt, ask for explicit consent from the affected person before using this workflow. For example, I only use this workflow to analyze my own accomplishments or others with explicit consent, focusing solely on work-relevant channels and professional contributions.\n\n\nInterview Question Generation\nThis use-case automates the creation of tailored interview questions from candidate resume data, generating structured multi-part questions with natural bridges to your team’s work.\n\n\n\nThe end-to-end workflow for automated interview question generation\n\n\nThe workflow:\n\nResume Analysis: Provide a candidate’s relevant resume section(s). The AI identifies key accomplishments, especially quantified ones (percentages, scale numbers, cost savings, cross-team impact).\nQuestion Generation: For each major accomplishment, generate a multi-part question with:\n\nContext acknowledgment (show you read their resume)\nTechnical decision probing (“How did you design…”)\nProcess/collaboration questions (“What was your process for…”)\nScale/impact questions (“What were the key challenges in scaling…”)\n\nBridge Section: Each question includes a “bridge” that naturally transitions to your team’s context, helping assess adaptability and fit.\n\nBelow is a full example I generated based on my own resume, in particular my work at NVIDIA. This example assumes that I am applying for the “Senior Software Engineer, Metrics and Evaluation - Autonomous Vehicles” position (unfortunately the detailed role description is no longer available online). Below is a snippet from my resume that we’ll base the questions on.\n\nNVIDIA — Staff Software Engineer / Tech Lead\n📍 Santa Clara, California, USA | 📅 April 2024 – Present\n\n\n\n\n\n\n\nArea\nAccomplishments\n\n\n\n\nEvaluation Systems\nLead the design and implementation of evaluation system for NVIDIA’s end-to-end planning stack for autonomous vehicles\n\n\nData Products\nBuild compelling data-driven evaluation products by combining on-road driving analysis, large scale simulation, models, metrics, and dashboards (used by 250+ daily active users)\n\n\nML/AI Models\nContribute to development of novel VLA models for autonomous driving (based on NVIDIA’s Alpamayo-R1)\n\n\nFoundation Models\nIncorporate latest foundation models (VLMs, LLMs) into evaluation workflows (e.g. LLM-as-judge)\n\n\nInfrastructure\nDesign and implement large-scale distributed data and evaluation pipelines, data formats, and data storage backends\n\n\n\nThe questions below have been generated by Cursor without any further manual input or context engineering.\n\n\nQuestion A: Evaluation System Architecture & Scale\n\n“In your current role at NVIDIA, you’re leading the design and implementation of an evaluation system for the end-to-end planning stack, with data-driven products used by 250+ daily active users. Can you walk us through:\n\nSystem architecture decisions - What were the key architectural choices you made when designing the evaluation system? How did you structure it to serve both real-time on-road driving analysis and large-scale simulation results? What tradeoffs did you navigate between flexibility and standardization?\nUser adoption strategy - With 250+ daily active users relying on your dashboards and products, how did you approach the product design to ensure adoption? What feedback mechanisms did you implement, and how did you balance the needs of different user personas (engineers, leadership, domain experts)?\nData pipeline scalability - You mention large-scale distributed data pipelines and storage backends. What were the most significant scaling challenges you faced, and how did you design the data formats and storage architecture to handle the volume and variety of evaluation data?”\n\n\n\n\nBridge to Team Context\n“We’re building evaluation infrastructure that needs to serve multiple teams with different analytical needs while maintaining consistency in how we measure autonomous vehicle performance. How do you think about creating evaluation primitives that are both reusable and expressive enough to capture domain-specific nuances?”\nWhy this bridge matters:\n\nExplores their ability to balance standardization with flexibility\nReveals their philosophy on evaluation system design\nOpens discussion on our specific evaluation challenges\n\n\n\nQuestion B: Foundation Models in Evaluation (LLM-as-Judge)\n\n“You’re incorporating foundation models—VLMs and LLMs—into evaluation workflows, including ‘LLM-as-judge’ approaches. This is a rapidly evolving area. Can you tell us about:\n\nUse case selection - What specific evaluation problems are you solving with LLM-as-judge that traditional metrics couldn’t address? How did you identify where foundation models add value versus where deterministic metrics are more appropriate?\nValidation and calibration - How do you validate that your LLM-based evaluations are reliable and consistent? What’s your approach to handling hallucinations, prompt sensitivity, and ensuring reproducibility across model versions?\nIntegration architecture - How did you design the integration of these foundation models into your existing evaluation pipelines? What were the key considerations around latency, cost, and failure modes when running LLM evaluations at scale?”\n\n\n\n\nBridge to Team Context\n“We’re exploring similar applications of foundation models for evaluation—particularly for assessing planning quality in complex scenarios that are hard to capture with traditional metrics. What principles would you apply when deciding whether a given evaluation task is a good fit for LLM-based assessment versus traditional approaches?”\nWhy this bridge matters:\n\nAssesses their judgment on when to apply emerging techniques\nReveals depth of hands-on experience with LLM evaluation pitfalls\nConnects to our active exploration in this space\n\n\n\nKey Areas to Probe Based on Resume\n\n7+ years of AV experience across two major programs (Apple, NVIDIA)\nProgression from IC to Tech Lead role\nUnique combination of ML engineering + evaluation expertise\nExperience with both traditional and foundation-model-based evaluation\n\n\n\nPotential Red Flags to Watch For\n\n\n\n\n\n\n\nSignal\nWhy It Matters\n\n\n\n\nVague answers about “novel” or “leading” claims\nProbe for specifics to verify depth\n\n\nUnable to articulate sim-to-real validation approaches\nCore competency for evaluation role\n\n\nLack of depth on LLM-as-judge failure modes\nCurrent focus area requires hands-on experience\n\n\nDifficulty explaining technical concepts to non-technical stakeholders\nEssential for senior role\n\n\n\n\n\nStrong Signals to Look For\n\n\n\n\n\n\n\nSignal\nWhy It Matters\n\n\n\n\nConcrete metrics on evaluation system usage and impact\nEvidence of real-world influence\n\n\nNuanced understanding of when ML evaluation is appropriate vs. overkill\nShows judgment and maturity\n\n\nClear mental models for scenario coverage and validation\nFundamental evaluation competency\n\n\nEvidence of influencing team/org direction through evaluation insights\nLeadership indicator\n\n\n\nKey insights:\n\nThe generated questions are fairly comprehensive and cover a lot of ground. These questions are meant to be introductory and not occupy the majority of the interview time. Instead of going through them in detail, use them more as a buffet of topics to choose from.\nThe resume analysis step can focus on different aspects that are important to a given role (e.g. extract quantifiable accomplishments - percentages, scale numbers, cost savings, cross-team impact, etc.). Modify the prompt accordingly.\nThe question design starts with context to set up the question and uses a 3-part structure to enable a natural progression from architecture -&gt; process -&gt; scale.\nThe questions focus on the “how” and “why” not just the “what” to probe for the candidates understanding of how their work fits into the broader context of the team and the company. This is becoming increasingly important for senior roles.\nI’ve found a few effective patterns in my questions that elicit specific narratives from the candidate:\n\n“Can you walk us through…” (invites narrative)\n“How did you define and measure…” (probes rigor)\n“What was your process for balancing…” (reveals tradeoff thinking)\n“What were the key challenges in…” (surfaces problem-solving)\n\n\n\n\n\nInterview Notes Summarization\nI have been interviewing an increasing number of candidates recently (up to 3 a week) and while the preparation time for each interview is sub-linear (given that I can reuse a question I am well-calibrated on), the interview post-processing time scales linearly with the number of candidates. As such, I have been experimenting with structuring my interview notes in a way that makes them more easily digestible by an LLM. This use-case automates the analysis of interview notes to generate structured candidate evaluations with rubric-based scoring.\nI do want to point out that I view the LLM-generated evaluations as a starting point for my own evaluation. While I am more willing to trust code generated by LLMs (because it can be easily verified via tests), automated evaluations of candidates are not as trustworthy (for a number of reasons, e.g. because written notes miss non-verbal cues and context about a candidate’s performance) and need to be verified and put into context by the interviewer. In that sense, I use an LLM-assisted summarization approach rather than a fully hands-off one.\nTo simplify the process of summarizing interview notes, I recently started using tags in my notes to provide additional non-textual context to the LLM. I found that oftentimes, the LLM would give the candidate a glowing review based on my notes because I did not capture enough context about the conversation.\nChallenges:\n\nRaw interview notes are often unstructured and chronological rather than thematic\nExtracting signal from interviewer tags ([CQ], [CS], [Q]) requires careful interpretation (e.g. [CQ] is not always a negative signal, more on the exact meaning of these tags in the next section)\nSynthesizing technical depth, communication quality, and domain knowledge into actionable verdicts is time-consuming\nNeed consistent scoring framework across candidates and interviewers\n\nIn particular, I introduced the following tags into my notes:\n\n\n\n\n\n\n\n\nTag\nMeaning\nImpact on Assessment\n\n\n\n\n[INTERVIEWER]\nPrompt/question\nNeutral - sets context\n\n\n[Q]\nFollow-up question from the candidate\nNeutral - natural flow\n\n\n[CQ]\nClarifying question asked by interviewer\nSlightly negative if excessive - candidate should have been clearer\n\n\n[CS]\nClarifying statement explaining concept by the interviewer\nNegative - gap in expected knowledge\n\n\n\nI expect the set of tags to evolve over time as I gain more experience with this particular process and as I find new ways to structure my notes.\n\n\n\nThe end-to-end workflow for automated interview notes summarization\n\n\nThe workflow:\n\nInput: Interview notes with tags marking interviewer prompts, candidate responses, and areas requiring clarification.\nAnalysis: The AI extracts positive signals (structured thinking, proactive clarification, domain expertise) and negative signals (concepts requiring explanation, confusion on fundamentals).\nScoring: Generate scores across multiple axes (Problem Solving, Domain Knowledge, Communication, Fit) with justification for each score.\nOutput: Structured summary with verdict, rubric scores, strengths, concerns, and clear hire/no-hire recommendation.\n\nScoring guidelines I use:\n\n5/5: Exceptional, beyond expectations, strong hire signal\n4/5: Strong performance, meets senior bar\n3/5: Adequate, meets expectations\n2/5: Below expectations, concerns identified\n1/5: Significant gaps, strong no-hire signal\n\nKey insights:\n\nAn LLM-assisted summarization approach offers a number of advantages over a purely manual approach:\n\nConsistency: Same rubric applied systematically across all notes (and candidates)\nCompleteness: No signal lost from lengthy notes\nObjectivity: Tag-based analysis reduces interpretation bias\nActionable: Clear verdict with supporting evidence\nStructured: Easy to compare across candidates\n\nThis tag-based approach also forces some structure onto me as the interviewer to ensure that I capture all the relevant context about the conversation.\nTags provide a clearer way of identifying where the candidate struggled or excelled and enable effective analysis patterns:\n\nTag frequency analysis reveals candidate clarity ([CQ] count)\n[CS] tags indicate knowledge gaps to probe in follow-ups\nCandidate questions to interviewer show curiosity and engagement\n\nSummaries are generated with a few guidelines in mind:\n\nStart with verdict (busy readers need bottom line first)\nScore all applicable rubric axes with justification\nCite specific examples from notes for each assessment\nBalance strengths and concerns\nNote any axes not tested in this interview format\n\nThe hire/no-hire (or rather proceed/don’t proceed) recommendation explicitly has to come from the interviewer and not the LLM.\n\nFuture Enhancements:\n\nCombine with Interview Question Generation for full interview loop\nTrack rubric scores over time for interview calibration\nProvide the LLM with the job description for additional context."
  },
  {
    "objectID": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#documentation",
    "href": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#documentation",
    "title": "The Curious World of MCP Servers",
    "section": "📚 Documentation",
    "text": "📚 Documentation\n\nCreating Reference Documentation\nThis use-case demonstrates creating comprehensive, reusable reference documentation for complex workflows. What we aim for here is to build a living knowledge base - the very documentation that enabled many of the other use-cases in this post.\nThe problem:\nOftentimes, you figure out a clever workflow, use it successfully once, and then six months later you’re staring at a blank screen trying to remember how you did it. The solution is obviously documentation, but documentation takes time — time that you don’t always have in the thick of deliverables and deadlines. Documentation, however, becomes increasingly important not just as a record for yourself but as a reference for others to learn from. This reuse and sharing of knowledge builds an institutional knowledge base that is invaluable for the organization.\nThe barrier to creating such documentation can be the lack of standardized approaches and platforms. Some documentation lives in Confluence, some in Google Docs, some in JIRA, some in Slack, etc. Besides just generating documentation, this workflow also aims at standardizing the generated documentation as Markdown or HTML files and storing them in plain Github repos. Documents stored in a Git repo are clearly not as interactively modifiable as Confluence or Google Docs, but they can be manipulated much more easily via LLMs (which makes them “almost” as modifiable as Google Docs).\n\n\n\nThe end-to-end workflow for automated reference documentation creation\n\n\nThe virtuous cycle:\nAfter completing a new experiment or workflow with MCP servers, I ask the AI assistant to update my internal reference document. The key insight is that the AI assistant that just helped you complete a task has all the context needed to document it:\n\nCapture immediately: Document right after completion while the conversation history contains everything\nStructure consistently: Problem statement, solution approach, example prompts, results, and lessons learned\nGeneralize: Strip out commit hashes, specific ticket numbers, and names — make it a reusable template\nOrganize for discovery: Group by theme, add a table of contents, and cross-link related use-cases\n\nExample prompt:\nAdd this workflow to my MCP use-cases document. Include the problem \nwe solved, the approach we took, a generalized version of the prompt \nI can reuse, and any lessons learned. Follow the existing document \nstructure and formatting.\nExample artifacts:\n\nA Jira workflow reference with MCP commands, common JQL patterns, and troubleshooting tips\nThis use-cases document itself (the internal version that this blog post is based on)\nReusable templates for ticket creation, code review workflows, and status report generation\n\nKey insights:\n\nMake templates generic: Replace specific values with placeholders like &lt;ticket-id&gt; or &lt;project-name&gt;\nInclude the prompts: The exact wording that worked is often more valuable than the explanation\nDocument failure modes: What didn’t work is as useful as what did\nAttribute AI involvement: Keep notes like “Ticket created by AI assistant” visible for transparency\n\n\n\nCreate Onboarding Guides\nThis use-case shows how to use Google Drive and Slack MCP integration to fetch existing documentation, messages, and links from scattered sources and synthesize comprehensive onboarding guides.\nChallenges:\n\nOnboarding information is typically scattered across Google Docs, Confluence, wikis, and tribal knowledge\nManual consolidation is time-consuming and error-prone\nDocuments frequently become outdated without anyone noticing\n\n\n\n\nThe end-to-end workflow for automated onboarding guide creation\n\n\nThe workflow:\n\nFetch Source Documents: Use MCP Google Drive integration to retrieve full document content from multiple sources. The integration preserves headings, lists, and code blocks. Optionally augment this with Slack messages from the relevant channels (and the links contained therein)\nIdentify Gaps: Cross-reference multiple documents to find inconsistencies or missing information.\nSynthesize: Combine information from multiple sources into a single, well-structured guide with consistent formatting.\nAdd Troubleshooting: Aggregate troubleshooting tips from scattered sources (such as Slack threads that discuss common issues) into a dedicated section.\n\nQuality improvements:\n\nConsistent structure and formatting across all content\nCross-referenced information validated for accuracy\nComprehensive troubleshooting compiled from multiple sources\nSingle source of truth that’s easier to maintain\n\nKey insights\n\nAlways verify that document links exist and are accessible before including them—some internal pages may be restricted or have been removed.\nVerify all information synthesized by the LLM to avoid hallucinations (this includes links added to the document, Slack handles, any reference, etc.)"
  },
  {
    "objectID": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#the-elusive-status-update",
    "href": "posts/2025_12_10_the_curious_world_of_mcp_servers/index.html#the-elusive-status-update",
    "title": "The Curious World of MCP Servers",
    "section": "The Elusive Status Update",
    "text": "The Elusive Status Update\nOne use-case I haven’t managed to automate with Cursor and MCP servers is the surprisingly involved use-case of providing status updates to various stakeholders. Collecting all the required context from a variety of different sources is as much work as the actual writing of the status update itself. This, however, is more a limitation of my note-taking than a limitation of Cursor and MCP servers. Currently, I mostly use the MacOS / iOS Notes app for this purpose, which does not integrate well with Cursor. To facilitate the integration with Cursor, I’ll try out the following:\n\n[Markdown] Move to a Markdown-based note-taking app like Obsidian or Notion\n[Structured notes] Currently, I create a new note every day. Instead, I will move to a more topic-based note-taking approach such that each project / line of work has its own note. That should enable Cursor to more easily extract the relevant context for a given topic and summarize it.\n[Tags] I already use tags quite heavily in my notes but mostly to simplify manual lookups. I expect tags to enable Cursor to more easily cross-reference notes.\n[Links] Context is often provided in various documents and Slack threads whose context is not directly imported into my notes. All these data sources should be easily available via MCP servers. One risk here is that too much context can quickly saturate the context window of the LLM (which for Opus is 200k tokens). So I’ll need to be careful with the amount of context I include in each note."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "Welcome to Layer by Layer, where I try to peel back the complexity of modern machine learning to reveal insights, patterns, and understanding - or mostly just share things in the ML space that I find interesting and also document my own learning journey, one layer at a time.\nI’m fascinated (though also intimidated and sometimes overwhelmed) by how rapidly the machine learning field is evolving - just reading AI newsletters is often tough to fit into a busy work schedule (let alone drinking from the firehose of all the papers that are coming out). My goal here is simple: to explore interesting ideas and ML fundamentals, break down complex concepts, and post by post build an ML engineering foundation that will hopefully make it easier to break into this exciting field. This blog is meant as an extension of my own learning journey.\nYou’ll find a mix of content here (or at least that is the aspiration):\n\nIn-depth tutorials that break down complex ML techniques into digestible steps - tutorials that I wish existed when I was learning certain techniques\nPractical guides for implementing state-of-the-art models and methodologies\nAccessible summaries of recent research papers that highlight key contributions but also notes I’ve found helpful or interesting\nThoughts on tools and frameworks I’ve tried, and the ecosystem powering modern ML\nDiscussions about trends and where ML might be heading\n\nI’m writing for fellow enthusiasts - people who are curious and excited about machine learning and want to understand it better, whether you’re just starting out or have been in the field for a while.\nThis isn’t about presenting myself as an expert but about becoming one - and sharing what I’m learning and thinking about along the way. I hope you’ll join me on this journey as we build knowledge together — layer by layer."
  },
  {
    "objectID": "disclaimer/index.html",
    "href": "disclaimer/index.html",
    "title": "Disclaimer",
    "section": "",
    "text": "Opinions expressed on this Site are the author’s own in his personal capacity. They do not reflect the views of the United States Government, NVIDIA Inc. or of any organisation, company or board he is associated with."
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html",
    "href": "posts/2025_05_05_welcome/index.html",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "Hello and welcome to Layer by Layer - and experiment in learning in public where I’ll be sharing my exploration of machine learning concepts, techniques, and research.\n\n\nThe machine learning landscape is evolving at a breathtaking pace. From transformers to diffusion models, from reinforcement learning to neural radiance fields - there’s an overwhelming amount of innovation happening. As someone navigating this field, I often find myself wanting to document my learning journey, break down complex ideas, and create resources I wish had existed when I was starting out.\nThis blog is my attempt to:\n\nDocument my learning process - Writing helps me solidify my understanding\nDevelop and share practical tutorials - With code examples that actually work. This is inspired by a few of the ML blogging greats: Sebastian Raschka’s blog, Andrej Karpathy’s blog, Lilian Weng’s blog, and many more.\nSummarize interesting papers - Distilling key ideas from research into accessible explanations (in the spirit of Two Minute Papers but in longer written form). This is inspired by Jay Alammar’s Illustrated Transformer his Illustrated Deep Seek R1 and overall his Illustrated* series now available at Substack.\nBuild in public - Showing both successes and the inevitable struggles, and also post project walkthroughs.\nLink to resources - Occasionally, I’ll be posting links to useful resources - though I don’t want to make this another newslettter or news aggregator.\n\n\n\n\nI’m Daniel Pickem, a researcher and engineer with a background in robotics and multi-agent systems. I completed my PhD at Georgia Tech where I developed the Robotarium, a remotely accessible swarm robotics testbed. My swarm robotics days are somewhat behind me - or maybe just on pause until swarm robotics has more practical applications. Until then, I am really excited and curious about machine learning - most of all foundation models of all modalities.\n\n\n\nRobotarium\n\n\n\n\n\nI hope you’ll find the content here useful, whether you’re just starting out in ML or are already well into your own journey. Feel free to reach out with questions, corrections, or suggestions for topics you’d like to see covered.\nLet’s explore the fascinating world of machine learning together, one layer at a time.\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html#why-i-started-this-blog",
    "href": "posts/2025_05_05_welcome/index.html#why-i-started-this-blog",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "The machine learning landscape is evolving at a breathtaking pace. From transformers to diffusion models, from reinforcement learning to neural radiance fields - there’s an overwhelming amount of innovation happening. As someone navigating this field, I often find myself wanting to document my learning journey, break down complex ideas, and create resources I wish had existed when I was starting out.\nThis blog is my attempt to:\n\nDocument my learning process - Writing helps me solidify my understanding\nDevelop and share practical tutorials - With code examples that actually work. This is inspired by a few of the ML blogging greats: Sebastian Raschka’s blog, Andrej Karpathy’s blog, Lilian Weng’s blog, and many more.\nSummarize interesting papers - Distilling key ideas from research into accessible explanations (in the spirit of Two Minute Papers but in longer written form). This is inspired by Jay Alammar’s Illustrated Transformer his Illustrated Deep Seek R1 and overall his Illustrated* series now available at Substack.\nBuild in public - Showing both successes and the inevitable struggles, and also post project walkthroughs.\nLink to resources - Occasionally, I’ll be posting links to useful resources - though I don’t want to make this another newslettter or news aggregator."
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html#about-me",
    "href": "posts/2025_05_05_welcome/index.html#about-me",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "I’m Daniel Pickem, a researcher and engineer with a background in robotics and multi-agent systems. I completed my PhD at Georgia Tech where I developed the Robotarium, a remotely accessible swarm robotics testbed. My swarm robotics days are somewhat behind me - or maybe just on pause until swarm robotics has more practical applications. Until then, I am really excited and curious about machine learning - most of all foundation models of all modalities.\n\n\n\nRobotarium"
  },
  {
    "objectID": "posts/2025_05_05_welcome/index.html#join-me-on-this-journey",
    "href": "posts/2025_05_05_welcome/index.html#join-me-on-this-journey",
    "title": "Welcome to Layer by Layer",
    "section": "",
    "text": "I hope you’ll find the content here useful, whether you’re just starting out in ML or are already well into your own journey. Feel free to reach out with questions, corrections, or suggestions for topics you’d like to see covered.\nLet’s explore the fascinating world of machine learning together, one layer at a time.\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/2025_05_13_llms_from_scratch/index.html",
    "href": "posts/2025_05_13_llms_from_scratch/index.html",
    "title": "Build a Large Language Model (From Scratch)",
    "section": "",
    "text": "I recently finished reading (or rather working through) my technical book of the month - Build a Large Language Model (From Scratch) by Sebastian Raschka and wanted to share my notes and codeing-along Jupyter notebooks here.\n\nMy take\nFirst, I want to emphasize what an awesome read this was. It presumes very little prior knowledge (other than being able to code in Python) and does a great job of building up the concepts, theory, and intuition for LLMs - all the way to training your own GPT-2 model and instruction fine-tuning it.\nMost of my previous technical books have been O’Reilly books, which also impressed me - especially Generative Deep Learning, 2nd Edition by David Foster. That book and most O’Reilly books I’ve read have provide a good balance of theory and practice / coding but Sebastian’s book does a better job of building from the ground up. In that sense, “Building a Large Language Model from Scratch” reminds me more of Andrej Karpathy’s Neural Networks: Zero to Hero, which I can also highly recommend. The latter goes even more into detail on the math and theory (including through derivations of backpropagation, which Sebastian skips in favor of focusing on the code).\nOverall, I can highly recommend Sebastian’s book to anyone who wants to understand LLMs and wants to follow a well-structured longform tutorial on building LLMs.\n\n\nBook summary (by Gemini)\nThe book “Build a Large Language Model (From Scratch)” by Sebastian Raschka guides readers through the process of creating, training, and fine-tuning Large Language Models (LLMs) from the ground up. This hands-on book aims to demystify LLMs by teaching readers how to build one step-by-step, without relying on existing LLM libraries. The core idea is that by building an LLM (comparable to GPT-2 in capabilities) yourself, you gain a deep understanding of its internal workings, limitations, and customization methods. The resulting LLM can be run on a standard laptop.\nKey learnings from the book include:\n\nPlanning and Coding: Learn to design and code all components of an LLM.\nDataset Preparation: Understand how to prepare datasets suitable for LLM training.\nTraining Pipeline: Construct a complete training pipeline.\nPretraining and Fine-tuning: Pretrain the model on a general corpus and then fine-tune it for specific tasks like text classification or with custom data.\nInstruction Following: Use human feedback to ensure the LLM adheres to instructions.\nLoading Pretrained Weights: Learn how to load pretrained weights into an LLM.\n\nBy working through the book, readers can expect to build a GPT-style LLM, evolve it into a text classifier, and ultimately create a chatbot that can follow conversational instructions.\nThe target audience for this book is individuals with intermediate Python skills and some existing knowledge of machine learning. While a GPU is recommended for faster training, it is optional.\nThe book is praised for its practical, code-driven approach that makes complex concepts accessible. You can find more details on the Manning Publications website: https://www.manning.com/books/build-a-large-language-model-from-scratch.\n\n\nJupyter notebooks\nOver the next few days, I’ll be adding Jupyter notebooks to this blog - one notebook for each chapter of the book. Besides functional code that mostly follows the book (but improves upon it in terms of type annotation, structure, and readability), these notebooks also contain additional material that expand upon certain concepts I wanted to explore more deeply than the book does.\nThe full set of notebooks is also available on my GitHub repo.\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/2025_10_06_open_ai_dev_day_recap/index.html",
    "href": "posts/2025_10_06_open_ai_dev_day_recap/index.html",
    "title": "OpenAI Dev Day 2025 Recap",
    "section": "",
    "text": "After a full day of talks, demos, and AMAs I just wanted to provide a little more color and detail about Open AI’s dev day 2025. I did not exactly know what to expect from this event but it felt like Open AI’s WWDC where a ton of new products were announced and demo’ed - glued together by talks and stage conversations from OpenAI leadership. I’ll just mention the ones that left the biggest impression of me:\n\nAgents SDK: To be quite frank here, my opinion of agents was that they are overrated - a solution looking for a problem. However, the new agents SDK and the accompanying web drag-and-drop UI together with powerful eval and deployment tooling is changing my mind a bit. This SDK will definitely make agents more accessible beyond the software engineering agent use-case. See the Agent Builder UI below: \nApps SDK: This SDK enables apps to be directly integrated into ChatGPT’s main web (or mobile) UI. Two aspects I want to highlight here are the direct integration of apps into the ChatGPT UI, which enables a seamless experience between chat/reasoning and app-specific interaction and customizable UI elements or widgets as they OpenAI called them (such as maps). The other aspect is the power of MCP extensions that Codex and chat can take advantage of to, for example, take control of physical devices like onstage lighting, cameras, and audio systems. The Codex SDK took this even further where the Codex agent was directly integrated into a mobile fitness tracker app such that the app could “self-evolve” and be changed on-the-fly (triggered by a user prompt to add more trend plots for example)\n\nCodex SDK\nApps SDK\nOfficlal example apps and full end-to-end example \n\nCodex everywhere - in CLI, IDE, and web. Not just for writing code but also for powerful code review capabilities. I’ve seen before how automated code review can introduce a lot of noise with overzealous commenting. Codex review, on the other hand, focuses on providing fewer but higher signal comments. These are supposedly based on Codex writing tests and scripts to “pen test” the PR under review in a sense. In a sense, Codex is writing the code while another Codex instance (with a separate context) is reviewing it. Definitely something I am excited to try out to address the code review bottleneck. Code review was shown via the Codex CLI (via ) but also the Github web UI (see the Code review setup guide). \nCodex with Figma MCP integration and also Chrome dev-tools MCP for performance optimization. I enjoyed this demo quite a bit too for the main reason that it almost entirely automated the web-app development - from designs in Figma, to MVP implementation, to performance tuning via the Chrome dev tools MCP server. What’s more, Codex can take screenshots via Playwright to verify that the implementation stays faithful to the original design. A highly practical demonstration of agentic workflows and the reasoning and tool calling power of gpt-5-codex. The demo app “Wanderlust” - an app that proposes travel itineraries in a sleek web UI - has been recreated in this (unofficial?) repo. See the photo below (borrowed from Simon Willison’s blog) \ngpt-oss: My main takeaway of this talk was the impressive possibilities of running powerful reasoning and agentic models directly on-device. Even the 120B version of the model can run on consumer hardware like a MacBook (via Ollama). This is not unique to OpenAI’s GPT gpt-oss but it was nonetheless impressive to see a demo using tool calling (including outsourcing web UI development to the more capable but cloud-based gpt-5). I’ll definitely be trying out gpt-oss for on device applications. The only thing I wish gpt-oss supported was multimodal (in particular image) inputs.\nLast but not least, the conversation between Sam and Jony Ive was refreshing to listen to. No big product reveals but they did hint at a “product family” and that they have 15-20 promising ideas. That leaves much to speculation as to what the supposed wearable is going to be.\n\nOpenAI covered a lot of ground during the dev day and any of the topics above warrants more in-depth experimentation and discussions - stay tuned for those in follow-up blog posts.\nStay curious!\nDaniel"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "",
    "text": "Overall, I am always looking for ways to automate the repetitive tasks and busywork of my day-to-day work. In this post, I describe my attempt at building a note-taking system that serves as a knowledge base for my work and simplifies providing status updates for the many standup meetings I (have to) attend.\nI’ll describe how I changed the way I capture my daily work logs - moving away from manually writing notes to capturing them in a structured way that is accessible to modern AI tools. Accessibility to automated LLM-processing is one of the key enabling features of my new note-taking system that unlocks new levels of productivity because it allows me to provide context and raw data rather than concern myself with the actual writing of notes from scratch.\nOne could argue that completely revamping / restructuring one’s note-taking approach is overkill for just providing status updates. However, I found it to have completely changed the way I track my daily work, how I access my work log archive, and how I extract value from these notes. Overall, this new system has provided a lot of value besides automating status updates in the process.\nTo provide some context about the origins of this project and this post:\nThe project description below is the (mostly) LLM-generated README for the note-taking system. In a sense, this summary is the “product” of this LLM-powered note-taking system (with some light touching up by me).\nOne section I want to highlight in this post is LLM-Powered Workflows, which details how integrating large language models transforms raw work inputs—like Slack threads or meeting transcripts—into structured, actionable notes with minimal manual effort. If you read only one section, that’s the one that can provide the most value in your daily work."
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#template-80-templatesdaily.md",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#template-80-templatesdaily.md",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Template (80-Templates/daily.md)",
    "text": "Template (80-Templates/daily.md)\n---\ndate: {{date}}\ntype: daily\narea: \ntags:\n  - daily\n  - {{date:YYYY}}/{{date:MM}}\nprojects_touched: []\n---\n\n# {{date:dddd, MMMM D, YYYY}}\n\n## 🎯 Focus Areas\n- \n\n## 📋 Tasks\n\n### High Priority [p:: 1]\n- [ ] Task [p:: 1]\n\n### Medium Priority [p:: 2]\n- [ ] Task [p:: 2]\n\n### Upcoming\n- [ ] Task\n\n---\n\n## 🔄 Work Log\n\n\n---\n\n## 🚧 Blockers\n- \n\n---\n\n## 💡 Notes & Ideas\n- \n\n---\n\n## 📎 References\n\n### Slack Threads\n- \n\n### Google Docs\n- \n\n### Code & PRs\n- \n\n### Related Notes\n- \n\n---\n\n## Related\n- Previous: [[{{date:YYYY-MM-DD|-1d}}]]"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#what-a-real-daily-note-looks-like",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#what-a-real-daily-note-looks-like",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "What a Real Daily Note Looks Like",
    "text": "What a Real Daily Note Looks Like\nHere’s an example of a typical work day:\n---\ndate: 2026-01-06\ntype: daily\narea: data-platform\ntags:\n  - daily\n  - 2026/01\nprojects_touched:\n  - \"[[ML Pipeline Refactor]]\"\n  - \"[[API Gateway Migration]]\"\n---\n\n# Monday, January 6, 2026\n\n## 🎯 Focus Areas\n\n**Summary**: Created comprehensive data transformation workflow documentation \nand automation script.\n\n### Open Follow-ups\n- [ ] Test the transformation script with production data sample [p:: 1]\n- [ ] Validate output format against downstream consumers [p:: 2]\n- [ ] Follow up on infra team's capacity planning meeting [p:: 1]\n\n## 📋 Tasks Completed\n- [x] Researched Spark → Flink migration patterns\n- [x] Documented the new streaming pipeline architecture\n- [x] Created Python automation script for schema validation\n\n## 🔄 Work Log\n\n### Data Pipeline Workflow Documentation\n\nCreated a comprehensive workflow document capturing the migration process:\n\n**Key Components:**\n1. **Input Format**: Parquet files from data lake\n2. **Transformation Layer**: Flink streaming jobs\n3. **Output Format**: Delta tables for ML training\n4. **Monitoring**: Prometheus metrics + Grafana dashboards\n\n### Artifacts Created\n\n| Artifact | Location | Description |\n|----------|----------|-------------|\n| Workflow Doc | `40-Resources/workflows/spark-to-flink-migration.md` | Process documentation |\n| Python Script | `40-Resources/scripts/schema_validator.py` | Automation script |\n\n## 📎 References\n\n### Slack Threads\n- [Pipeline Architecture Discussion](https://slack.com/...) — sync thread\n\n### Google Docs\n- [Migration Playbook](https://docs.google.com/...) — Original playbook\n- [Data Platform Strategy](https://docs.google.com/...) — Workflows\n\n### Related Notes\n- [[spark-to-flink-migration]] — Comprehensive workflow documentation\nThe daily note becomes a chronological log of everything I touched, with links to all relevant resources. While these daily note markdown files can be viewed directly in Cursor, I am also using Obsidian as my UI as it provides a few powerful features to help me manage my notes (such as the plugin system)."
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#template-80-templatesproject-overview.md",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#template-80-templatesproject-overview.md",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Template (80-Templates/project-overview.md)",
    "text": "Template (80-Templates/project-overview.md)\n---\ntype: project\nstatus: active  # active | on-hold | completed | cancelled\npriority: high  # high | medium | low\nowner: \nstakeholders: []\nstart_date: {{date}}\ntarget_date:\ntags:\n  - project\n  - status/active\njira_epic:\nconfluence_page:\n---\n\n# {{title}}\n\n## Summary\n&lt;!-- 2-3 sentence project summary for status reports --&gt;\n\n## Objectives\n1. \n\n## Current Status\n&lt;!-- Updated weekly - this section is scraped for status reports --&gt;\n**Last Updated:** {{date}}\n**Health:** 🟢 On Track | 🟡 At Risk | 🔴 Blocked\n\n### This Week\n- \n\n### Next Week\n- \n\n### Risks & Blockers\n- \n\n## Team\n- \n\n## Links\n- [[Projects Index|All Projects]]"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#real-example-ml-pipeline-refactor",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#real-example-ml-pipeline-refactor",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Real Example: ML Pipeline Refactor",
    "text": "Real Example: ML Pipeline Refactor\n# ML Pipeline Refactor\n\n## Summary\n\nModernizing the ML training pipeline from batch Spark jobs to real-time \nFlink streaming, reducing data freshness from 24 hours to under 5 minutes \nwhile improving cost efficiency.\n\n## Key Components\n- **Data Ingestion** — Kafka → Flink streaming\n- **Feature Store** — Real-time feature computation\n- **Model Training** — Incremental learning support\n- **Monitoring** — End-to-end latency tracking\n\n## Current Status\n**Health:** 🟡 At Risk\n\n### Risks & Blockers\n- Feature store migration blocked on schema freeze\n- Need capacity planning approval for new Flink cluster\n\n## Key Workstreams\n\n### Streaming Architecture\n- Replace batch Spark jobs with Flink stateful processing\n- Target: &lt;5 min data freshness (currently 24h)\n\n### Feature Store Migration\n- Move from offline feature store to hybrid online/offline\n- Support both batch training and real-time inference\n\n## Team\n- Alice Chen — Tech Lead\n- Bob Martinez — Data Engineer  \n- Carol Singh — ML Engineer\n\n## Links\n- [[Projects Index|All Projects]]\n- [[API Gateway Migration]] — Dependent project\n- [[Pipeline Architecture Docs]] — Technical reference\n- [[2025-12-11]] — Initial project setup\nThe project note serves as a living document — I update it weekly with current status, and it links to all related daily notes, docs, and resources."
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#priority-system",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#priority-system",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Priority System",
    "text": "Priority System\nTasks use inline fields for priority:\n\n\n\nPriority\nMeaning\nExample\n\n\n\n\n[p:: 1]\n🔴 Critical\n- [ ] Fix prod bug [p:: 1]\n\n\n[p:: 2]\n🟠 High\n- [ ] Review PR [p:: 2]\n\n\n[p:: 3]\n🟡 Medium\n- [ ] Update docs [p:: 3]\n\n\n[p:: 4]\n🟢 Low\n- [ ] Refactor code [p:: 4]\n\n\n\nThis means I can scatter tasks throughout my daily notes, and they all roll up into a single prioritized view."
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#opening-the-vault-in-cursor",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#opening-the-vault-in-cursor",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Opening the Vault in Cursor",
    "text": "Opening the Vault in Cursor\nThe entire vault is a Cursor workspace. This means Claude can read any note, search across files, and understand context from my work history. Coincidentally, this setup is very similar to what Anthropic made available today via Cowork.\ncursor ~/workspace/obsidian/work-notes\nThis simple setup unlocks powerful workflows where I rarely write notes from scratch - instead, I feed raw inputs to Claude and get structured, linked outputs."
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-1-slack-thread-daily-note",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-1-slack-thread-daily-note",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Workflow 1: Slack Thread → Daily Note",
    "text": "Workflow 1: Slack Thread → Daily Note\nInstead of manually summarizing Slack discussions, I copy the thread and let Claude do the work.\nInput: Raw Slack thread (copy-pasted or via MCP Slack integration)\nPrompt:\nHere's a Slack thread about the data pipeline migration. \nSummarize this into my daily note format:\n1. Extract the key decisions made\n2. List any action items with owners\n3. Note any blockers or risks mentioned\n4. Add relevant links\n\nFormat it for the Work Log section of today's daily note.\nOutput: Structured summary ready to paste:\n### Slack: Data Pipeline Migration Discussion\n\n**Thread:** [#data-platform](https://slack.com/...) — 2026-01-06\n\n**Key Decisions:**\n- Agreed to use Flink over Spark Streaming for stateful processing\n- Schema freeze pushed to Friday to accommodate late feedback\n- Will run both pipelines in parallel for 2 weeks before cutover\n\n**Action Items:**\n- [ ] @alice — Update capacity estimates by Wednesday [p:: 2]\n- [ ] @bob — Set up parallel pipeline environment [p:: 1]\n- [ ] @carol — Draft rollback plan for review [p:: 2]\n\n**Blockers:**\n- Waiting on infra team approval for new Kafka cluster\n\n**Links:**\n- [Original thread](https://slack.com/...)\n- [[ML Pipeline Refactor]] — Parent project\nTime saved: 15 minutes → 2 minutes"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-2-meeting-transcript-structured-notes",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-2-meeting-transcript-structured-notes",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Workflow 2: Meeting Transcript → Structured Notes",
    "text": "Workflow 2: Meeting Transcript → Structured Notes\nAfter a meeting, I often have a raw transcript (from Teams, Otter.ai, or manual notes). Claude transforms this into actionable documentation.\nInput: Raw meeting transcript (stored in 40-Resources/meeting-transcripts/)\nPrompt:\nHere's the transcript from today's architecture review meeting.\nCreate a meeting note following my template (80-Templates/meeting.md).\n\nInclude:\n1. Executive summary (2-3 sentences)\n2. Key decisions with rationale\n3. Action items with owners and due dates (as checkboxes)\n4. Open questions that need follow-up\n5. Links to any documents or resources mentioned\n\nAlso extract any tasks that should go in today's daily note.\nOutput: Two artifacts:\n1. Meeting Note (40-Resources/meetings/2026-01-06-architecture-review.md):\n---\ntype: meeting\ndate: 2026-01-06\nproject: \"[[ML Pipeline Refactor]]\"\nattendees: [Alice Chen, Bob Martinez, Carol Singh, David Kim]\ntags:\n  - meeting\n  - architecture\n---\n\n# Architecture Review: Streaming Pipeline\n\n## Executive Summary\n\nReviewed the proposed Flink architecture for the ML pipeline migration. \nApproved the stateful processing approach with a 2-week parallel run period. \nIdentified capacity planning as the main risk to timeline.\n\n## Attendees\n- Alice Chen (Tech Lead)\n- Bob Martinez (Data Engineer)\n- Carol Singh (ML Engineer)\n- David Kim (Infra)\n\n## Key Decisions\n\n| Decision | Rationale | Owner |\n|----------|-----------|-------|\n| Use Flink over Spark Streaming | Better exactly-once semantics, native Kafka integration | Alice |\n| 2-week parallel run | Validate data quality before cutover | Bob |\n| Schema freeze by Friday | Accommodate late feedback from ML team | Carol |\n\n## Action Items\n- [ ] @alice — Finalize Flink job configurations — Due: 2026-01-08\n- [ ] @bob — Set up parallel pipeline environment — Due: 2026-01-10\n- [ ] @david — Submit capacity request for 3 additional nodes — Due: 2026-01-07\n- [ ] @carol — Update downstream consumers about schema changes — Due: 2026-01-09\n\n## Open Questions\n1. What's the fallback if capacity request is denied?\n2. How do we handle late-arriving data during parallel run?\n3. Should we notify customers about potential latency changes?\n\n## Follow-ups\n- [[2026-01-08]] — Capacity request decision expected\n- [[ML Pipeline Refactor]] — Update project status\n\n## References\n- [Architecture Diagram](https://docs.google.com/...)\n- [Capacity Planning Sheet](https://docs.google.com/...)\n2. Tasks for Daily Note:\n## 📋 Tasks from Architecture Review\n- [ ] Review Flink job configurations before Friday [p:: 2]\n- [ ] Follow up on capacity request with David [p:: 1]\nTime saved: 30 minutes → 5 minutes"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-3-weekly-or-daily-status-generation",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-3-weekly-or-daily-status-generation",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Workflow 3: Weekly (or Daily) Status Generation",
    "text": "Workflow 3: Weekly (or Daily) Status Generation\nAt the end of each week (or daily), Claude reads all my daily notes and generates a status report. The prompt below is usually just a starting point that I modify based on the expected format for a given standup or status meeting. For my daily standups, for example, I use a dedicated slash command in Cursor to generate the status report.\nPrompt:\nBased on this week's daily notes (2026-01-06 through 2026-01-10), \ngenerate a weekly status update for the ML Pipeline Refactor project.\n\nFormat:\n1. **Accomplishments** — What got done (bullet points)\n2. **In Progress** — What's actively being worked on\n3. **Blockers** — What's preventing progress\n4. **Next Week** — Planned focus areas\n5. **Metrics** — Any relevant numbers (optional)\n\nKeep it concise—this goes to leadership.\nOutput:\n## ML Pipeline Refactor — Week of Jan 6, 2026\n\n### ✅ Accomplishments\n- Completed Flink job design and configuration review\n- Set up parallel pipeline environment for validation\n- Finalized schema changes with ML team (freeze complete)\n- Documented migration workflow and rollback procedures\n\n### 🔄 In Progress\n- Parallel run validation (started Jan 10, running for 2 weeks)\n- Capacity planning approval (submitted, awaiting response)\n- Downstream consumer notifications\n\n### 🚧 Blockers\n- **Capacity approval pending** — Need 3 additional nodes; decision expected Monday\n- **Late schema feedback** — Pushed freeze by 2 days, absorbed into timeline\n\n### 📅 Next Week\n- Monitor parallel run data quality metrics\n- Begin stakeholder communication about cutover timeline\n- Draft customer notification if latency changes expected\n\n### 📊 Metrics\n- Data freshness: 24h → targeting &lt;5min post-migration\n- Parallel run started: 0/14 days complete\nTime saved: 45 minutes → 5 minutes"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-4-design-proposal-drafting",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-4-design-proposal-drafting",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Workflow 4: Design Proposal Drafting",
    "text": "Workflow 4: Design Proposal Drafting\nFor new features or tools, I describe the problem and let Claude draft the initial proposal.\nPrompt:\nI want to build a voice cloning TTS system that:\n- Runs on my Linux desktop with GPU\n- Is accessible from my Mac via API\n- Integrates with Cursor via MCP server\n- Uses my cloned voice\n\nDraft a design proposal covering:\n1. Architecture diagram\n2. Technology choices with pros/cons\n3. Implementation phases\n4. Hardware requirements\n5. Open questions\n\nStore it in 40-Resources/design_proposals/\nOutput: A comprehensive design document ready for iteration.\nThis is how I created proposals like (see this Github repo):\n\nvoice-cloning-tts-mcp-server.md\nai-meeting-assistant-teams.md\ncursor-config-sync.md\n\nTime saved: 2 hours → 20 minutes (for first draft)"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-summary",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflow-summary",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Workflow Summary",
    "text": "Workflow Summary\n\n\n\n\n\n\n\n\n\nWorkflow\nInput\nOutput\nTime Saved\n\n\n\n\nSlack → Daily Note\nRaw thread\nStructured summary + action items\n15 min → 2 min\n\n\nTranscript → Meeting Note\nRaw transcript\nFormatted note + tasks\n30 min → 5 min\n\n\nDaily Notes → Status\nWeek’s notes\nLeadership-ready update\n45 min → 5 min\n\n\nProblem → Design Proposal\nRequirements\nComprehensive proposal\n2 hr → 20 min\n\n\n\nThe key insight: I rarely write notes from scratch. I capture raw inputs (transcripts, threads, ideas) and let LLMs structure them."
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflows",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#workflows",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Workflows",
    "text": "Workflows\nDetailed process documentation that Claude helped write:\n# Spark to Flink Migration Workflow\n\n## Pipeline Overview\n\n```mermaid\ngraph TD\n    A[Kafka Topics] --&gt;|Consume| B[Flink Jobs]\n    B --&gt;|Transform| C[Feature Store]\n    C --&gt;|Serve| D[ML Models]\n```\n\n## Step 1: Prepare Input Schemas\n\nThe input topics should contain:\n| Field | Type | Required |\n|-------|------|----------|\n| `event_id` | STRING | ✅ |\n| `timestamp` | TIMESTAMP | ✅ |\n| `payload` | JSON | ✅ |\n\n[... comprehensive documentation continues ...]"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#scripts",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#scripts",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Scripts",
    "text": "Scripts\nPython automation that lives with my notes:\n40-Resources/scripts/\n├── schema_validator.py        # Validate data schemas\n├── generate_test_data.py      # Create test fixtures\n└── output/                    # Generated artifacts\n    ├── migration_report.csv\n    └── validation_results.json"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#data-files",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#data-files",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Data Files",
    "text": "Data Files\nCSV files, datasets, and reports:\n40-Resources/data/\n├── schema_inventory.csv\n├── migration_status.csv\n├── performance_benchmarks.csv\n└── capacity_planning.md"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#images",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#images",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Images",
    "text": "Images\nScreenshots and diagrams, organized by date or project:\n40-Resources/images/\n├── daily/\n│   └── 2025-12-15/\n│       └── architecture_diagram.png\n└── projects/\n    └── ml-pipeline/\n        └── data_flow_diagram.png"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#daily-project-links",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#daily-project-links",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Daily → Project Links",
    "text": "Daily → Project Links\nEvery daily note includes projects_touched in frontmatter:\nprojects_touched:\n  - \"[[ML Pipeline Refactor]]\"\n  - \"[[API Gateway Migration]]\""
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#bidirectional-references",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#bidirectional-references",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Bidirectional References",
    "text": "Bidirectional References\nProject notes link back to relevant daily notes:\n## Links\n- [[2025-12-11]] — Initial project setup\n- [[2026-01-06]] — Migration workflow documented"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#resource-links",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#resource-links",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Resource Links",
    "text": "Resource Links\nWorkflows and docs link to source notes:\n## Related\n- Project: [[ML Pipeline Refactor]]\n- Daily: [[2026-01-06]] — When this was created\nThis creates a web of connections that makes context retrieval easy—both for me and for Claude."
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#dataview-is-the-secret-weapon",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#dataview-is-the-secret-weapon",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Dataview is the Secret Weapon",
    "text": "Dataview is the Secret Weapon\nDataview lets me query my notes like a database:\n// List all high-priority incomplete tasks\nTABLE file.link AS \"Source\", text AS \"Task\"\nFROM \"10-Daily\"\nFLATTEN file.tasks AS item\nWHERE !item.completed AND item.p = 1\n// Recent project updates\nLIST\nFROM \"20-Projects\"\nWHERE status = \"active\"\nSORT file.mtime DESC"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#morning-5-min",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#morning-5-min",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Morning (~5 min)",
    "text": "Morning (~5 min)\n\nOpen today’s daily note (auto-created by Periodic Notes)\nReview yesterday’s carryover tasks\nSet focus areas for the day"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#during-the-day",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#during-the-day",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "During the Day",
    "text": "During the Day\n\nLog work under Work Log as I go\nAdd tasks with - [ ] and priorities\nPaste links to Slack threads, docs, PRs\nTake meeting notes inline or in separate meeting notes"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#end-of-day-5-min",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#end-of-day-5-min",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "End of Day (~5 min)",
    "text": "End of Day (~5 min)\n\nMark completed tasks with [x]\nReview blockers section\nUpdate project notes if significant progress\nSummarize today’s note into bulleted daily status updates for the standup meeting"
  },
  {
    "objectID": "posts/2026_01_13_obsidian_note_taking_system/index.html#weekly-15-min",
    "href": "posts/2026_01_13_obsidian_note_taking_system/index.html#weekly-15-min",
    "title": "LLM-Powered Work Notes: How I Use Obsidian and Cursor to Capture Everything",
    "section": "Weekly (~15 min)",
    "text": "Weekly (~15 min)\n\nUse Claude to generate status updates from daily notes\nUpdate project “Current Status” sections\nReview aggregated task list in 30-Areas/tasks.md"
  },
  {
    "objectID": "posts/2026_02_01_project_second_brain/index.html",
    "href": "posts/2026_02_01_project_second_brain/index.html",
    "title": "Project Second Brain",
    "section": "",
    "text": "I’ve recently posted about automating some of my daily work that’s not directly software engineering (mostly project management-related tasks, see my Obsidian note-taking system). Besides the codification of daily workflows, the biggest outcome of that project has been the lessons learned in building knowledge bases - after all, one of the goals for my note-taking system was to build a knowledge base for all the content I consume and produce at work (daily notes, project logs, work logs, meeting/thread summaries, etc.).\nAs with most projects, this one also spawned a number of follow-up threads, new ideas, and questions about how to improve knowledge management and the building of knowledge bases or knowledge representations more generally:"
  },
  {
    "objectID": "posts/2026_02_01_project_second_brain/index.html#the-origin-story",
    "href": "posts/2026_02_01_project_second_brain/index.html#the-origin-story",
    "title": "Project Second Brain",
    "section": "The Origin Story",
    "text": "The Origin Story\nAddressing the above questions formed the origin story for my “Second Brain” project. My initial project idea was not much more than a one-pager but it formalized the idea of taking the notion of a personal knowledge base even further.\nI wanted to create a system that acts as my personal knowledge base for everything — i.e., ingest all the various sources of written (and also audio) content I consume. The idea of building a personal knowledge base has been on my mind for a while, but the first time I attempted to build it was before LLM tooling was capable enough, and building a sufficiently capable system would not have been feasible given my limited time. Less than a year later, the advent of Opus 4.5 / GPT-5.2 class models made this system entirely possible. In fact, it took me less than a month to build a fairly feature-complete version of Project Second Brain. The project is described in a lot of detail in the main README file, the design docs, implementation plans, and tutorials. In this post, I just want to highlight some of the key features, observations, and lessons learned from the build process and use of “Second Brain”.\nThe idea of a knowledge base was part of the initial motivation. The other half was my growing concern about the limited knowledge absorption capabilities of the human mind. Ultimately, we absorb knowledge in a linear fashion (say 200 words per minute). Knowledge, though, is generated at increasingly exponential rates. So besides cataloging everything I consume, I was also looking for ways to speed up my learning — or rather, focus my learning on the highest value information. I wanted to build a learning system on top of my knowledge base that both summarizes information, extracts relevant and high-value content, and then helps me absorb that content in an efficient fashion (inspired by modern learning theory)."
  },
  {
    "objectID": "posts/2026_02_01_project_second_brain/index.html#overview-architecture-and-key-features",
    "href": "posts/2026_02_01_project_second_brain/index.html#overview-architecture-and-key-features",
    "title": "Project Second Brain",
    "section": "Overview, Architecture, and Key Features",
    "text": "Overview, Architecture, and Key Features\n\n\n\n\n\n\nTip📝 LLM-Generated Content\n\n\n\nThe content below was automatically generated based on the project’s design docs and README.\n\n\nThe result of this effort is Project Second Brain — an LLM-enabled personal knowledge management and learning system. It ingests data from various sources (academic papers, online articles, blog posts, newsletters, physical books, voice notes, podcasts, code repositories, and fleeting ideas), processes them through LLM-powered pipelines, stores everything in a graph-based knowledge representation, and then helps me actively learn from that content through exercises and spaced repetition.\nThe guiding quote for the project captures the philosophy well:\n\n“Tell me and I forget, teach me and I may remember, involve me and I learn.” — Xun Kuang\n\n\nSystem Architecture\nAt a high level, “Second Brain” follows a pipeline architecture: data sources feed into an ingestion layer, which passes through LLM-powered processing, and ultimately lands in a knowledge hub (Obsidian), a knowledge graph (Neo4j), and a learning system — all accessible through a web application and an AI-powered learning assistant.\n\n\n\nSecond Brain System Architecture (the layered presentation fits nicely into the “layer by layer” theme)\n\n\n\n\nKey Features\n“Second Brain” is built around three core capabilities:\n\nAutomated Ingestion — Diverse data source pipelines that handle PDFs (with handwriting OCR via Mistral Vision), web articles (via Raindrop.io API), physical book pages (photo → OCR → text), GitHub repositories (structure analysis + code summarization), and quick capture for fleeting ideas.\nIntelligent Processing — LLM-powered summarization, key concept extraction, semantic tag classification, automatic connection discovery between notes, follow-up task generation, and exercise/quiz generation.\nActive Learning — A full spaced repetition system (FSRS algorithm), AI-generated exercises (free recall, self-explanation, worked examples, code debugging, teach-back), mastery tracking, and an AI tutor that can query the knowledge graph conversationally.\n\n\n\nTech Stack\nThe system is built on a modern stack optimized for async processing, graph-based knowledge representation, and a responsive frontend:\n\n\n\n\n\n\n\n\nComponent\nTechnology\nPurpose\n\n\n\n\nBackend\n \nAsync REST API with OpenAPI docs\n\n\nFrontend\n  \nModern, fast web interface\n\n\nKnowledge Hub\n\nMarkdown-based, local-first note storage\n\n\nGraph DB\n\nKnowledge graph with Cypher queries\n\n\nRelational DB\n\nLearning records, user data, scheduling\n\n\nCache\n\nSession state, rate limiting\n\n\nTask Queue\n\nAsync background job processing\n\n\nLLM Interface\n\nUnified API to 100+ LLMs\n\n\nOCR / Vision\n \nPDF processing, handwriting recognition\n\n\nLLM Providers\n  \nSummarization, exercises, assistant\n\n\nContainerization\n\nOne-command deployment\n\n\n\n\n\nBackend (FastAPI)\nThe backend is a FastAPI application that serves as the brain of the operation. It exposes REST APIs organized into six domains:\n\n/api/ingest/* — Content ingestion (PDF, Raindrop, OCR, GitHub)\n/api/knowledge/* — Graph queries, search, connections, topics\n/api/practice/* — Exercise generation, response submission, feedback\n/api/review/* — Spaced repetition scheduling, due items, confidence updates\n/api/analytics/* — Learning curves, topic mastery, session history, weak spots\n/api/assistant/* — Chat interface, question generation, connection explanation\n\nBackground processing is handled by Celery workers with Redis as the message broker, enabling parallel ingestion of multiple sources without blocking the API.\n\n\nFrontend (React + Vite + TailwindCSS)\nThe frontend is a modern React 18 application built with Vite and styled with TailwindCSS. It features a dark-themed interface optimized for focused learning and includes the following pages:\nDashboard — The home screen answers “What should I do today?” at a glance. It shows your current streak, due review cards, daily progress, quick actions, weak spots, and a quick capture input for rapidly saving ideas or URLs.\n\n\n\nDashboard\n\n\nPractice Session — Enables deep learning through structured exercises grounded in cognitive science. You select topics with visual mastery indicators, configure session parameters, and practice through free recall, self-explanation, worked examples, code debugging, and teach-back prompts — all with immediate LLM-powered feedback.\n\n\n\nPractice Session\n\n\nReview Queue (Spaced Repetition) — Implements the FSRS (Free Spaced Repetition Scheduler) algorithm. Cards are presented with active recall — you type your answer before seeing the correct response. The LLM evaluates answers for semantic correctness, and you rate confidence (Again/Hard/Good/Easy) to adjust scheduling.\n\n\n\nReview Queue\n\n\nKnowledge Explorer — A unified interface for browsing the entire Obsidian-based knowledge base. Toggle between tree and list views, search notes in real-time, use the command palette (⌘K), and render notes inline with full markdown support including syntax highlighting, LaTeX, and wiki-link navigation.\n\n\n\nKnowledge Explorer\n\n\nKnowledge Graph — An interactive D3.js force-directed visualization of the Neo4j knowledge graph. Different node types (Content, Concepts, Notes) are color-coded, with edges representing relationships like RELATES_TO, CITES, EXTENDS, and PREREQUISITE_FOR.\n\n\n\nKnowledge Graph\n\n\nAnalytics Dashboard — Comprehensive learning insights including total time invested, streak tracking, mastery percentages, activity charts over configurable time periods, topic mastery radar, and weak spots analysis with targeted “Practice Now” actions.\n\n\n\nAnalytics Dashboard\n\n\nLearning Assistant — An AI-powered chat interface for conversational exploration of the knowledge base. Ask questions like “What do I know about attention mechanisms?” and the assistant searches the vault and knowledge graph, synthesizes information, and provides source citations.\n\n\n\nLearning Assistant\n\n\n\n\nMobile Capture (PWA)\nA critical bottleneck in knowledge management is capture friction — the effort required to get information into the system. To minimize this, “Second Brain” includes a companion Progressive Web App (PWA) — a web application that can be installed on your phone and behaves like a native app. Once installed via “Add to Home Screen,” the PWA gets its own app icon, launches in a standalone window without browser, works offline via a service worker that caches assets and queues requests, and can receive shared content (URLs, text, images) from other apps through the OS share sheet. Unlike a native iOS/Android app, though, there’s no App Store submission — it’s just a lightweight frontend served from the same backend, installable in one tap.\nThe PWA provides a mobile-optimized interface for on-the-go capture with offline support:\n\n\n\n\n\n\n\nFeature\nDetails\n\n\n\n\nInstallable\n“Add to Home Screen” — launches like a native app\n\n\nOffline capable\nService worker caches assets and queues captures in IndexedDB\n\n\nBackground sync\nAutomatically uploads queued captures when connection is restored\n\n\nShare target\nReceive shared URLs, text, and images from other apps (Android)\n\n\n&lt; 3 second capture\nMinimal UI with large touch targets optimized for speed\n\n\n\nThe PWA runs as a separate lightweight frontend and communicates with the same backend API. All captures are processed asynchronously via Celery and flow into the standard ingestion pipeline.\n\n\n\n\n\n\nHome Screen\n\n\n\n\n\n\n\nText Capture\n\n\n\n\n\n\n\nURL Capture\n\n\n\n\n\n\n\nData Layer\nThe data layer is split across four storage systems, each optimized for its role:\n\n\n\nSecond Brain Data Layer Architecture\n\n\nNeo4j stores the knowledge graph with nodes for Concepts, Sources, Topics, Authors, and Tags. Edges encode relationships: RELATES_TO, CITES, CONTRADICTS, EXTENDS, and PREREQUISITE_FOR. This enables queries like “What do I know about X?” and “What connects A to B?” — powering the Graph RAG that feeds the learning assistant.\nObsidian serves as the human-readable knowledge hub. Notes are organized by content type (sources/papers/, sources/articles/, etc.) with secondary semantic tags (ml/transformers, systems/distributed) and rich bidirectional wiki-links. In practice, Obsidian and the web UI provide overlapping visualization and browsing capabilities — the web app’s Knowledge Explorer can do everything Obsidian does for reading and navigating notes. Obsidian may be deprecated as the user-facing layer in the future to avoid the overhead of keeping two data sources in sync, with the Markdown vault retained purely as a local-first storage backend.\nPostgreSQL tracks all learning activity — practice attempts, confidence ratings, FSRS scheduling parameters, time invested, and mastery progression over time.\nRedis handles ephemeral state — active user sessions, temporary exercise state during practice, and rate limiting for API calls.\n\n\nLearning Theory Foundations\nWhat sets “Second Brain” apart from a pure knowledge management system is its grounding in cognitive science research on learning and memory. The learning system is not just flashcards — it’s designed around evidence-based principles (see the full Learning Theory document for detailed research summaries):\n\n\n\nResearch\nKey Finding\nHow “Second Brain” Implements It\n\n\n\n\nEricsson (2008) — Deliberate Practice\nExpertise requires structured practice with feedback, not passive experience\nAdaptive difficulty + immediate LLM feedback on exercises\n\n\nBjork & Bjork (2011) — Desirable Difficulties\nSpacing, interleaving, and generation enhance long-term retention\nFSRS spaced repetition + varied exercise types\n\n\nDunlosky et al. (2013) — Learning Techniques\nPractice testing and distributed practice are highest utility; highlighting/rereading are lowest\nRetrieval-based exercises; avoids recognition-only tasks\n\n\nVan Gog et al. (2011) — Cognitive Load\nWorked examples before problems for novices\nAdaptive: examples → testing as mastery increases\n\n\nChi et al. (1994) — Self-Explanation\nPrompting self-explanation builds correct mental models\nSelf-explanation prompts embedded in exercises\n\n\n\nThe core principles that emerge:\n\nLearning ≠ Performance — Easy recall during study (retrieval strength) doesn’t guarantee long-term retention (storage strength). The system optimizes for storage strength.\nGeneration over Recognition — Producing answers from memory beats re-reading or highlighting. All exercises require active generation.\nDesirable Difficulties — Spacing, interleaving, testing, and variation slow immediate performance but dramatically enhance retention.\nAdaptive Scaffolding — Novices get worked examples; as mastery increases, the system shifts to retrieval practice and interleaved questioning.\n\nExercise types span the full cognitive spectrum:\n\n\n\n\n\n\n\n\nContent Type\nExercise Types\nDifficulty Applied\n\n\n\n\nConceptual\nExplain-in-own-words, compare/contrast, teach-back\nGeneration effect (no notes allowed)\n\n\nTechnical\nImplement from scratch, debug code, extend functionality\nGeneration + Variation\n\n\nProcedural\nReconstruct steps from memory, adapt to new scenario\nRetrieval practice + Interleaving\n\n\nAnalytical\nCase study analysis, predict outcomes, critique approaches\nGeneration + Spacing\n\n\n\n\n\n\n\n\n\nEnd of LLM-generated content"
  },
  {
    "objectID": "posts/2026_02_01_project_second_brain/index.html#observations-about-the-build-design-process",
    "href": "posts/2026_02_01_project_second_brain/index.html#observations-about-the-build-design-process",
    "title": "Project Second Brain",
    "section": "Observations About the Build / Design Process",
    "text": "Observations About the Build / Design Process\nBefore concluding, I just want to point out a few meta-observations about the build / design process that are not strictly technical. Instead, these are more about the changing role of the software engineer in the software development process and reflect on some of the conclusions influential figures in the field have drawn about the future of software engineering:\n\nSteve Yegge (creator of “Gas Town” and a recent interview with the Pragmatic Engineer podcast)\nAndrej Karpathy’s tweet about “I’ve never felt this much behind as a programmer.”\nBoris Cherny’s tweet (creator of Claude Code) about “Using Claude Code”\n\n\nAgentic Coding vs Vibe Coding\nThis project was as much about creating a usable product as it was about improving my skillset (in agentic coding, LLM-based workflows, app development, etc.). While this project was almost entirely implemented by Cursor / Claude Code, I still want to draw a distinction between vibe-coding and principled AI-assisted software development. While I let coding agents implement the entire system, the design process was highly iterative and involved and significantly more guided than the hands-off approach of vibe coding.\nThe limitations of vibe coding have been humorously illustrated in the “Ralph Wiggum as a”software engineer” blog post by Geoffrey Huntley — the idea that you can make coding agents work autonomously by simply running them in a loop until they achieve “complete.” Boris Cherny released a variation that learns from its mistakes, using failures as data. While these approaches kind of work, in practice, iteration without structure is just thrashing.\nInstead of iterating on code directly or on vague prompts, this iterative discipline should be moved upstream: iterate on the spec until it’s well-defined, iterate on acceptance criteria until they’re testable, iterate on tests until they meaningfully constrain behavior. I tried to apply that same methodology to this project and directed Cursor to draft lengthy design and implementation plans that I iterated on multiple times before writing any code. Reading these docs consumed most of my time (not the review of the code). But given these robust design docs, I could trust the agents a lot more to get it right in one or two shots.\n\n\nIteration on Code\nThere were iterations on the code side as well, but I never manually corrected the code directly. I went back to the drawing board instead (i.e., design docs), updated them, and then had the agent update the implementation.\n\n\n\nTwo approaches to iterating on code — fix code directly vs. update the design spec first\n\n\nI also want to highlight the importance of good test coverage. I used unit and integration tests extensively to sanity check the codebase and catch regressions early. Any time I discovered a bug or undesired behavior, Cursor would automatically fix it and add a test to prevent it from happening again. At this point, the project has well over a thousand unit tests, hundreds of integration tests, and dozens of full end-to-end frontend tests. I would not have the confidence I have in the codebase without these tests.\n\n\nParallelization of Agents\nWhile I had multiple agents work on the project at the same time, I managed the parallelization manually, i.e., I directed the individual agents to work on different non-overlapping parts of the project (backend vs. frontend vs. design docs). In my next project I’ll definitely leverage git trees and a much higher number of agents (maybe even Gas Town).\nIn terms of the “8 Stages of Dev Evolution To AI” (see image above), this was somewhere between stage 4 (still in an IDE) and stage 6 (multiple parallel agents working on different parts of the project) but still included a fair amount of code review (at least on the backend design and data model).\n\n\nBackend vs Frontend\nI spent most of my time getting the backend right, i.e., the data model, database structure, API endpoints, ingestion workflows, etc. In contrast, I reviewed almost none of the frontend code — mostly because a broken frontend is almost immediately noticeable. Backend and data issues, in my experience, can be more insidious and harder to diagnose.\n\n\nOpus-class Models are Beasts\nI knew Opus 4.5 / GPT-5.2 class models were really capable, but I was still impressed by how much coherent code they could produce. “Second Brain” has grown to over 100k lines of code and it works - using tons of unit and integration testing as a sanity check.\nThis impression was accompanied by a bittersweet realization of the permanently changed nature of the software engineer’s role - we’ll never go back to writing code by hand in a text editor again (Steve Yegge captures this sentiment nicely in his now one-year old post about “The death of the junior developer”).\nOn the other hand, I am really excited that these powerful models enable every developer (and non-developer) to build software at an order of magnitude higher velocity than ever before. We are no longer constrained by execution but only by our own creativity, imagination, and the number of ideas we can generate.\n\nIn the meantime, I’ll borrow Andrew Ng’s catchphrase here: “Keep building!”\nDaniel"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "",
    "text": "This notebook explores full LLM architecture for GPT based on Sebastian Raschka’s book (Chapter 4), implementing normalization layers, shortcut connections, and transformer blocks. This notebook also shows how to compute the parameter count as well as the storage requirements of GPT-like models.\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 4\n\nGPT-2 original paper: “Language Models Are Unsupervised Multitask Learners”\n\n\n\n\nGPT architecture\n\n\nThe components of a GPT-like architecture are shown in the image below.\n\n\n\nGPT components\n\n\nThe following figure shows the flow of data through the model and how they are transformed at each stage.\n\n\n\nGPT data flow\n\n\n\nimport dataclasses\nfrom typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\n\nimport tiktoken"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#resources",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 4\n\nGPT-2 original paper: “Language Models Are Unsupervised Multitask Learners”\n\n\n\n\nGPT architecture\n\n\nThe components of a GPT-like architecture are shown in the image below.\n\n\n\nGPT components\n\n\nThe following figure shows the flow of data through the model and how they are transformed at each stage.\n\n\n\nGPT data flow\n\n\n\nimport dataclasses\nfrom typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\n\nimport tiktoken"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#example",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#example",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "Example",
    "text": "Example\n\n# Example of layer normalization.\ntorch.set_printoptions(sci_mode=False)\ntorch.manual_seed(123)\n\n# Create two training examples with 5 features each.\nbatch_example = torch.randn(2, 5)\n\n# Create a basic neural network layer consisting of a Linear layer followed by a non-linear\n# activation function, ReLU (short for rectified linear unit).\nlayer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\nout = layer(batch_example)\n\n# Compute mean and variance along the feature dimension (i.e. the last dimension).\n# NOTE: Using keepdim=True in operations like mean or variance calculation ensures that the output\n#       tensor retains the same number of dimensions as the input tensor, even though the operation\n#       reduces the tensor along the dimension specified via dim. Here, without keepdim=True, the\n#       output tensor would be a two-dimensional vector (e.g. [1, 2]) rather than a 2x1-dimensional\n#       matrix (e.g. [[1], [2]]).\nmean = out.mean(dim=-1, keepdim=True)\nvar = out.var(dim=-1, keepdim=True)\nprint(f\"\\nRaw layer outputs:\\n{'-' * 18}\\n{out}\")\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n\n# Apply layer normalization.\nout_norm = (out - mean) / torch.sqrt(var)\nmean = out_norm.mean(dim=-1, keepdim=True)\nvar = out_norm.var(dim=-1, keepdim=True)\nprint(f\"\\nNormalized layer outputs:\\n{'-' * 25}\\n{out_norm}\")\nprint(f\"\\nMean:\\n{mean}\")\nprint(f\"\\nVariance:\\n{var}\")\n\n\nRaw layer outputs:\n------------------\ntensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n       grad_fn=&lt;ReluBackward0&gt;)\nMean:\n tensor([[0.1324],\n        [0.2170]], grad_fn=&lt;MeanBackward1&gt;)\nVariance:\n tensor([[0.0231],\n        [0.0398]], grad_fn=&lt;VarBackward0&gt;)\n\nNormalized layer outputs:\n-------------------------\ntensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n       grad_fn=&lt;DivBackward0&gt;)\n\nMean:\ntensor([[    0.0000],\n        [    0.0000]], grad_fn=&lt;MeanBackward1&gt;)\n\nVariance:\ntensor([[1.0000],\n        [1.0000]], grad_fn=&lt;VarBackward0&gt;)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#a-layer-normalization-class",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#a-layer-normalization-class",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "A layer normalization class",
    "text": "A layer normalization class\n\nclass LayerNorm(nn.Module):\n    \"\"\"Layer normalization (https://arxiv.org/abs/1607.06450).\n\n    This specific implementation of layer normalization operates on the last dimension of\n    the input tensor x, which represents the embedding dimension (emb_dim).\n    \"\"\"\n\n    def __init__(self, emb_dim: int):\n        super().__init__()\n\n        # Use a small constant which will be added to the variance to prevent division by zero.\n        self.eps = 1e-5\n\n        # The scale and shift are two trainable parameters (of the same dimension as the input)\n        # that the LLM automatically adjusts during training.\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n\n\n# Test the layer normalization class.\nln = LayerNorm(emb_dim=5)\nout_ln = ln(batch_example)\nmean = out_ln.mean(dim=-1, keepdim=True)\nvar = out_ln.var(dim=-1, unbiased=False, keepdim=True)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)\n\nMean:\n tensor([[    -0.0000],\n        [     0.0000]], grad_fn=&lt;MeanBackward1&gt;)\nVariance:\n tensor([[1.0000],\n        [1.0000]], grad_fn=&lt;VarBackward0&gt;)\n\n\nGELU activation function\nThis section is skipped in this notebook since torch already implements to full version of GELU as well as the curve fitting approximation.\nSee GELU activation function\nThe below figure shows a comparison of GELU and ReLU.\n\n\n\nFeed forward network"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#analyzing-the-model-architecture",
    "href": "posts/2025_05_16_llms_from_scratch_part_4/chapter_04_gpt_from_scratch.html#analyzing-the-model-architecture",
    "title": "LLMs From Scratch - Chapter 4: GPT from Scratch",
    "section": "Analyzing the model architecture",
    "text": "Analyzing the model architecture\n\n# NOTE: The reason for the total number of parameters to be 163M instead of 124M is a concept called\n#       weight tying, which was used in the original GPT-2 architecture. It means that the original\n#       GPT-2 architecture reuses the weights from the token embedding layer in its output layer.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")\nprint(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\nprint(\"Output layer shape:\", model.out_head.weight.shape)\n\nTotal number of parameters: 163,009,536\nToken embedding layer shape: torch.Size([50257, 768])\nOutput layer shape: torch.Size([50257, 768])\n\n\n\ntotal_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\nprint(\n    f\"Number of trainable parameters \"\n    f\"considering weight tying: {total_params_gpt2:,}\"\n)\n\nNumber of trainable parameters considering weight tying: 124,412,160\n\n\n\n# Calculates the total size in bytes (assuming float32, 4 bytes per parameter).\ntotal_size_bytes = total_params * 4\ntotal_size_mb = total_size_bytes / (1024 * 1024)\nprint(f\"Total size of the model: {total_size_mb:.2f} MB\")\n\nTotal size of the model: 621.83 MB"
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html",
    "href": "posts/2025_05_06_website_tech_stack/index.html",
    "title": "Tech Stack for This Website",
    "section": "",
    "text": "For the longest time I have been hosting some version of my website using WordPress, which was the style at the time (this is 2012 onwards but pre-ChatGPT). While I really enjoyed the flexibility and ease of use of WordPress, it’s always been cumbersome to actually host the website. My last setup was to use the most basic AWS EC2 instance and redirect my domain to it. That setup always came with a high maintenance burden given the fact that the container had to be kept up to date and the website content (i.e. the DB backing the website) had to be manually backed up."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#pros",
    "href": "posts/2025_05_06_website_tech_stack/index.html#pros",
    "title": "Tech Stack for This Website",
    "section": "Pros",
    "text": "Pros\n\nFull control, blazing fast, easy to integrate MDX (Markdown + React), perfect for code-heavy tutorials.\nCustom Domain: Easy to connect on both Vercel and Netlify."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#cons",
    "href": "posts/2025_05_06_website_tech_stack/index.html#cons",
    "title": "Tech Stack for This Website",
    "section": "Cons",
    "text": "Cons\n\nNeeds some dev setup. You write in Markdown or MDX."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#a-note-about-deployment",
    "href": "posts/2025_05_06_website_tech_stack/index.html#a-note-about-deployment",
    "title": "Tech Stack for This Website",
    "section": "A note about deployment",
    "text": "A note about deployment\nPublishing updates to the website is trivial with Quarto. Their publishing workflow is well-documented and well-integrated with Netlify. Overall, Quarto’s documenation is extensive which makes it easy to get started and get support. All that’s required is to create a _publish.yml file in the root directory of the repository and executing the following command:\nquarto publish netlify\nThe contents of the _publish.yml file look something like the following:\n- source: project\n  netlify:\n    - id: \"5f3abafe-68f9-4c1d-835b-9d668b892001\"\n      url: \"https://danielpickem.com\"\nThis tells Quarto to publish the website to Netlify and use the ID and URL from Netlify’s “Site settings” dashboard (see image below).\n\n\n\nNetlify Site Settings\n\n\nThis blog post is already a useful tutorial as I just had to look up how to deploy the updated website to Netlify!\nAnother useful resource was this blogpost about deploying a Github page via Netlify."
  },
  {
    "objectID": "posts/2025_05_06_website_tech_stack/index.html#quarto-templates",
    "href": "posts/2025_05_06_website_tech_stack/index.html#quarto-templates",
    "title": "Tech Stack for This Website",
    "section": "Quarto templates",
    "text": "Quarto templates\nQuarto also boasts an extensive gallery of templates for various use-cases (such as books, blogs, presentations, etc.). This website is using a blog template, in particular the one by Chris von Csefalvay - thanks Chris! Modifying an existing template gets you up and running really quickly, especially if you have an AI-assisted code editor like Cursor - but that’s a topic for another post.\n\n\n\nQuarto Blog Template\n\n\nStay curious,\nDaniel"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "This notebook explores the fine-tuning process of LLMs with the purpose of creating a classification model based on Sebastian Raschka’s book (Chapter 6). In particular, it discusses the following:\n\nIntroducing different LLM fine-tuning approaches\nPreparing a dataset for text classification\nModifying a pretrained LLM for fine-tuning\nFine-tuning an LLM to identify spam messages\nEvaluating the accuracy of a fine-tuned LLM classifier\nUsing a fine-tuned LLM to classify new data\n\n\n\n\nInstruction-tuned models can typically handle a broader range of tasks\nMore general approach that can handle multiple tasks\nBest suited for models that need to handle a variety of tasks based on complex user instructions\nThese models improve flexibility and interaction quality\nInstruction fine-tuning requires larger datasets and greater computational resources\n\n\n\n\n\nIdeal for projects requiring precise categorization into predefined classes (e.g. sentiment analysis or spam detection)\nSpecialized approach targeted at outputting a specific set of labels\nThe model is restricted to only the labels encountered during training\nRequires less data and compute power\n\n\n\n\nAll concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content.\n\n\n\n\nSebastian Raschka’s GitHub\nBook Information\n\nChapter 6\n\n\n\n\n\nTopic overview\n\n\n\n# Install import-ipynb for importing ipynb files.\n# %pip install import-ipynb\n\n\nfrom typing import Optional, Tuple\nimport urllib.request\nimport zipfile\nimport os\nfrom pathlib import Path\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\nfrom chapter_02_dataset_creation import create_dataloader_v1\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\n\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n)\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#instruction-fine-tuning",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#instruction-fine-tuning",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "Instruction-tuned models can typically handle a broader range of tasks\nMore general approach that can handle multiple tasks\nBest suited for models that need to handle a variety of tasks based on complex user instructions\nThese models improve flexibility and interaction quality\nInstruction fine-tuning requires larger datasets and greater computational resources"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-fine-tuning",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-fine-tuning",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "Ideal for projects requiring precise categorization into predefined classes (e.g. sentiment analysis or spam detection)\nSpecialized approach targeted at outputting a specific set of labels\nThe model is restricted to only the labels encountered during training\nRequires less data and compute power"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#acknowledgment",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#acknowledgment",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "All concepts, architectures, and implementation approaches are credited to Sebastian Raschka’s work. This repository serves as my personal implementation and notes while working through the book’s content."
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#resources",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#resources",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "",
    "text": "Sebastian Raschka’s GitHub\nBook Information\n\nChapter 6\n\n\n\n\n\nTopic overview\n\n\n\n# Install import-ipynb for importing ipynb files.\n# %pip install import-ipynb\n\n\nfrom typing import Optional, Tuple\nimport urllib.request\nimport zipfile\nimport os\nfrom pathlib import Path\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport tiktoken\nfrom tqdm.notebook import tqdm\n\nimport pandas as pd\n\n# Import previous chapter dependencies.\n# See https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n# NOTE: Importing these functions seems to run the entire cell the symbol is defined in, which would\n#       suggest that symbols should be defined in separate cells from the test code.\nimport import_ipynb\nfrom gpt_download import download_and_load_gpt2\nfrom chapter_02_dataset_creation import create_dataloader_v1\nfrom chapter_04_gpt_from_scratch import (\n    GPTConfig,\n    GPTModel,\n    generate_text_simple,\n)\n\n# NOTE: Importing another ipynb file basically runs the entire imported notebook.\nfrom chapter_05_pretraining_on_unlabeled_data import (\n    generate,\n    token_ids_to_text,\n    text_to_token_ids,\n)\n\n# Define the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Determine the device to run the model on.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#download-the-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#download-the-dataset",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Download the dataset",
    "text": "Download the dataset\n\nurl = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\nzip_path = Path(\"data/sms_spam_collection.zip\")\nextracted_path = Path(\"data/sms_spam_collection\")\ndata_file_path = extracted_path / \"SMSSpamCollection.tsv\"\n\n\ndef download_and_unzip_spam_data(\n    url: str, zip_path: Path, extracted_path: Path, data_file_path: Path\n):\n    \"\"\"Download and unzip the spam data from the UCI repository.\n\n    Args:\n        url: The URL of the zip file.\n        zip_path: The path to save the zip file.\n        extracted_path: The path to save the extracted files.\n        data_file_path: The path to save the data file.\n    \"\"\"\n    # Check if the file already exists.\n    if data_file_path.exists():\n        print(f\"{data_file_path} already exists. Skipping download \" \"and extraction.\")\n        return\n\n    # Download the zip file.\n    with urllib.request.urlopen(url) as response:\n        with open(zip_path, \"wb\") as out_file:\n            out_file.write(response.read())\n\n    # Extract the zip file.\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extracted_path)\n\n    # Add a .tsv extension to the file (tab-separated values).\n    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n    os.rename(original_file_path, data_file_path)\n    print(f\"File downloaded and saved as {data_file_path}\")\n\n\ndownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#load-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#load-dataset",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Load dataset",
    "text": "Load dataset\n\n# Load data into a pandas DataFrame.\ndf = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n\n# Show the label count.\nprint(df[\"Label\"].value_counts())\n\n# Show a few examples.\ndf.head()"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#balancing-the-dataset",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#balancing-the-dataset",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Balancing the dataset",
    "text": "Balancing the dataset\n\n# Create a balanced dataset by undersampling the majority class.\ndef create_balanced_dataset(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Create a balanced dataset by undersampling the majority class.\n\n    NOTE: This function can quite significantly reduce the size of the dataset.\n\n    Args:\n        df: The input DataFrame.\n\n    Returns:\n        A balanced DataFrame.\n    \"\"\"\n    num_spam = len(df[df[\"Label\"] == \"spam\"])\n    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n    return balanced_df.reset_index(drop=True)\n\n\n# Create a balanced dataset.\nbalanced_df = create_balanced_dataset(df)\nprint(balanced_df[\"Label\"].value_counts())\n\n# Convert string labels to integers.\nbalanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\nbalanced_df"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#splitting-the-datast",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#splitting-the-datast",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Splitting the datast",
    "text": "Splitting the datast\n\ndef random_split(\n    df: pd.DataFrame, train_frac: float, validation_frac: float\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Split a DataFrame into train, validation, and test sets.\n\n    NOTE: The size of the test set is implied to be the remainder of train and validation fraction\n          (all fractions should add up to 1).\n\n    Args:\n        df: The input DataFrame.\n        train_frac: The fraction of the dataset to use for training.\n        validation_frac: The fraction of the dataset to use for validation.\n\n    Returns:\n        A tuple of DataFrames for train, validation, and test sets.\n    \"\"\"\n    # Shuffle the entire DataFrame\n    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n\n    # Calculate split indices (for train and validation explicitly.)\n    train_end = int(len(df) * train_frac)\n    validation_end = train_end + int(len(df) * validation_frac)\n\n    # Split the DataFrame.\n    train_df = df[:train_end]\n    validation_df = df[train_end:validation_end]\n    test_df = df[validation_end:]\n\n    return train_df, validation_df, test_df\n\n\n# Test size is implied to be 0.2 as the remainder.\ntrain_df, validation_df, test_df = random_split(\n    df=balanced_df, train_frac=0.7, validation_frac=0.1\n)\nprint(f\"Train set size: {len(train_df)}\")\nprint(f\"Validation set size: {len(validation_df)}\")\nprint(f\"Test set size: {len(test_df)}\")\n\n# Save the DataFrames to CSV files.\ntrain_df.to_csv(extracted_path / \"train.csv\", index=None)\nvalidation_df.to_csv(extracted_path / \"validation.csv\", index=None)\ntest_df.to_csv(extracted_path / \"test.csv\", index=None)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-datasets",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-datasets",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Creating the datasets",
    "text": "Creating the datasets\nPreviously, we utilized a sliding window technique to generate uniformly sized text chunks, which we then grouped into batches for more efficient model training. Each chunk functioned as an individual training instance. However, we are now working with a spam dataset that contains text messages of varying lengths. To batch these messages as we did with the text chunks, we have two primary options:\n\nTruncate all messages to the length of the shortest message in the dataset or batch.\nPad all messages to the length of the longest message in the dataset or batch.\n\nThe first option is computationally cheaper, but it may result in significant information loss if shorter messages are much smaller than the average or longest messages, potentially reducing model performance. So, we opt for the second option, which preserves the entire content of all messages.\nTo implement batching, where all messages are padded to the length of the longest message in the dataset, we add padding tokens to all shorter messages. For this purpose, we use “&lt;|endoftext|&gt;” as a padding token. However, instead of appending the string “&lt;|endoftext|&gt;” to each of the text messages directly, we can add the token ID corresponding to “&lt;|endoftext|&gt;” to the encoded text messages\n\n\n\nPadding approach\n\n\nThe example below shows what a training batch looks like. A single training batch consisting of eight text messages represented as token IDs. Each text message consists of 120 token IDs. A class label array stores the eight class labels corresponding to the text messages, which can be either 0 (“not spam”) or 1 (“spam”).\n\n\n\nTraining batch example\n\n\n\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nprint(tokenizer.encode(\"&lt;|endoftext|&gt;\", allowed_special={\"&lt;|endoftext|&gt;\"}))\n\n\n# Creating a Dataset class.\nclass SpamDataset(Dataset):\n    \"\"\"Dataset class for the spam dataset.\"\"\"\n\n    def __init__(\n        self,\n        csv_file: Path,\n        tokenizer: tiktoken.Encoding,\n        max_length: Optional[int] = None,\n        pad_token_id: int = 50256,\n    ):\n        \"\"\"\n        Initializes the SpamDataset class.\n\n        Args:\n            csv_file: The path to the CSV file containing the data.\n            tokenizer: The tokenizer to use.\n            max_length: The maximum length of the encoded texts.\n            pad_token_id: The ID of the padding token.\n        \"\"\"\n        # Load the data from the CSV file.\n        self.data = pd.read_csv(csv_file)\n\n        # Pretokenize all texts.\n        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n\n        if max_length is None:\n            # If no maximum length is provided, use the longest encoded text.\n            self.max_length = self._longest_encoded_length()\n        else:\n            # Truncate sequences if they are longer than max_length.\n            self.max_length = max_length\n            self.encoded_texts = [\n                encoded_text[: self.max_length] for encoded_text in self.encoded_texts\n            ]\n\n        # Pads sequences to the longest sequence\n        self.encoded_texts = [\n            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n            for encoded_text in self.encoded_texts\n        ]\n\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        encoded = self.encoded_texts[index]\n        label = self.data.iloc[index][\"Label\"]\n        return (\n            torch.tensor(encoded, dtype=torch.long),\n            torch.tensor(label, dtype=torch.long),\n        )\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def _longest_encoded_length(self) -&gt; int:\n        \"\"\"Determine the longest encoded text length.\"\"\"\n        return max(len(encoded_text) for encoded_text in self.encoded_texts)\n\n\n# Load the training dataset.\ntrain_dataset = SpamDataset(\n    csv_file=extracted_path / \"train.csv\", max_length=None, tokenizer=tokenizer\n)\n\n# Load the validation and test sets and limit the max length to the same value as the training set.\n# NOTE: Importantly, any validation and test set samples exceeding the length of the longest\n#       training example are truncated using encoded_text[:self.max_length] in the SpamDataset code\n#       we defined earlier. This truncation is optional; you can set max_length=None for both\n#       validation and test sets, provided there are no sequences exceeding 1,024 tokens in these\n#       sets.\nval_dataset = SpamDataset(\n    csv_file=extracted_path / \"validation.csv\",\n    max_length=None,\n    tokenizer=tokenizer,\n)\ntest_dataset = SpamDataset(\n    csv_file=extracted_path / \"test.csv\",\n    max_length=None,\n    tokenizer=tokenizer,\n)\n\n# Show the maximum length of the encoded texts.\nprint(f\"Maximum length of the encoded texts: {train_dataset.max_length}\")\nprint(f\"Maximum length of the encoded texts: {val_dataset.max_length}\")\nprint(f\"Maximum length of the encoded texts: {test_dataset.max_length}\")\n\n# Verify that the maximum length does not exceed the context length.\nassert train_dataset.max_length &lt;= GPT_CONFIG_124M.context_length\nassert val_dataset.max_length &lt;= GPT_CONFIG_124M.context_length\nassert test_dataset.max_length &lt;= GPT_CONFIG_124M.context_length"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-data-loaders",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#creating-the-data-loaders",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Creating the data loaders",
    "text": "Creating the data loaders\n\n# This num_worker setting ensures compatibility with most computers.\nnum_workers = 0\nbatch_size = 8\ntorch.manual_seed(123)\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n    drop_last=True,\n)\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\ntest_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    drop_last=False,\n)\n\n# Show the size of the data loaders.\nprint(f\"Train set size: {len(train_loader)}\")\nprint(f\"Validation set size: {len(val_loader)}\")\nprint(f\"Test set size: {len(test_loader)}\")\n\n# Show the first batch of the training set.\n# NOTE: As we can see, the input batches consist of eight training examples with 120 tokens each,\n#       as expected. The label tensor stores the class labels corresponding to the eight training\n#       examples.\nprint(\"\\nFirst training batch:\")\nfor input_batch, target_batch in train_loader:\n    print(\"Input batch dimensions:\", input_batch.shape)\n    print(\"Label batch dimensions\", target_batch.shape)\n    break"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#initializing-a-model-with-pretrained-weights",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#initializing-a-model-with-pretrained-weights",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Initializing a model with pretrained weights",
    "text": "Initializing a model with pretrained weights\n\nimport dataclasses\n\n# Load the base config.\nGPT_CONFIG_124M = GPTConfig(\n    vocab_size=50257,  # as used by the BPE tokenizer for GPT-2.\n    context_length=1024,\n    emb_dim=768,\n    n_heads=12,\n    n_layers=12,\n    dropout_rate=0.0,  # disable dropout for inference\n    qkv_bias=False,\n)\n\n# Update the model configuration to conform to the model size.\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Instantiate a base config.\ntmp_config = dataclasses.asdict(GPT_CONFIG_124M)\n\n# Load the overlay parameters.\nmodel_name = \"gpt2-small (124M)\"\ntmp_config.update(model_configs[model_name])\n\n# Update the context length to match OpenAI's GPT-2 models.\ntmp_config.update({\"context_length\": 1024})\n\n# OpenAI used bias vectors in the multi-head attention module’s linear layers to implement the\n# query, key, and value matrix computations. Bias vectors are not commonly used in LLMs anymore as\n# they don’t improve the modeling performance and are thus unnecessary. However, since we are\n# working with pretrained weights, we need to match the settings for consistency and enable these\n# bias vectors.\ntmp_config.update({\"qkv_bias\": True})\n\n# Instantiate the new configuration.\nNEW_CONFIG = GPTConfig(**tmp_config)\n\n# Initialize the model with the new configuration.\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()\n\n\n# NOTE: This code is copied from chapter_05_pretraining_on_unlabeled_data.ipynb because the import\n#       from load_weights_into_gpt.py is not working.\n\n\ndef assign(left, right):\n    \"\"\"Safely assign the right weight tensor to the left layer.\n\n    Checks whether two tensors or arrays (left and right) have the same dimensions or shape and\n    returns the right tensor as trainable PyTorch parameters.\n    \"\"\"\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n\n    return torch.nn.Parameter(torch.tensor(right))\n\n\ndef load_weights_into_gpt(gpt: GPTModel, params: dict):\n    # Sets the model’s positional and token embedding weights to those specified in params.\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n\n    # Iterates over each transformer block in the model.\n    for b in range(len(params[\"blocks\"])):\n        # The np.split function is used to divide the attention and bias weights into three equal\n        # parts for the query, key, and value components.\n        q_w, k_w, v_w = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.weight = assign(\n            gpt.trf_blocks[b].mha.W_q.weight, q_w.T\n        )\n        gpt.trf_blocks[b].mha.W_k.weight = assign(\n            gpt.trf_blocks[b].mha.W_k.weight, k_w.T\n        )\n        gpt.trf_blocks[b].mha.W_v.weight = assign(\n            gpt.trf_blocks[b].mha.W_v.weight, v_w.T\n        )\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n        )\n        gpt.trf_blocks[b].mha.W_q.bias = assign(gpt.trf_blocks[b].mha.W_q.bias, q_b)\n        gpt.trf_blocks[b].mha.W_k.bias = assign(gpt.trf_blocks[b].mha.W_k.bias, k_b)\n        gpt.trf_blocks[b].mha.W_v.bias = assign(gpt.trf_blocks[b].mha.W_v.bias, v_b)\n        gpt.trf_blocks[b].mha.out_proj.weight = assign(\n            gpt.trf_blocks[b].mha.out_proj.weight,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].mha.out_proj.bias = assign(\n            gpt.trf_blocks[b].mha.out_proj.bias,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n        )\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n        )\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n        )\n        gpt.trf_blocks[b].pre_attention_norm.scale = assign(\n            gpt.trf_blocks[b].pre_attention_norm.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_attention_norm.shift = assign(\n            gpt.trf_blocks[b].pre_attention_norm.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.scale = assign(\n            gpt.trf_blocks[b].pre_ff_norm.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n        )\n        gpt.trf_blocks[b].pre_ff_norm.shift = assign(\n            gpt.trf_blocks[b].pre_ff_norm.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n        )\n\n        gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n        gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n\n        # The original GPT-2 model by OpenAI reused the token embedding weights in the output layer\n        # to reduce the total number of parameters, which is a concept known as weight tying.\n        gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n\n\n# Download the GPT-2 weights.\nsettings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n\n# Load the weights into the model.\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\n\n\n# Test the model to verify that it can generate coherent text.\ntext_1 = \"Every effort moves you\"\ntoken_ids = generate_text_simple(\n    model=gpt.to(device),\n    idx=text_to_token_ids(text_1, tokenizer).to(device),\n    max_new_tokens=15,\n    context_size=NEW_CONFIG.context_length,\n)\nprint(token_ids_to_text(token_ids, tokenizer))\n\n\n# Check if the model is already capable of classifying spam and ham messages via instruction\n# examples.\ntext_2 = (\n    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n    \" 'You are a winner you have been specially\"\n    \" selected to receive $1000 cash or a $2000 award.'\"\n)\ntoken_ids = generate_text_simple(\n    model=gpt.to(device),\n    idx=text_to_token_ids(text_2, tokenizer).to(device),\n    max_new_tokens=23,\n    context_size=NEW_CONFIG.context_length,\n)\nprint(token_ids_to_text(token_ids, tokenizer))"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#modify-model-for-fine-tuning-adding-a-classification-head",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#modify-model-for-fine-tuning-adding-a-classification-head",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Modify model for fine-tuning (adding a classification head)",
    "text": "Modify model for fine-tuning (adding a classification head)\nAdapting a GPT model for spam classification by altering its architecture. Initially, the model’s linear output layer mapped 768 hidden units to a vocabulary of 50,257 tokens. To detect spam, we replace this layer with a new output layer that maps the same 768 hidden units to just two classes, representing “spam” and “not spam.”\n\nFine-tuning selected layers vs. all layers\nSince we start with a pretrained model, it’s not necessary to fine-tune all model layers. In neural network-based language models, the lower layers generally capture basic language structures and semantics applicable across a wide range of tasks and datasets. So, fine-tuning only the last layers (i.e., layers near the output), which are more specific to nuanced linguistic patterns and task-specific features, is often sufficient to adapt the model to new tasks. A nice side effect is that it is computationally more efficient to fine-tune only a small number of layers.\n\n\n\nModel modification\n\n\n\n# Prepare the model for fine-tuning.\n\n# 1. Freeze all parameters in the model.\nfor param in gpt.parameters():\n    param.requires_grad = False\n\n# 2. Replace the final linear layer with a new one for the two classes.\ntorch.manual_seed(123)\nnum_classes = 2\ngpt.out_head = nn.Linear(GPT_CONFIG_124M.emb_dim, num_classes)\n\n# Mark additional layers as trainable, in particular the last transformer block as well as the\n# final layer norm.\nfor param in gpt.trf_blocks[-1].parameters():\n    param.requires_grad = True\nfor param in gpt.final_norm.parameters():\n    param.requires_grad = True\n\n\n# Try running the model with a random input to see that it is working.\ninputs_str = \"Do you have time\"\ninputs = tokenizer.encode(inputs_str)\ninputs = torch.tensor(inputs).unsqueeze(0)\nprint(\"Inputs:\", inputs_str)\nprint(\"Inputs dimensions:\", inputs.shape)  # B x T, i.e. batch size x sequence length\n\nwith torch.no_grad():\n    outputs = gpt.to(device)(inputs.to(device))\n\n# NOTE: The output shape is B x T x 2, i.e. batch size x sequence length x number of classes.\n#       The model produces logits for each class and for each token in the input sequence.\n# NOTE: We are interested in fine-tuning this model to return a class label indicating whether a\n#       model input is “spam” or “not spam.” We don’t need to fine-tune all four output rows;\n#       instead, we can focus on a single output token. In particular, we will focus on the last\n#       row corresponding to the last output token.\nprint(\"Outputs:\\n\", outputs)\nprint(\n    \"Outputs dimensions:\", outputs.shape\n)  # B x T x 2, i.e. batch size x sequence length x number of classes\nprint(\"Last output token:\", outputs[:, -1, :])\n\n\n\nSelecting the right output for fine-tuning\n\n\n\nOutput selection\n\n\nTo understand why we are particularly interested in the last output token only let’s take a look at the attention mechanism. We have already explored the attention mechanism, which establishes a relationship between each input token and every other input token, and the concept of a causal attention mask. This mask restricts a token’s focus to its current position and the those before it, ensuring that each token can only be influenced by itself and the preceding tokens (as shown below).\nThe empty cells indicate masked positions due to the causal attention mask, preventing tokens from attending to future tokens. The values in the cells represent attention scores; the last token, time, is the only one that computes attention scores for all preceding tokens.\nThe last token in a sequence accumulates the most information since it is the only token with access to data from all the previous tokens. Therefore, in our spam classification task, we focus on this last token during the fine-tuning process.\n\n\n\nOutput selection"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#evaluation-utilities",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#evaluation-utilities",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Evaluation utilities",
    "text": "Evaluation utilities\nSimilar to next token prediction, we use softmax to compute probabilities for the output logits, in particular, probabilities for each class (spam, not spam) - as shown below.\n\n\n\nComputing classification probabilities\n\n\n\n# Compute the probabilities for the last output token.\nprobas = torch.softmax(outputs[:, -1, :], dim=-1)\n\n# Compute the class label.\n# NOTE: {\"ham\": 0, \"spam\": 1}\nlabel = torch.argmax(probas)\nprint(\"Inputs:\", inputs_str)\nprint(\"Class label:\", label.item())\n\n\n# A utility function for computing classification accuracy for a data loader.\ndef calc_accuracy_loader(\n    data_loader: DataLoader,\n    model: GPTModel,\n    device: torch.device,\n    num_batches: int = None,\n) -&gt; float:\n    \"\"\"Compute the accuracy of a model on a data loader.\n\n    Args:\n        data_loader: The data loader to compute the accuracy on.\n        model: The model to compute the accuracy on.\n        device: The device to compute the accuracy on.\n        num_batches: The number of batches to compute the accuracy on. Defaults to None.\n\n    Returns:\n        The accuracy of the model on the data loader.\n    \"\"\"\n    # Set the model to evaluation mode (to avoid tracking gradients).\n    model.eval()\n\n    # Initialize the number of correct predictions and the number of examples.\n    correct_predictions, num_examples = 0, 0\n\n    # If the number of batches is not specified, use all batches.\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n\n    # Iterate over the data loader.\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        # If the number of batches has not been reached, compute the accuracy.\n        if i &lt; num_batches:\n            input_batch = input_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            # Compute the logits for the last output token.\n            with torch.no_grad():\n                # NOTE: The output shape is B x T x 2, i.e. batch size x sequence length x number\n                #       of classes. Here, we are only interested in the logits for the last output\n                #       token.\n                logits = model(input_batch)[:, -1, :]\n\n            # Compute the predicted labels.\n            # NOTE: dim=-1 computes the argmax over the classes.\n            predicted_labels = torch.argmax(logits, dim=-1)\n\n            # Update the number of examples and the number of correct predictions.\n            num_examples += predicted_labels.shape[0]\n            correct_predictions += (predicted_labels == target_batch).sum().item()\n        else:\n            break\n\n    return correct_predictions / num_examples\n\n\n# Compute baseline accuracy for the not yet fine-tuned model.\n\n# Move the model to the device.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpt.to(device)\ntorch.manual_seed(123)\n\n# Compute the accuracy for the training, validation, and test sets.\ntrain_accuracy = calc_accuracy_loader(train_loader, gpt, device, num_batches=10)\nval_accuracy = calc_accuracy_loader(val_loader, gpt, device, num_batches=10)\ntest_accuracy = calc_accuracy_loader(test_loader, gpt, device, num_batches=10)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n\n\nDefine the loss function\nHowever, before we begin fine-tuning the model, we must define the loss function we will optimize during training. Our objective is to maximize the spam classification accuracy of the model, which means that the preceding code should output the correct class labels: 0 for non-spam and 1 for spam. Because classification accuracy is not a differentiable function, we use cross-entropy loss as a proxy to maximize accuracy.\n\ndef calc_loss_batch(\n    input_batch: torch.Tensor,\n    target_batch: torch.Tensor,\n    model: GPTModel,\n    device: torch.device,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the loss for a batch of inputs and targets.\n\n    Args:\n        input_batch: The input batch.\n        target_batch: The target batch.\n        model: The model.\n        device: The device.\n\n    Returns:\n        The loss for the batch.\n    \"\"\"\n    # Move the input and target batches to the device.\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    # Compute the logits for the last output token.\n    logits = model(input_batch)[:, -1, :]\n\n    # Compute the loss (only for the last output token).\n    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n\n    return loss\n\n\ndef calc_loss_loader(\n    data_loader: DataLoader,\n    model: GPTModel,\n    device: torch.device,\n    num_batches: int = None,\n) -&gt; float:\n    \"\"\"Compute the loss for a data loader.\n\n    Args:\n        data_loader: The data loader.\n        model: The model.\n        device: The device.\n        num_batches: The number of batches to compute the loss on.\n\n    Returns:\n        The loss for the data loader.\n    \"\"\"\n    # Initialize the total loss.\n    total_loss = 0.0\n\n    # If the data loader is empty, return NaN.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    # If the number of batches is not specified, use all batches.\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches = min(num_batches, len(data_loader))\n\n    # Iterate over the data loader.\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i &lt; num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n\n    return total_loss / num_batches\n\n\n# Compute the loss for the training, validation, and test sets.\n\n# Disables gradient tracking for efficiency because we are not training yet\nwith torch.no_grad():\n    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=5)\n    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches=5)\n    test_loss = calc_loss_loader(test_loader, gpt, device, num_batches=5)\n\nprint(f\"Training loss: {train_loss:.3f}\")\nprint(f\"Validation loss: {val_loss:.3f}\")\nprint(f\"Test loss: {test_loss:.3f}\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#loss-visualization",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#loss-visualization",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Loss visualization",
    "text": "Loss visualization\nThe model’s training and validation loss over the five training epochs. Both the training loss, represented by the solid line, and the validation loss, represented by the dashed line, sharply decline in the first epoch and gradually stabilize toward the fifth epoch. This pattern indicates good learning progress and suggests that the model learned from the training data while generalizing well to the unseen validation data.\n\n# Plot the training and validation losses.\nimport matplotlib.pyplot as plt\n\n\ndef plot_values(\n    epochs_seen: torch.Tensor,\n    examples_seen: torch.Tensor,\n    train_values: list[float],\n    val_values: list[float],\n    label: str = \"loss\",\n):\n    \"\"\"Plot the training and validation losses.\n\n    Args:\n        epochs_seen: The number of epochs seen.\n        examples_seen: The number of examples seen.\n        train_values: The training values.\n        val_values: The validation values.\n        label: The label for the plot.\n    \"\"\"\n    # Create the plot.\n    fig, ax1 = plt.subplots(figsize=(8, 6))\n\n    # Plot the training and validation losses against the epochs.\n    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n\n    # Set the x-axis label.\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(label.capitalize())\n    ax1.legend()\n\n    # Creates a second x-axis for examples seen\n    ax2 = ax1.twiny()\n\n    # Invisible plot for aligning ticks\n    ax2.plot(examples_seen, train_values, alpha=0)\n    ax2.set_xlabel(\"Examples seen\")\n\n    # Adjusts layout to make room.\n    fig.tight_layout()\n\n    # Save the plot.\n    plt.savefig(f\"{label}-plot.pdf\")\n\n    # Show the plot.\n    plt.show()\n\n\n# Create the epochs and examples seen tensors.\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n\n# Plot the values.\nplot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-accuracy-plot",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#classification-accuracy-plot",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Classification accuracy plot",
    "text": "Classification accuracy plot\nBoth the training accuracy (solid line) and the validation accuracy (dashed line) increase substantially in the early epochs and then plateau, achieving almost perfect accuracy scores of 1.0. The close proximity of the two lines throughout the epochs suggests that the model does not overfit the training data very much.\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\nexamples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n\nplot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#performance-metrics-on-all-data-sets",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#performance-metrics-on-all-data-sets",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Performance metrics on all data sets",
    "text": "Performance metrics on all data sets\nThe training and test set performances are almost identical. The slight discrepancy between the training and test set accuracies suggests minimal overfitting of the training data. Typically, the validation set accuracy is somewhat higher than the test set accuracy because the model development often involves tuning hyperparameters to perform well on the validation set, which might not generalize as effectively to the test set. This situation is common, but the gap could potentially be minimized by adjusting the model’s settings, such as increasing the dropout rate (drop_rate) or the weight_decay parameter in the optimizer configuration.\n\ntrain_accuracy = calc_accuracy_loader(train_loader, gpt, device)\nval_accuracy = calc_accuracy_loader(val_loader, gpt, device)\ntest_accuracy = calc_accuracy_loader(test_loader, gpt, device)\n\nprint(f\"Training accuracy: {train_accuracy*100:.2f}%\")\nprint(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\nprint(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#using-the-model-for-classification",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#using-the-model-for-classification",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Using the model for classification",
    "text": "Using the model for classification\n\n\n\nModel usage\n\n\n\ndef classify_review(\n    text: str,\n    model: GPTModel,\n    tokenizer: tiktoken.Encoding,\n    device: torch.device,\n    max_length: Optional[int] = None,\n    pad_token_id: int = 50256,\n):\n    \"\"\"Classify a review using a fine-tuned GPT model.\n\n    Args:\n        text: The review to classify.\n        model: The fine-tuned GPT model.\n        tokenizer: The tokenizer.\n        device: The device to classify the review on.\n        max_length: The maximum length of the review.\n        pad_token_id: The padding token ID (defaults to the end-of-text token).\n\n    Returns:\n        The predicted label.\n    \"\"\"\n    model.eval()\n    # Prepares inputs to the model\n    input_ids = tokenizer.encode(text)\n\n    # Determine the maximum supported context length.\n    supported_context_length = model.pos_emb.weight.shape[1]\n\n    # Truncates sequences if they are too long\n    input_ids = input_ids[: min(max_length, supported_context_length)]\n\n    # Determine the maximum sequence length.\n    max_length = max_length if max_length is not None else supported_context_length\n\n    # Pad sequences to the longest sequence length.\n    input_ids += [pad_token_id] * (max_length - len(input_ids))\n\n    # Add a batch dimension.\n    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n\n    # Model inference without gradient tracking.\n    with torch.no_grad():\n        # Logits for the last output token.\n        logits = model(input_tensor)[:, -1, :]\n\n    predicted_label = torch.argmax(logits, dim=-1).item()\n\n    # Return the predicted label.\n    return \"spam\" if predicted_label == 1 else \"not spam\"\n\n\n# Try classifying some examples.\ntext_1 = (\n    \"You are a winner you have been specially\"\n    \" selected to receive $1000 cash or a $2000 award.\"\n)\nprint(\n    classify_review(\n        text=text_1,\n        model=gpt,\n        tokenizer=tokenizer,\n        device=device,\n        max_length=train_dataset.max_length,\n    )\n)\n\ntext_2 = (\n    \"Hey, just wanted to check if we're still on\" \" for dinner tonight? Let me know!\"\n)\nprint(\n    classify_review(\n        text=text_2,\n        model=gpt,\n        tokenizer=tokenizer,\n        device=device,\n        max_length=train_dataset.max_length,\n    )\n)"
  },
  {
    "objectID": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#save-the-model-checkpoint-to-disk",
    "href": "posts/2025_05_16_llms_from_scratch_part_6/chapter_06_fine_tuning_for_classification.html#save-the-model-checkpoint-to-disk",
    "title": "LLMs From Scratch - Chapter 6: Fine-tuning for Classification",
    "section": "Save the model checkpoint to disk",
    "text": "Save the model checkpoint to disk\n\n# Save the model checkpoint to disk.\ntorch.save(gpt.state_dict(), \"review_classifier.pth\")\n\n\n# Load the model checkpoint from disk.\nmodel_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\ngpt.load_state_dict(model_state_dict)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Currently, I am a Staff Software Engineer at NVIDIA, where I focus on defining and developing metrics for autonomous vehicle (AV) software, building data-driven evaluation products, and incorporating LLMs and VLMs into evaluation workflows. Just like AV planning systems move from rule-based systems to end-to-end learned planners, I am convinvced the systems tasked with evaluating AV planning stacks need to move to a data-driven approach that incorporates the nuanced world understanding and context AV systems themselves need to be able to reason about.\nPrior to NVIDIA (and fresh out of grad school), I was a Senior Machine Learning Engineer at Apple, working on robotics and decision making for autonomous vehicles. That is a fairly coarse summary for the 7+ years I’ve been with Apple’s SPG project tasked to work on autonomous systems. I’ve worked on everything from machine learned semantic map annotation, to the the initial rule-based planning system (and its transition to a hybrid rule-based/learned planner), to high signal-to-noise evaluation systems for said planner (with the goal of identifying test progressions and preventing regressions).\nI’ve received a B.S. degree in Electrical Engineering from the Vienna University of Technology, an M.S. degree in Electrical and Computer Engineering, and a Ph.D. degree in Robotics from the Georgia Institute of Technology. My doctoral research focused on self-reconfigurable multi-robot systems.\nI’m a recipient of the Fulbright scholarship and have held research positions at Carnegie Mellon University, Georgia Institute of Technology, and industry roles at Qualcomm, BMW, and Apple. My work on the Robotarium received the Best Paper Award on Multi-Robot Systems at the IEEE International Conference on Robotics and Automation (ICRA) in 2017, and I was a Student Best Paper Finalist at the IEEE Conference on Decision and Control in 2015.\n\n\n\nFoundation models: LLMs, VLMs, multi-modal models, diffusion models (for image, video, and audio generation), mechanistic interpretability and alignment via RLHF (reinforcement learning from human feedback), RLAIF (reinforcement learning from AI feedback), RLVR (reinforcement learning from verifiable rewards)\nMachine Learning: Reinforcement learning\nAutonomous Vehicles: Building the next generation of AV evaluation systems based on foundation models that enable detailed scene understanding and reasoning, adverse scenario generation, scene similarity computation based on embeddings, VLA models that enable interpretable reasoning about AV behavior\nRobotics: Multi-robot systems, self-reconfigurable robotics, swarm robotics (though this is a field I haven’t had the chance to work in for a while now)\n\n\n\n\nNVIDIA | Staff Software Engineer | 2024-Present\nFocus on defining and developing metrics for autonomous vehicle software, building data-driven evaluation products, and incorporating LLMs into evaluation workflows\nApple | Senior Machine Learning Engineer | 2017-2024 Worked on robotics and decision making for autonomous vehicles\nVarious Research Positions | 2012-2017\nResearch positions at Carnegie Mellon University and Georgia Institute of Technology\n\n\n\nGeorgia Institute of Technology | Ph.D., Robotics | 2016 Dissertation: Self-reconfigurable multi-robot systems\nGeorgia Institute of Technology | M.S., Electrical and Computer Engineering | 2012\nVienna University of Technology | B.S., Electrical Engineering | 2010\n\n\n\n\nBest Paper Award on Multi-Robot Systems | IEEE ICRA | 2017\nStudent Best Paper Finalist | IEEE CDC | 2015\nFulbright Scholarship | 2010-2012"
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Foundation models: LLMs, VLMs, multi-modal models, diffusion models (for image, video, and audio generation), mechanistic interpretability and alignment via RLHF (reinforcement learning from human feedback), RLAIF (reinforcement learning from AI feedback), RLVR (reinforcement learning from verifiable rewards)\nMachine Learning: Reinforcement learning\nAutonomous Vehicles: Building the next generation of AV evaluation systems based on foundation models that enable detailed scene understanding and reasoning, adverse scenario generation, scene similarity computation based on embeddings, VLA models that enable interpretable reasoning about AV behavior\nRobotics: Multi-robot systems, self-reconfigurable robotics, swarm robotics (though this is a field I haven’t had the chance to work in for a while now)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Daniel Pickem",
    "section": "",
    "text": "NVIDIA | Staff Software Engineer | 2024-Present\nFocus on defining and developing metrics for autonomous vehicle software, building data-driven evaluation products, and incorporating LLMs into evaluation workflows\nApple | Senior Machine Learning Engineer | 2017-2024 Worked on robotics and decision making for autonomous vehicles\nVarious Research Positions | 2012-2017\nResearch positions at Carnegie Mellon University and Georgia Institute of Technology"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Georgia Institute of Technology | Ph.D., Robotics | 2016 Dissertation: Self-reconfigurable multi-robot systems\nGeorgia Institute of Technology | M.S., Electrical and Computer Engineering | 2012\nVienna University of Technology | B.S., Electrical Engineering | 2010"
  },
  {
    "objectID": "about.html#awards-honors",
    "href": "about.html#awards-honors",
    "title": "Daniel Pickem",
    "section": "",
    "text": "Best Paper Award on Multi-Robot Systems | IEEE ICRA | 2017\nStudent Best Paper Finalist | IEEE CDC | 2015\nFulbright Scholarship | 2010-2012"
  }
]
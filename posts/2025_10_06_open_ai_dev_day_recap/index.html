<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Daniel Pickem">
<meta name="dcterms.date" content="2025-09-19">

<title>Spinning Up in Reinforcement Learning – Daniel Pickem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-0b37c64f34216b628666a8dac638b53b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Spinning Up in Reinforcement Learning – Daniel Pickem">
<meta property="og:description" content="Daniel Pickem is currently a Staff Software Engineer at NVIDIA, where he focuses on developing scalable evaluation systems for autonomous vehicles. His passion, however, lies in LLMs, multimodal models, agentic AI, and frontier / foundation models in general.">
<meta property="og:image" content="https://learning.oreilly.com/covers/urn:orm:book:9781835882702/">
<meta property="og:site_name" content="Daniel Pickem">
<meta name="twitter:title" content="Spinning Up in Reinforcement Learning – Daniel Pickem">
<meta name="twitter:description" content="Daniel Pickem is currently a Staff Software Engineer at NVIDIA, where he focuses on developing scalable evaluation systems for autonomous vehicles. His passion, however, lies in LLMs, multimodal models, agentic AI, and frontier / foundation models in general.">
<meta name="twitter:image" content="https://learning.oreilly.com/covers/urn:orm:book:9781835882702/">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Daniel Pickem</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Spinning Up in Reinforcement Learning</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">learning</div>
                <div class="quarto-category">reinforcement-learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Daniel Pickem </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 19, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-importance-of-rl" id="toc-the-importance-of-rl" class="nav-link active" data-scroll-target="#the-importance-of-rl">The importance of RL</a></li>
  <li><a href="#impactful-papers-in-rl" id="toc-impactful-papers-in-rl" class="nav-link" data-scroll-target="#impactful-papers-in-rl">Impactful papers in RL</a>
  <ul class="collapse">
  <li><a href="#reward-design-paradigms" id="toc-reward-design-paradigms" class="nav-link" data-scroll-target="#reward-design-paradigms">Reward design paradigms</a>
  <ul class="collapse">
  <li><a href="#rlhf-reinforcement-learning-from-human-feedback" id="toc-rlhf-reinforcement-learning-from-human-feedback" class="nav-link" data-scroll-target="#rlhf-reinforcement-learning-from-human-feedback">RLHF (Reinforcement Learning from <strong>Human</strong> Feedback)</a></li>
  <li><a href="#rlaif-reinforcement-learning-from-ai-feedback" id="toc-rlaif-reinforcement-learning-from-ai-feedback" class="nav-link" data-scroll-target="#rlaif-reinforcement-learning-from-ai-feedback">RLAIF (Reinforcement Learning from <strong>AI</strong> Feedback)</a></li>
  <li><a href="#rlvr-reinforcement-learning-from-verifiable-rewards" id="toc-rlvr-reinforcement-learning-from-verifiable-rewards" class="nav-link" data-scroll-target="#rlvr-reinforcement-learning-from-verifiable-rewards">RLVR (Reinforcement Learning from <strong>Verifiable</strong> Rewards)</a></li>
  <li><a href="#comparison" id="toc-comparison" class="nav-link" data-scroll-target="#comparison">Comparison</a></li>
  </ul></li>
  <li><a href="#optimization-algorithms" id="toc-optimization-algorithms" class="nav-link" data-scroll-target="#optimization-algorithms">Optimization algorithms</a>
  <ul class="collapse">
  <li><a href="#ppo---proximal-policy-optimization" id="toc-ppo---proximal-policy-optimization" class="nav-link" data-scroll-target="#ppo---proximal-policy-optimization">PPO - Proximal Policy Optimization</a></li>
  <li><a href="#dpo---direct-policy-optimization" id="toc-dpo---direct-policy-optimization" class="nav-link" data-scroll-target="#dpo---direct-policy-optimization">DPO - Direct Policy Optimization</a></li>
  <li><a href="#grpo---group-relative-policy-optimization" id="toc-grpo---group-relative-policy-optimization" class="nav-link" data-scroll-target="#grpo---group-relative-policy-optimization">GRPO - Group-relative policy optimization</a></li>
  <li><a href="#comparison-1" id="toc-comparison-1" class="nav-link" data-scroll-target="#comparison-1">Comparison</a></li>
  </ul></li>
  <li><a href="#notable-combos" id="toc-notable-combos" class="nav-link" data-scroll-target="#notable-combos">Notable combos</a>
  <ul class="collapse">
  <li><a href="#rlhf-ppo" id="toc-rlhf-ppo" class="nav-link" data-scroll-target="#rlhf-ppo">RLHF + PPO</a></li>
  <li><a href="#rlaif-ai-feedback-ppo-preference-optimization" id="toc-rlaif-ai-feedback-ppo-preference-optimization" class="nav-link" data-scroll-target="#rlaif-ai-feedback-ppo-preference-optimization">RLAIF (AI feedback) + PPO / preference optimization</a></li>
  <li><a href="#rlvr-verifiable-rewards-ppogrpo-style-rl" id="toc-rlvr-verifiable-rewards-ppogrpo-style-rl" class="nav-link" data-scroll-target="#rlvr-verifiable-rewards-ppogrpo-style-rl">RLVR (verifiable rewards) + PPO/GRPO-style RL</a></li>
  <li><a href="#rlhfrlaif-signals-dpo-no-online-rl-loop" id="toc-rlhfrlaif-signals-dpo-no-online-rl-loop" class="nav-link" data-scroll-target="#rlhfrlaif-signals-dpo-no-online-rl-loop">RLHF/RLAIF signals + DPO (no online RL loop)</a></li>
  <li><a href="#rlvr-grpo" id="toc-rlvr-grpo" class="nav-link" data-scroll-target="#rlvr-grpo">RLVR + GRPO</a></li>
  <li><a href="#generalized-preference-optimization-umbrella" id="toc-generalized-preference-optimization-umbrella" class="nav-link" data-scroll-target="#generalized-preference-optimization-umbrella">“Generalized” preference optimization (umbrella)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#getting-started-with-rl" id="toc-getting-started-with-rl" class="nav-link" data-scroll-target="#getting-started-with-rl">Getting started with RL</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>I recently read Maxim Lapan’s book <a href="https://learning.oreilly.com/library/view/deep-reinforcement-learning/9781835882702/">Deep Reinforcement Learning Hands-On - Third Edition</a> and wanted to share my notes and code-along Jupyter notebooks here. This post just presents a brief summary and intro to RL while I’ll publish the deep dives for select chapters in separate posts. My main motivation in reading this book was to get up to speed on RL methods as applied to large language models as well as robotic foundation models. Earlier editions of the book focused on RL for robotic and video game applications (mostly through <a href="https://gymnasium.farama.org/index.html">Gymnasium</a> environments). The third edition of the book, however, has been updated to include more LLM-relevant chapters such as trust region methods (PPO, SAC) but also a chapter on RLHF.</p>
<p>The book overall covers a lot of ground and provides a good overview of RL (it was published in November 2024). Nonetheless, I recommend supplementing it with OpenAI’s <a href="https://spinningup.openai.com/en/latest/">Spinning Up</a> (which was last updated in January 2020 but contains great reference implementations and a broad collection of papers).</p>
<p>Two more posts worth pointing out are Sebastian Raschka’s notebook on <a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb">Direct Preference Optimization (DPO) from Scratch</a> which is a hands-on guide to implement this alternative method to Proximal Policy Optimization (PPO) for RLHF and a similar repo on <a href="https://github.com/policy-gradient/GRPO-Zero">Group-relative policy optimization (GRPO)</a>. The latter implements a more recent alternative to PPO for RLHF.</p>
<p>A recent post on <a href="https://tokens-for-thoughts.notion.site/post-training-101#262b8b68a46d80098b4dec60987a0c17">Post-training 101</a> outlines various RL methods used in foundation model post-training well and is a great starting point with a broad overview of the field. This post draws from Nathan Lambert’s excellent book on <a href="https://rlhfbook.com/">Reinforcement Learning from Human Feedback (RLHF)</a> which is available for free online.</p>
<p>One last resource I want to mention is a noteworthy collection of papers relating to RLHF - <a href="https://github.com/opendilab/awesome-RLHF">awesome-RLHF</a></p>
<section id="the-importance-of-rl" class="level1">
<h1>The importance of RL</h1>
<p>My original goal with this post was just to announce a series of RL-focused posts. Instead, I’ll also use this post to summarize the most common reward design paradigms and optimization algorithms for RL applied to LLMs - not just RLHF but also RLAIF and RLVR (the latter playing a key role in the latest batch of reasoning models in particular with applications to math and software engineering).</p>
<p>Reinforcement learning (RL) is a branch of machine learning focused on training agents to make sequences of decisions by interacting with an environment. In RL, an agent observes the current state of the environment, selects an action according to a policy (which maps an observation of the environment to an action, and can be deterministic or stochastic), and receives feedback in the form of a reward signal. The environment then transitions to a new state based on the agent’s action. The key components of RL are:</p>
<ul>
<li><strong>Agent</strong>: The learner or decision maker.</li>
<li><strong>Environment</strong>: The external system the agent interacts with.</li>
<li><strong>Policy</strong>: The strategy or mapping from states to actions that the agent uses to decide what to do next.</li>
<li><strong>Reward signal</strong>: A scalar feedback signal that tells the agent how good or bad its action was in a given state.</li>
<li><strong>Value function</strong>: An estimate of how good it is for the agent to be in a given state (or to perform a certain action in a state), in terms of expected future rewards.</li>
<li><strong>Model (optional)</strong>: Some RL methods use a model of the environment to predict future states and rewards.</li>
</ul>
<p>The agent’s goal is to learn a policy that maximizes the cumulative reward over time, often by balancing exploration (trying new actions) and exploitation (choosing actions known to yield high rewards). RL is particularly powerful for solving complex, sequential decision-making problems where explicit supervision is difficult or impossible.</p>
<p>In the context of post-training of LLMs via RLHF, we can think of the base model or instruction-tuned model (mostly via SFT) as the policy acting in an “environment” of prompts. We then learn a scalar reward that captures what we want—such as human preferences, verifiable outcomes, or step-level scores (this would be the reward model in RLHF). The typical RLHF loop is: sample outputs from the policy, score them with the reward model, and update the policy using KL-regularized policy gradients to favor higher-reward behavior, while anchoring to a reference model to prevent drift (see <a href="https://tokens-for-thoughts.notion.site/post-training-101#262b8b68a46d80098b4dec60987a0c17">this post</a> for more details).</p>
<p>RLHF transformed the development of large language models by making reinforcement learning practical at scale. Introduced by Christiano et al.&nbsp;(2017, <a href="https://arxiv.org/pdf/1706.03741">Deep reinforcement learning from human preferences</a>) and scaled in InstructGPT (Ouyang et al., 2022, <a href="https://arxiv.org/pdf/2203.02155">Training language models to follow instructions with human feedback</a>), it combined supervised fine-tuning, preference-based reward modeling, and reinforcement learning (typically PPO) to align models more closely with human intent. This approach established the modern alignment pipeline behind systems like ChatGPT and marked the pivotal moment when RL became central to LLM training.</p>
<p>Besides RLHF, the RL toolbox in modern post-training pipelines has been expanded to include the following methods:</p>
<ul>
<li>RLAIF (Reinforcement Learning from AI Feedback) replaces costly human annotations with feedback from a stronger model, reducing reliance on human labelers while preserving the preference-model + RL pipeline.</li>
<li>RLVR (Reinforcement Learning from Verifiable Rewards) avoids subjective human or model judgments altogether by using objective, checkable criteria (e.g., correctness of math steps, program execution results, rule satisfaction) as the reward signal.</li>
</ul>
<p>Both methods are positioned as natural extensions of RLHF: RLAIF scales supervision, while RLVR improves reliability by grounding training in verifiable signals (more on that in the sections below).</p>
</section>
<section id="impactful-papers-in-rl" class="level1">
<h1>Impactful papers in RL</h1>
<p><a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html">Spinning up</a> has a great collection of papers that are essential reading for anyone interested in RL. This section mostly aims to highlight the most impactful subset of papers relating to RLHF, RLAIF, and RLVR as well as the underlying alignment methods (PPO, DPO, GRPO). Since Spinning Up was last updated in January 2020, this section also highlights more recent papers (such as GRPO). The following two sections characterize two axes of the RL toolbox: reward design paradigms and optimization algorithms.</p>
<section id="reward-design-paradigms" class="level2">
<h2 class="anchored" data-anchor-id="reward-design-paradigms">Reward design paradigms</h2>
<p>The <strong>reward source</strong> refers to the origin and nature of the feedback signal used to guide the learning process in reinforcement learning. In the context of LLM post-training, the reward source determines <em>how</em> we evaluate and score model outputs, which in turn shapes the agent’s behavior. Each reward source has trade-offs in terms of scalability, reliability, cost, and alignment with desired behaviors. The choice of reward source is a foundational design decision in any RL-based post-training pipeline for LLMs.</p>
<section id="rlhf-reinforcement-learning-from-human-feedback" class="level3">
<h3 class="anchored" data-anchor-id="rlhf-reinforcement-learning-from-human-feedback">RLHF (Reinforcement Learning from <strong>Human</strong> Feedback)</h3>
<p>In this setup, human annotators provide rankings, ratings, or preference judgments on model outputs. This data is used to train a reward model that predicts human preferences, which then serves as the reward function for RL optimization (as in RLHF). RLHF was one of the earliest methods to scale reinforcement learning for LLMs but it is human labor intensive.</p>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03741">Deep reinforcement learning from human preferences, 2017</a> by Christiano et al.&nbsp;(original RLHF)</li>
<li><a href="https://arxiv.org/pdf/1909.08593">Fine-Tuning Language Models from Human Preferences, 2019</a> by Ziegler et al.&nbsp;(direct precursor to InstructGPT)</li>
<li><a href="https://arxiv.org/pdf/2203.02155">Training language models to follow instructions with human feedback, 2022</a> by Ouyang et al.&nbsp;(InstructGPT)</li>
</ul>
</section>
<section id="rlaif-reinforcement-learning-from-ai-feedback" class="level3">
<h3 class="anchored" data-anchor-id="rlaif-reinforcement-learning-from-ai-feedback">RLAIF (Reinforcement Learning from <strong>AI</strong> Feedback)</h3>
<p>Instead of relying on humans, a stronger or more specialized AI model provides the feedback—either by ranking outputs, scoring them, or generating preference data (e.g.&nbsp;preference labels). This approach can scale supervision and reduce costs, but the quality of feedback depends on the teacher model’s capabilities and biases. This method is much cheaper and scalable but the quality depends on the teacher model.</p>
<ul>
<li><a href="https://arxiv.org/pdf/2212.08073">Constitutional AI: Harmlessness from AI Feedback, 2022</a> by Bai et al.&nbsp;(Anthropic)</li>
</ul>
</section>
<section id="rlvr-reinforcement-learning-from-verifiable-rewards" class="level3">
<h3 class="anchored" data-anchor-id="rlvr-reinforcement-learning-from-verifiable-rewards">RLVR (Reinforcement Learning from <strong>Verifiable</strong> Rewards)</h3>
<p>The reward is computed automatically based on objective, verifiable criteria—such as correctness of a math solution, passing unit tests for code, or satisfying explicit rules. This paradigm (RLVR) removes subjective judgments and grounds the reward in measurable outcomes, which is especially useful for tasks with clear success criteria. OpenAI’s o1-style training is a key example of this paradigm (see <a href="https://openai.com/index/learning-to-reason-with-llms/?utm_source=chatgpt.com">blog post</a>).</p>
<ul>
<li><a href="https://arxiv.org/pdf/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025</a> (DeepSeek)</li>
<li><a href="https://arxiv.org/pdf/2506.14245">Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs, 2025</a> by Wen et al.&nbsp;(Microsoft)</li>
<li><a href="https://arxiv.org/pdf/2503.23829v1">Expanding RL with Verifiable Rewards Across Diverse Domains, 2025</a> by Su et al.&nbsp;(Tencent)</li>
<li><a href="https://arxiv.org/pdf/2411.15124">Tülu 3: Pushing Frontiers in Open Language Model Post-Training, 2025</a> by Lambert et al.&nbsp;(Allen Institute for AI)</li>
<li><a href="https://arxiv.org/pdf/2504.20571">Reinforcement Learning for Reasoning in Large Language Models with One Training Example, 2025</a> by Wang et al.&nbsp;(Microsoft)</li>
</ul>
</section>
<section id="comparison" class="level3">
<h3 class="anchored" data-anchor-id="comparison">Comparison</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Reward Design</strong></th>
<th><strong>Definition</strong></th>
<th><strong>Pros</strong></th>
<th><strong>Cons</strong></th>
<th><strong>Seminal Paper</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>RLHF</strong> (Reinforcement Learning from Human Feedback)</td>
<td>Human annotators rank or rate model outputs → reward model → optimization</td>
<td>- High-quality signal from humans<br>- Captures nuanced human values<br>- Proven to work (InstructGPT, ChatGPT)</td>
<td>- Very expensive &amp; slow (needs labelers)<br>- Limited scalability<br>- Inconsistent/biased labels</td>
<td><a href="https://arxiv.org/abs/2203.02155">InstructGPT (OpenAI, 2022)</a></td>
</tr>
<tr class="even">
<td><strong>RLAIF</strong> (Reinforcement Learning from AI Feedback)</td>
<td>Teacher models provide preference labels, often guided by rules/constitutions</td>
<td>- Cheaper &amp; scalable<br>- Consistent labeling<br>- Can encode safety rules explicitly</td>
<td>- Dependent on teacher quality<br>- Risk of propagating teacher’s biases/errors</td>
<td><a href="https://arxiv.org/abs/2212.08073">Constitutional AI (Anthropic, 2022)</a></td>
</tr>
<tr class="odd">
<td><strong>RLVR</strong> (Reinforcement Learning from Verifiable Rewards)</td>
<td>Objective, automatically checkable signals (math correctness, passing code tests, factual checks)</td>
<td>- Fully automatable, infinite data<br>- No human bias<br>- Perfectly scalable for tasks with verifiable outputs</td>
<td>- Limited to domains with hard checkers<br>- Binary rewards are sparse (unstable for RL)<br>- Requires careful optimization (PPO struggles)</td>
<td><a href="https://arxiv.org/pdf/2501.12948">DeepSeek-R1, 2025</a></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="optimization-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="optimization-algorithms">Optimization algorithms</h2>
<p>This section provides an overview of the key optimization methods used to update a policy (the model) based on reward or preference signals. These approaches are central to RL-based post-training, enabling language models to better align with desired behaviors by effectively leveraging feedback.</p>
<section id="ppo---proximal-policy-optimization" class="level3">
<h3 class="anchored" data-anchor-id="ppo---proximal-policy-optimization">PPO - Proximal Policy Optimization</h3>
<p>PPO is a widely used on-policy policy-gradient algorithm designed to train RL agents in a stable yet efficient way. It introduces a surrogate objective with a KL-style penalty (or clipping) that limits how far the new policy can deviate from the old one, preventing collapse while still allowing meaningful updates. This balance between reward maximization and stability made PPO the algorithm of choice in the original RLHF pipelines at OpenAI (<a href="https://arxiv.org/pdf/2203.02155">InstructGPT</a>) and Anthropic (<a href="https://arxiv.org/pdf/2204.05862">HH-RLHF</a>), and it remains one of the most influential algorithms in modern RL practice.</p>
<ul>
<li><a href="https://arxiv.org/pdf/1707.06347">Proximal Policy Optimization Algorithms, 2017</a> by Schulman et al.&nbsp;(OpenAI)</li>
</ul>
</section>
<section id="dpo---direct-policy-optimization" class="level3">
<h3 class="anchored" data-anchor-id="dpo---direct-policy-optimization">DPO - Direct Policy Optimization</h3>
<p>DPO reframes RLHF as a supervised learning problem by directly optimizing on human preference pairs (preferred vs.&nbsp;dispreferred responses). Instead of learning an explicit reward model or running PPO training loops, DPO uses a simple objective that encourages the policy to favor preferred outputs. This makes it far easier to implement, more stable in practice, and avoids the complexity of full RL infrastructure while still capturing the benefits of preference-based alignment.</p>
<p>DPO is not an RL algorithm (therefore also not on-policy and not a policy-gradient method). It’s essentially supervised learning with a cleverly derived loss that mimics the effect of RLHF, but without explicit RL.</p>
<ul>
<li><a href="https://arxiv.org/pdf/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model, 2023</a> by Rafailov et al.&nbsp;(Stanford)</li>
</ul>
</section>
<section id="grpo---group-relative-policy-optimization" class="level3">
<h3 class="anchored" data-anchor-id="grpo---group-relative-policy-optimization">GRPO - Group-relative policy optimization</h3>
<p>Group Relative Policy Optimization (GRPO) is an efficient reinforcement learning algorithm for training large language models that improves upon Proximal Policy Optimization (PPO) by eliminating the need for a separate value network (i.e.&nbsp;the critic model in actor-critic algorithms). GRPO generates multiple responses to a single prompt, calculates their average reward, and then uses each response’s advantage relative to this group average to update the language model’s policy. This “group-based advantage estimation” stabilizes training and enhances reasoning abilities by providing a robust, memory-efficient baseline for policy updates without requiring complex value functions. A comparison of PPO and GRPO is shown below while a more detailed explanation is available <a href="https://medium.com/@sulbha.jindal/proximal-policy-optimization-ppo-vs-group-relative-policy-optimization-grpo-988fa7af0241">here</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWFPuXgPat7kWmBOBSqx4A.png" class="img-fluid figure-img"></p>
<figcaption>PPO vs.&nbsp;GRPO comparison from the DeepSeek paper that introduced GRPO: “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”</figcaption>
</figure>
</div>
<ul>
<li><a href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024</a> by DeepSeek</li>
<li><a href="https://arxiv.org/pdf/2503.11486v1">A Review of DeepSeek Models’ Key Innovative Techniques, 2025</a></li>
</ul>
</section>
<section id="comparison-1" class="level3">
<h3 class="anchored" data-anchor-id="comparison-1">Comparison</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Optimizer</strong></th>
<th><strong>Definition</strong></th>
<th><strong>Pros</strong></th>
<th><strong>Cons</strong></th>
<th><strong>Seminal Paper</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>PPO</strong> (Proximal Policy Optimization)</td>
<td>Policy gradient RL with KL-penalty to keep model close to a reference</td>
<td>- Stable relative to vanilla policy gradients<br>- Widely implemented &amp; tested<br>- Pairs naturally with reward models</td>
<td>- Requires critic/reward model<br>- Infrastructure heavy<br>- Struggles with sparse/verifiable rewards</td>
<td><a href="https://arxiv.org/abs/2203.02155">InstructGPT (OpenAI, 2022)</a></td>
</tr>
<tr class="even">
<td><strong>DPO</strong> (Direct Preference Optimization)</td>
<td>Reformulates preference alignment as supervised learning over preference pairs</td>
<td>- No reward model needed<br>- Simple, stable, easy to implement<br>- Efficient offline training</td>
<td>- Needs lots of high-quality preference pairs<br>- Less flexible than RL for long-horizon tasks</td>
<td><a href="https://arxiv.org/abs/2305.18290">DPO (Stanford, 2023)</a></td>
</tr>
<tr class="odd">
<td><strong>GRPO</strong> (Group Relative Policy Optimization)</td>
<td>Uses group baselines among sampled outputs instead of a critic; ideal for 0/1 rewards</td>
<td>- Handles sparse/verifiable rewards well<br>- No critic/reward model required<br>- Scales better for math/code reasoning</td>
<td>- Newer, less mature<br>- Needs multiple samples per prompt (compute heavy)</td>
<td><a href="https://arxiv.org/abs/2402.03300">DeepSeekMath (2024)</a></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="notable-combos" class="level2">
<h2 class="anchored" data-anchor-id="notable-combos">Notable combos</h2>
<p>Just to put all these techniques into perspective, in this section I want to highlighht a few notable combinations of reward design paradigms and optimization algorithms that frontier labs have used to train their models.</p>
<section id="rlhf-ppo" class="level3">
<h3 class="anchored" data-anchor-id="rlhf-ppo">RLHF + PPO</h3>
<p>Flagship: InstructGPT (OpenAI, 2022): human preference data → reward model → PPO against a reference model. <a href="https://arxiv.org/abs/2203.02155">Neur IPS Proceedings</a> <a href="https://arxiv.org/abs/2203.02155">arXiv</a></p>
<p><a href="https://arxiv.org/abs/2402.03300">DeepSeekMath (2024)</a></p>
</section>
<section id="rlaif-ai-feedback-ppo-preference-optimization" class="level3">
<h3 class="anchored" data-anchor-id="rlaif-ai-feedback-ppo-preference-optimization">RLAIF (AI feedback) + PPO / preference optimization</h3>
<p>Flagship: Constitutional AI (Anthropic, 2022): AI-written critiques &amp; revisions guided by a “constitution”; includes a supervised phase and a reinforcement phase (can be implemented with PPO-style optimization or offline preference objectives). <a href="https://arxiv.org/abs/2212.08073">arXiv</a> <a href="https://arxiv.org/abs/2212.08073">Anthropic</a></p>
</section>
<section id="rlvr-verifiable-rewards-ppogrpo-style-rl" class="level3">
<h3 class="anchored" data-anchor-id="rlvr-verifiable-rewards-ppogrpo-style-rl">RLVR (verifiable rewards) + PPO/GRPO-style RL</h3>
<p>Trend: Reasoning-era training (math/code) where the reward is a deterministic check (e.g., correct final answer, passing unit tests). OpenAI frames o1-style training as large-scale RL with verifiable signals; community papers analyze RLVR explicitly (often with GRPO). <a href="https://arxiv.org/abs/2402.03300">OpenAI</a></p>
<p>Examples in the literature: GRPO with verifiable rewards for reasoning; analyses/theory of RLVR dynamics appear in 2025 works. <a href="https://arxiv.org/abs/2402.03300">arXiv</a> <a href="https://arxiv.org/abs/2402.03300">OpenAI</a></p>
</section>
<section id="rlhfrlaif-signals-dpo-no-online-rl-loop" class="level3">
<h3 class="anchored" data-anchor-id="rlhfrlaif-signals-dpo-no-online-rl-loop">RLHF/RLAIF signals + DPO (no online RL loop)</h3>
<p>Flagship: DPO (Stanford et al., 2023): turns preference alignment into a simple classification-style loss; widely adopted as a drop-in alternative to PPO-based RLHF. (Also extended to other modalities like diffusion.) <a href="https://arxiv.org/abs/2305.18290">arXiv</a></p>
</section>
<section id="rlvr-grpo" class="level3">
<h3 class="anchored" data-anchor-id="rlvr-grpo">RLVR + GRPO</h3>
<p>Flagship use: GRPO (introduced in DeepSeekMath; later popularized in reasoning LLM training such as DeepSeek-R1): group-relative policy gradients fit naturally with 0/1 verifiable rewards. <a href="https://arxiv.org/abs/2402.03300">arXiv</a></p>
</section>
<section id="generalized-preference-optimization-umbrella" class="level3">
<h3 class="anchored" data-anchor-id="generalized-preference-optimization-umbrella">“Generalized” preference optimization (umbrella)</h3>
<p>GPO/GRPO family: Google DeepMind’s Generalized Preference Optimization unifies DPO/IPO/SLiC-style offline preference objectives; GRPO can be viewed as a complementary, RL-style optimizer that pairs well with verifiable rewards. <a href="https://arxiv.org/abs/2402.03300">arXiv</a></p>
</section>
</section>
</section>
<section id="getting-started-with-rl" class="level1">
<h1>Getting started with RL</h1>
<p>One of my motivations for working through the book was to get more exposure and practice writing PyTorch code. From what I’ve found so far, implementing RL algorithms is 80% engineering in Python and 20% ML (give or take). Some of the components that need to be in place for making the training of an RL algorithm work are:</p>
<ul>
<li>Environment setup and interaction (including wrappers)</li>
<li>Experience collection</li>
<li>Replay buffer (experience storage and sampling)</li>
<li>Agent scaffolding</li>
<li>Model architecture (this is the 20% ML part)</li>
<li>Training loop (this is also part of the 20% ML part)</li>
<li>Evaluation</li>
<li>Logging</li>
<li>etc.</li>
</ul>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p><em>Keep learning!</em></p>
<p>Daniel</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/dpickem\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Daniel Pickem, 2025 - <a href="disclaimer">Disclaimer</a></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>